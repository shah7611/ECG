{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c28d8d",
   "metadata": {},
   "source": [
    "## bpm normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75410ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pywt\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64   \n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "\n",
    "\n",
    "def decrypt(input_file):\n",
    "    \"\"\"Decrypt encrypted JSON file\"\"\"\n",
    "    private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    with open(input_file, 'rb') as f:\n",
    "        encrypted_data = f.read()\n",
    "    enc = base64.b64decode(encrypted_data[24:])\n",
    "    data = unpad(cipher.decrypt(enc), 16)\n",
    "    decoded_string = data.decode('utf-8')\n",
    "    return json.loads(decoded_string)\n",
    "\n",
    "    \n",
    "def baseline_wander(X):\n",
    "    def get_median_filter_width(sampling_rate, duration):\n",
    "        res = int(sampling_rate * duration)\n",
    "        res += (res % 2) - 1\n",
    "        return res\n",
    "\n",
    "    ms_flt_array = [0.2, 0.6]\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype=\"int\")\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(500, ms_flt_array[i])\n",
    "    X0 = X\n",
    "    for mi in range(0, len(mfa)):\n",
    "        X0 = sp_signal.medfilt(X0, mfa[mi])\n",
    "    X0 = np.subtract(X, X0)\n",
    "    return X0\n",
    "\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    \"\"\"Normalize signal to range [0, 1].\"\"\"\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    data = normalize(signal_data, min_val, max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_process(filename):\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "    \n",
    "    # print(f\"Type of filename['dataL2']: {type(filename['dataL2'])}\")\n",
    "    # print(f\"Content of filename['dataL2']: {filename['dataL2']}\")\n",
    "    \n",
    "    for key in keys:\n",
    "        sig = np.array(filename[key])\n",
    "        datas.append(sig.astype('float32'))\n",
    "    \n",
    "    datas_array = np.array(datas)\n",
    "    print(datas_array.shape)\n",
    "    min_val = np.min(datas_array)\n",
    "    max_val = np.max(datas_array)\n",
    "    \n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(process_signal(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    print(final_data)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0: \n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        # If we have 2-3 outlier peaks, remove them\n",
    "        # if 2 <= high_amp_count <= 3:\n",
    "        # if 3 <= high_amp_count <= 4 and len(peaks) - high_amp_count > 0:\n",
    "        # if 5 <= high_amp_count <= 6 and len(peaks) - high_amp_count > 0:\n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            # mask[high_amp_indices] = False\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            # Check if this could be a T-wave (150-450ms after R-peak)\n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                # T-waves are typically smaller and wider\n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    # Measure width at half maximum\n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    # T-waves are wider than QRS complexes\n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    \"\"\"Multi-strategy robust QRS detection for difficult cases\"\"\"\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated, \n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None):\n",
    "    \"\"\"\n",
    "    Enhanced QRS detection with Amplitude Guardrails for AV Blocks\n",
    "    \"\"\"\n",
    "    # Apply baseline wander removal\n",
    "    # data_clean = baseline_wander(data) \n",
    "    data_clean = data # Keeping your override\n",
    "\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "            \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # =================================================================\n",
    "    # CRITICAL FIX: GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    # =================================================================\n",
    "    if len(cleaned_r) >= 2:\n",
    "        # Calculate reference height (Median of existing peaks)\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        # Only fill gaps if median_rr suggests a normal rhythm (< 1.5s).\n",
    "        # If median_rr is already 2.0s (bradycardia), huge gaps are normal.\n",
    "        if median_rr < 1.5: \n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                        \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    # Lower threshold slightly for gap search\n",
    "                    low_thresh = mean_val * 0.6 \n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                            \n",
    "                        # 1. Check Sharpness\n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            # 2. Refine Position\n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            # 3. AMPLITUDE CHECK (The Fix)\n",
    "                            # Even if it's sharp, is it tall enough?\n",
    "                            # AV Block P-waves are sharp but short.\n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            # Must be at least 40-50% of the median R-peak height\n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "\n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # =================================================================\n",
    "\n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    # Relaxed expectations for Bradycardia/AV Block\n",
    "    min_expected_peaks = int(30/60 * segment_duration) \n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    # Fallback to robust only if counts are extremely off\n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        # Valid intervals widened to account for Bradycardia/Pauses\n",
    "        # valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 3.5)] \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)] \n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_data, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    max_len = len(ecg_data)\n",
    "    \n",
    "    if num_segments > 1:\n",
    "        window_step = (max_len - min_segment_length) / (num_segments - 1)\n",
    "        window_step = round(window_step)\n",
    "    else:\n",
    "        window_step = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = start_idx + min_segment_length\n",
    "        \n",
    "        if end_idx > max_len:\n",
    "            start_idx = max_len - min_segment_length\n",
    "            end_idx = max_len\n",
    "            \n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            end_idx = min(min_segment_length, max_len)\n",
    "        \n",
    "        segment = ecg_data[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment) < 100:\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        segment_duration = len(segment) / sampling_rate\n",
    "        ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(segment, sampling_rate, segment_duration)\n",
    "        print(f\"Segment {i+1}: Detected {len(r_peaks)} R-peaks, BPM: {bpm:.1f}\")\n",
    "        \n",
    "        adjusted_r_peaks = r_peaks + start_idx if len(r_peaks) > 0 else np.array([])\n",
    "        adjusted_cleaned_r = np.array(cleaned_r) + start_idx if len(cleaned_r) > 0 else np.array([])\n",
    "        \n",
    "        results.append({\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_filtered': ecg_filtered,\n",
    "            'r_peaks': adjusted_r_peaks,\n",
    "            'bpm': bpm,\n",
    "            'cleaned_r': adjusted_cleaned_r,\n",
    "            'ecg_raw': segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_ecg_segments(ecg_data, sampling_rate, results, title=\"ECG Segments with R-peaks and BPM\"):\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(15, 3*num_segments))\n",
    "    \n",
    "    if num_segments == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    for i, (ax, result) in enumerate(zip(axes, results)):\n",
    "        segment_num = result['segment_num']\n",
    "        start_idx = result['start_idx']\n",
    "        end_idx = result['end_idx']\n",
    "        bpm = result['bpm']\n",
    "        r_peaks = result['r_peaks']\n",
    "        \n",
    "        segment_time = time[start_idx:end_idx]\n",
    "        \n",
    "        ax.plot(segment_time, result['ecg_raw'], 'b-', alpha=0.7, linewidth=1, label='ECG Raw')\n",
    "        \n",
    "        if len(r_peaks) > 0:\n",
    "            r_times = r_peaks / sampling_rate\n",
    "            r_values = ecg_data[r_peaks.astype(int)]\n",
    "            ax.plot(r_times, r_values, 'ro', markersize=8, label='R-peaks', alpha=0.7)\n",
    "        \n",
    "        segment_duration = (end_idx - start_idx) / sampling_rate\n",
    "        ax.set_title(f'Segment {segment_num}: {start_idx}-{end_idx} samples '\n",
    "                    f'({segment_duration:.2f}s), BPM: {bpm:.1f}')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([segment_time[0], segment_time[-1]])\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ECG SEGMENT ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nSegment {result['segment_num']}:\")\n",
    "        print(f\"  Samples: {result['start_idx']}-{result['end_idx']}\")\n",
    "        print(f\"  Duration: {(result['end_idx']-result['start_idx'])/sampling_rate:.2f}s\")\n",
    "        print(f\"  BPM: {result['bpm']:.1f}\")\n",
    "        print(f\"  R-peaks detected: {len(result['r_peaks'])}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "def plot_full_ecg(ecg_data, sampling_rate, title=\"Full ECG Signal Analysis\"):\n",
    "    \"\"\"\n",
    "    Runs detection on the entire dataset and plots a single continuous view.\n",
    "    \"\"\"\n",
    "    # Run detection on the full unsegmented data\n",
    "    # Note: We ignore segment_duration to let the function calculate it automatically\n",
    "    _, r_peaks, global_bpm, _ = qrs_detect(ecg_data, sampling_rate)\n",
    "    \n",
    "    plt.figure(figsize=(20, 6)) # Width of 20 makes the 15k samples readable\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    # Plot the signal\n",
    "    plt.plot(time_axis, ecg_data, 'b-', linewidth=0.8, alpha=0.8, label='Filtered ECG')\n",
    "    \n",
    "    # Plot the peaks\n",
    "    if len(r_peaks) > 0:\n",
    "        # Filter out peaks that might be out of bounds (safety check)\n",
    "        valid_peaks = r_peaks[r_peaks < len(ecg_data)].astype(int)\n",
    "        \n",
    "        peak_times = valid_peaks / sampling_rate\n",
    "        peak_values = ecg_data[valid_peaks]\n",
    "        \n",
    "        plt.plot(peak_times, peak_values, 'ro', markersize=4, label='R-peaks')\n",
    "        \n",
    "        # Optional: Annotate every 5th peak to help navigation\n",
    "        for i, (t, v) in enumerate(zip(peak_times, peak_values)):\n",
    "            if i % 5 == 0:\n",
    "                plt.annotate(f'{t:.1f}s', (t, v), xytext=(0, 10), \n",
    "                             textcoords='offset points', ha='center', fontsize=8, color='red')\n",
    "\n",
    "    plt.title(f\"{title} | Global BPM: {global_bpm:.1f} | Total Peaks: {len(r_peaks)}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Normalized Amplitude\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Global Analysis: {len(r_peaks)} peaks detected over {len(ecg_data)/sampling_rate:.2f} seconds.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    " \n",
    "# # input_json = r\"simulator\\contec\\bigeminy_1756103016311.json\" \n",
    "# input_json = r\"simulator\\contec\\trigeminy_1756103085272.json\" \n",
    "# input_json = r\"simulator\\contec\\asystl_1756103447146.json\" \n",
    "# # input_json = r\"simulator\\contec\\1d av_1756104504294.json\" \n",
    "# input_json = r\"simulator\\contec\\3d av_1756104633918.json\" \n",
    "# input_json = r\"simulator\\contec\\280bpm_1756100716422.json\" \n",
    "# input_json = r\"simulator\\contec\\av sequence_1756106676125.json\"   #####\n",
    "# input_json = r\"simulator\\contec\\dmnd freq_1756106571373.json\"\n",
    "#    \n",
    "# input_json = r\"simulator\\fluke\\trigeminy_1754543043205.json\"   \n",
    "# input_json = r\"simulator\\fluke\\3d av_1754545068278.json\"   \n",
    "input_json = r\"simulator\\fluke\\asystole_1754544406847.json\"   \n",
    "\n",
    "with open(input_json, 'r') as file:\n",
    "    file_data = json.load(file)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# input_json = r\"v01_prob\\teton_ecg.ecgdatas.json\"  ####\n",
    "# with open(input_json, 'r') as file:\n",
    "#     all_id_data = json.load(file)\n",
    "\n",
    "# file_data = all_id_data[3]['ecgValue']   \n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"bpms\\afib_1766471694144.json\"\n",
    "# # input_json = r\"bpms\\bigeminy_1766467666407.json\"\n",
    "# input_json = r\"bpms\\pvc 6_1766467718685.json\"    ########\n",
    "# # input_json = r\"bpms\\tri_1766467618314.json\"\n",
    "# # input_json = r\"v01_prob/220_1767858669130.json\"\n",
    "# # input_json = r\"v01_prob/240bpm_1767858615562.json\"\n",
    "# # input_json = r\"v01_prob\\25 contec_1768375918389.json\"\n",
    "# # input_json = r\"v01_prob\\30bpm contec_1768375716454.json\"\n",
    "# # input_json = r\"v01_prob\\2d av_1754545008828.json\"  #####\n",
    "# # input_json = r\"v01_prob\\3rd_davb_1768554217066.json\"  #####\n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"exception\\L2_1759207950416.json\"  #####\n",
    "# input_json = r\"0_bpm\\L2_1760767200872.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760254470484.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760354748658.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760770290911.json\"  \n",
    "# # input_json = r\"issues\\L2_1757064122874.json\"  #####\n",
    "# # input_json = r\"v01_prob\\run 5 pvc.json\"  #####\n",
    "# # input_json = r\"issues\\L2_1757579288752.json\"\n",
    "# # input_json = r\"issues\\L2_1757737806463.json\"  #####\n",
    "# # input_json = r\"v01_prob\\L2_1765984517025.json\"  #####\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "# doubles = []\n",
    "# with open(input_json, \"rb\") as f:\n",
    "#     while chunk := f.read(8):\n",
    "#         if len(chunk) < 8:\n",
    "#             break\n",
    "#         value = struct.unpack(\"<d\", chunk)[0]\n",
    "#         doubles.append(value)\n",
    "\n",
    "# file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "#     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "#     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     return json.loads(decoded_string)\n",
    "\n",
    "# # input_json = r\"NHF2\\DATA_1750689015865.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750689460556.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750851207409.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750858856842.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750862721789.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "# # file_data = decrypt(input_json)\n",
    "\n",
    "# # input_json = r\"NHF\\DATA_1752067426678.json\"  #####\n",
    "# input_json = r\"NHF\\DATA_1752121970835.json\"  ########\n",
    "# # input_json = r\"NHF\\DATA_1754709586876.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted CSV file using AES ECB mode\"\"\"\n",
    "#     Private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    \n",
    "#     cipher = AES.new(Private_key.encode(), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     cipher = AES.new(Private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     data_list = decoded_string.split(\",\")\n",
    "#     float_list = [float(x) for x in data_list]\n",
    "\n",
    "#     return float_list\n",
    "\n",
    "# selected_path = \"v01_prob\\ECG_1735798172211.csv\"  ####\n",
    "# # selected_path = \"v01_prob\\ECG_L2_1738637533455.csv\"\n",
    "# file_data = decrypt(selected_path)\n",
    "# file_data = {'dataL2': file_data}\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def low_pass_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "\n",
    "def notch_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "    \n",
    "\n",
    "# data = data_process(file_data)\n",
    "data = data_process(low_pass_filter(notch_filter(file_data)))\n",
    "\n",
    "ecg_full = data[0, :15000, 0]\n",
    "# ecg_full = data[0, :15000, 0]\n",
    "sampling_rate = 500\n",
    "\n",
    "results = process_ecg_segments(\n",
    "    ecg_data=ecg_full,\n",
    "    sampling_rate=sampling_rate,\n",
    "    num_segments=4,\n",
    "    min_segment_length=4500\n",
    ")\n",
    "\n",
    "\n",
    "plot_ecg_segments(ecg_full, sampling_rate, results, \"ECG Analysis: 7 Segments with R-peak Detection\")\n",
    "\n",
    "# 2. Run the Full Data Plot (New logic)\n",
    "\n",
    "print(\"\\n--- Plotting Full Data ---\")\n",
    "plot_full_ecg(ecg_full, sampling_rate, \"Final Full Data View\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a296b3",
   "metadata": {},
   "source": [
    "## bpm normal & death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pywt\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64   \n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "\n",
    "# Define filter coefficients if not defined\n",
    "fs = 500  # sampling rate\n",
    "nyq = 0.5 * fs\n",
    "\n",
    "# Example low pass filter (cutoff 40 Hz)\n",
    "low_cutoff = 40 / nyq\n",
    "b_lp, a_lp = sp_signal.butter(4, low_cutoff, btype='low')\n",
    "\n",
    "# Example notch filter (50 Hz)\n",
    "q = 30\n",
    "w0 = 50 / nyq\n",
    "b_notch, a_notch = sp_signal.iirnotch(w0, q)\n",
    "\n",
    "def decrypt(input_file):\n",
    "    \"\"\"Decrypt encrypted JSON file\"\"\"\n",
    "    private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    with open(input_file, 'rb') as f:\n",
    "        encrypted_data = f.read()\n",
    "    enc = base64.b64decode(encrypted_data[24:])\n",
    "    data = unpad(cipher.decrypt(enc), 16)\n",
    "    decoded_string = data.decode('utf-8')\n",
    "    return json.loads(decoded_string)\n",
    "\n",
    "    \n",
    "def baseline_wander(X):\n",
    "    def get_median_filter_width(sampling_rate, duration):\n",
    "        res = int(sampling_rate * duration)\n",
    "        res += (res % 2) - 1\n",
    "        return res\n",
    "\n",
    "    ms_flt_array = [0.2, 0.6]\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype=\"int\")\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(500, ms_flt_array[i])\n",
    "    X0 = X\n",
    "    for mi in range(0, len(mfa)):\n",
    "        X0 = sp_signal.medfilt(X0, mfa[mi])\n",
    "    X0 = np.subtract(X, X0)\n",
    "    return X0\n",
    "\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    \"\"\"Normalize signal to range [0, 1].\"\"\"\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    data = normalize(signal_data, min_val, max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_process(filename):\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "    \n",
    "    for key in keys:\n",
    "        sig = np.array(filename[key])\n",
    "        datas.append(sig.astype('float32'))\n",
    "    \n",
    "    datas_array = np.array(datas)               # shape: (1, length) or (channels, length)\n",
    "    \n",
    "    # ── Compute real (raw) statistics here ───────────────────────────────\n",
    "    raw_min   = np.min(datas_array)\n",
    "    raw_max   = np.max(datas_array)\n",
    "    raw_mean  = np.mean(datas_array)\n",
    "    raw_std   = np.std(datas_array)\n",
    "    raw_var   = np.var(datas_array)\n",
    "    raw_median = np.median(datas_array)\n",
    "    \n",
    "    print(\"\\nRaw (pre-normalized) signal statistics:\")\n",
    "    print(f\"  Min    = {raw_min:12.4f}\")\n",
    "    print(f\"  Max    = {raw_max:12.4f}\")\n",
    "    print(f\"  Mean   = {raw_mean:12.4f}\")\n",
    "    print(f\"  Std    = {raw_std:12.4f}\")\n",
    "    print(f\"  Var    = {raw_var:14.6f}\")\n",
    "    print(f\"  Median = {raw_median:12.4f}\")\n",
    "    print(f\"  Range  = {raw_max - raw_min:.4f}\\n\")\n",
    "    \n",
    "    # Now do normalization (your existing code)\n",
    "    min_val = raw_min\n",
    "    max_val = raw_max\n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(normalize(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    \n",
    "    return final_data, {\n",
    "        'raw_min': raw_min, 'raw_max': raw_max, 'raw_mean': raw_mean,\n",
    "        'raw_std': raw_std, 'raw_var': raw_var, 'raw_median': raw_median,\n",
    "        'raw_range': raw_max - raw_min\n",
    "    }, datas_array[0]  # return flattened raw for simplicity\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0: \n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        # If we have 2-3 outlier peaks, remove them\n",
    "        # if 2 <= high_amp_count <= 3:\n",
    "        # if 3 <= high_amp_count <= 4 and len(peaks) - high_amp_count > 0:\n",
    "        # if 5 <= high_amp_count <= 6 and len(peaks) - high_amp_count > 0:\n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            # mask[high_amp_indices] = False\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            # Check if this could be a T-wave (160-450ms after R-peak)\n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                # T-waves are typically smaller and wider\n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    # Measure width at half maximum\n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    # T-waves are wider than QRS complexes\n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    \"\"\"Multi-strategy robust QRS detection for difficult cases\"\"\"\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated, \n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None, raw_segment=None):\n",
    "    if raw_segment is not None:\n",
    "        var_raw = np.var(raw_segment)\n",
    "        # if var_raw < 0.0095:                  \n",
    "        if var_raw < 0.005:                  \n",
    "            print(f\"Raw variance {var_raw:.6f} < 0.0095 → treating as asystole / flatline\")\n",
    "            return data, np.array([]), 0.0, np.array([])\n",
    "    else:\n",
    "        var = np.var(data)\n",
    "        if var < 0.00015:                     \n",
    "            print(f\"Normalized variance {var:.6f} too low → possible asystole\")\n",
    "            return data, np.array([]), 0.0, np.array([])\n",
    "\n",
    "    # data_clean = baseline_wander(data) \n",
    "\n",
    "    data_clean = data \n",
    "\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "            \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # =================================================================\n",
    "    # CRITICAL FIX: GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    # =================================================================\n",
    "    if len(cleaned_r) >= 2:\n",
    "        # Calculate reference height (Median of existing peaks)\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        # Only fill gaps if median_rr suggests a normal rhythm (< 1.5s).\n",
    "        # If median_rr is already 2.0s (bradycardia), huge gaps are normal.\n",
    "        if median_rr < 1.5: \n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                        \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    # Lower threshold slightly for gap search\n",
    "                    low_thresh = mean_val * 0.6 \n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                            \n",
    "                        # 1. Check Sharpness\n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            # 2. Refine Position\n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            # 3. AMPLITUDE CHECK (The Fix)\n",
    "                            # Even if it's sharp, is it tall enough?\n",
    "                            # AV Block P-waves are sharp but short.\n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            # Must be at least 40-50% of the median R-peak height\n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "\n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # =================================================================\n",
    "\n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    # Relaxed expectations for Bradycardia/AV Block\n",
    "    min_expected_peaks = int(30/60 * segment_duration) \n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    # Fallback to robust only if counts are extremely off\n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        # Valid intervals widened to account for Bradycardia/Pauses\n",
    "        # valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 3.5)] \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)] \n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_data, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    max_len = len(ecg_data)\n",
    "    \n",
    "    if num_segments > 1:\n",
    "        window_step = (max_len - min_segment_length) / (num_segments - 1)\n",
    "        window_step = round(window_step)\n",
    "    else:\n",
    "        window_step = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = start_idx + min_segment_length\n",
    "        \n",
    "        if end_idx > max_len:\n",
    "            start_idx = max_len - min_segment_length\n",
    "            end_idx = max_len\n",
    "            \n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            end_idx = min(min_segment_length, max_len)\n",
    "        \n",
    "        segment = ecg_data[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment) < 100:\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        segment_duration = len(segment) / sampling_rate\n",
    "        # ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(segment, sampling_rate, segment_duration)\n",
    "\n",
    "        raw_segment = raw_ecg[start_idx:end_idx]   # ← the real raw amplitudes\n",
    "        ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(\n",
    "            segment,\n",
    "            sampling_rate,\n",
    "            segment_duration,\n",
    "            raw_segment=raw_segment                # ← pass raw here\n",
    "        )\n",
    "\n",
    "        print(f\"Segment {i+1}: Detected {len(r_peaks)} R-peaks, BPM: {bpm:.1f}\")\n",
    "        \n",
    "        adjusted_r_peaks = r_peaks + start_idx if len(r_peaks) > 0 else np.array([])\n",
    "        adjusted_cleaned_r = np.array(cleaned_r) + start_idx if len(cleaned_r) > 0 else np.array([])\n",
    "        \n",
    "        results.append({\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_filtered': ecg_filtered,\n",
    "            'r_peaks': adjusted_r_peaks,\n",
    "            'bpm': bpm,\n",
    "            'cleaned_r': adjusted_cleaned_r,\n",
    "            'ecg_raw': segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_ecg_stats(signal, fs=500):\n",
    "    \"\"\"Compute common statistics for an ECG segment\"\"\"\n",
    "    if len(signal) == 0:\n",
    "        return {\n",
    "            'nsamples': 0,\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'var': np.nan,\n",
    "            'min': np.nan,\n",
    "            'max': np.nan,\n",
    "            'median': np.nan,\n",
    "            'rms': np.nan,\n",
    "            'duration_s': 0.0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'nsamples': len(signal),\n",
    "        'mean': float(np.mean(signal)),\n",
    "        'std': float(np.std(signal)),\n",
    "        'var': float(np.var(signal)),\n",
    "        'min': float(np.min(signal)),\n",
    "        'max': float(np.max(signal)),\n",
    "        'median': float(np.median(signal)),\n",
    "        'rms': float(np.sqrt(np.mean(signal**2))),\n",
    "        'duration_s': len(signal) / fs\n",
    "    }\n",
    "\n",
    "\n",
    "def format_stats_text(stats, prefix=\"\"):\n",
    "    \"\"\"Create a compact multi-line stats string for plotting\"\"\"\n",
    "    lines = [\n",
    "        f\"{prefix}Duration: {stats['duration_s']:.2f} s\",\n",
    "        f\"Samples:   {stats['nsamples']}\",\n",
    "        f\"Mean:      {stats['mean']:.4f}\",\n",
    "        f\"Std:       {stats['std']:.4f}\",\n",
    "        f\"Var:       {stats['var']:.6f}\",\n",
    "        f\"Min / Max: {stats['min']:.4f} / {stats['max']:.4f}\",\n",
    "        f\"Median:    {stats['median']:.4f}\",\n",
    "        f\"RMS:       {stats['rms']:.4f}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def plot_ecg_segments(ecg_data, sampling_rate, results, title=\"ECG Segments with R-peaks and BPM\", raw_ecg=None):\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(15, 3.5 * num_segments), sharex=False)\n",
    "    \n",
    "    if num_segments == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    global_stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "    fig.suptitle(f\"{title}\\nFull signal stats: {global_stats['duration_s']:.1f}s | \"\n",
    "                 f\"mean={global_stats['mean']:.4f}  std={global_stats['std']:.4f}\", \n",
    "                 fontsize=13, y=0.98)\n",
    "    \n",
    "    for i, (ax, result) in enumerate(zip(axes, results)):\n",
    "        segment_num = result['segment_num']\n",
    "        start_idx = result['start_idx']\n",
    "        end_idx = result['end_idx']\n",
    "        bpm = result['bpm']\n",
    "        r_peaks = result['r_peaks']\n",
    "        \n",
    "        segment_time = time[start_idx:end_idx]\n",
    "        segment_data = result['ecg_raw']\n",
    "        \n",
    "        ax.plot(segment_time, segment_data, 'b-', alpha=0.8, linewidth=1.1, label='ECG')\n",
    "        \n",
    "        if len(r_peaks) > 0:\n",
    "            r_times = r_peaks / sampling_rate\n",
    "            r_values = ecg_data[r_peaks.astype(int)]\n",
    "            ax.plot(r_times, r_values, 'ro', markersize=7, label='R-peaks', alpha=0.85)\n",
    "        \n",
    "        # ── Statistics box per segment (use raw if available) ───────────────────────────────\n",
    "        if raw_ecg is not None:\n",
    "            raw_segment = raw_ecg[start_idx:end_idx]\n",
    "            seg_stats = compute_ecg_stats(raw_segment, sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            seg_stats = compute_ecg_stats(segment_data, sampling_rate)\n",
    "            prefix = \"\"\n",
    "        stats_text = format_stats_text(seg_stats, prefix + f\"Seg {segment_num}  \")\n",
    "        stats_text += f\"\\nBPM:       {bpm:.1f}\"\n",
    "        \n",
    "        ax.text(0.02, 0.98, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=9.5,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.82, edgecolor='gray', boxstyle='round,pad=0.4'))\n",
    "        \n",
    "        segment_duration = (end_idx - start_idx) / sampling_rate\n",
    "        ax.set_title(f'Segment {segment_num}: {start_idx:,} – {end_idx:,}  |  BPM: {bpm:.1f}')\n",
    "        ax.set_ylabel('Amplitude (norm)')\n",
    "        ax.grid(True, alpha=0.35, linestyle='--')\n",
    "        ax.set_xlim([segment_time[0], segment_time[-1]])\n",
    "        ax.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (seconds)')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])   # make room for suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # ── Console summary ───────────────────────────────────────────────\n",
    "    print(\"═\" * 70)\n",
    "    print(\"ECG SEGMENT STATISTICS SUMMARY\")\n",
    "    print(\"═\" * 70)\n",
    "    for res in results:\n",
    "        if raw_ecg is not None:\n",
    "            s = compute_ecg_stats(raw_ecg[res['start_idx']:res['end_idx']], sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            s = compute_ecg_stats(res['ecg_raw'], sampling_rate)\n",
    "            prefix = \"Norm \"\n",
    "        print(f\"Segment {res['segment_num']:2d} | {s['duration_s']:5.2f}s | \"\n",
    "              f\"mean={s['mean']:8.4f}  std={s['std']:7.4f}  BPM={res['bpm']:5.1f} ({prefix.strip()})\"\n",
    "            )\n",
    "    print(\"═\" * 70)\n",
    "    \n",
    "    \n",
    "def plot_full_ecg(ecg_data, sampling_rate, title=\"Full ECG Signal Analysis\", raw_ecg=None):\n",
    "    \"\"\"\n",
    "    Runs detection on the entire dataset and plots a single continuous view.\n",
    "    \"\"\"\n",
    "    # _, r_peaks, global_bpm, _ = qrs_detect(ecg_data, sampling_rate)\n",
    "    _, r_peaks, global_bpm, _ = qrs_detect(\n",
    "        ecg_data,\n",
    "        sampling_rate,\n",
    "        raw_segment=raw_ecg[:len(ecg_data)]    # pass corresponding raw part\n",
    "    )\n",
    "        \n",
    "    if raw_ecg is not None:\n",
    "        stats = compute_ecg_stats(raw_ecg[:len(ecg_data)], sampling_rate)\n",
    "        prefix = \"Raw \"\n",
    "    else:\n",
    "        stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "        prefix = \"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 6)) # Width of 20 makes the 15k samples readable\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    # Plot the signal\n",
    "    plt.plot(time_axis, ecg_data, 'b-', linewidth=0.8, alpha=0.8, label='Filtered ECG')\n",
    "    \n",
    "    # Plot the peaks\n",
    "    if len(r_peaks) > 0:\n",
    "        # Filter out peaks that might be out of bounds (safety check)\n",
    "        valid_peaks = r_peaks[r_peaks < len(ecg_data)].astype(int)\n",
    "        \n",
    "        peak_times = valid_peaks / sampling_rate\n",
    "        peak_values = ecg_data[valid_peaks]\n",
    "        \n",
    "        plt.plot(peak_times, peak_values, 'ro', markersize=4, label='R-peaks')\n",
    "        \n",
    "        # Optional: Annotate every 5th peak to help navigation\n",
    "        for i, (t, v) in enumerate(zip(peak_times, peak_values)):\n",
    "            if i % 5 == 0:\n",
    "                plt.annotate(f'{t:.1f}s', (t, v), xytext=(0, 10), \n",
    "                             textcoords='offset points', ha='center', fontsize=8, color='red')\n",
    "\n",
    "    plt.title(f\"{title} | Global BPM: {global_bpm:.1f} | Total Peaks: {len(r_peaks)}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Normalized Amplitude\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Global Analysis: {len(r_peaks)} peaks detected over {len(ecg_data)/sampling_rate:.2f} seconds.\")\n",
    "    print(f\"{prefix}Full signal stats →  mean={stats['mean']:.4f}  std={stats['std']:.4f}  var={stats['var']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    " \n",
    "# input_json = r\"simulator\\contec\\trigeminy_1756103085272.json\" \n",
    "input_json = r\"simulator\\contec\\asystl_1756103447146.json\" \n",
    "# input_json = r\"simulator\\contec\\1d av_1756104504294.json\"  \n",
    "# input_json = r\"simulator\\contec\\3d av_1756104633918.json\"  \n",
    "# input_json = r\"simulator\\contec\\280bpm_1756100716422.json\" \n",
    "# input_json = r\"simulator\\contec\\av sequence_1756106676125.json\"  #####\n",
    "# input_json = r\"simulator\\contec\\dmnd freq_1756106571373.json\"  \n",
    "#    \n",
    "# input_json = r\"simulator\\fluke\\trigeminy_1754543043205.json\"   \n",
    "# input_json = r\"simulator\\fluke\\3d av_1754545068278.json\"   \n",
    "# input_json = r\"simulator\\fluke\\asystole_1754544406847.json\"   \n",
    "\n",
    "with open(input_json, 'r') as file:\n",
    "    file_data = json.load(file)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# input_json = r\"v01_prob\\teton_ecg.ecgdatas.json\"  ####\n",
    "# with open(input_json, 'r') as file:\n",
    "#     all_id_data = json.load(file)\n",
    "\n",
    "# file_data = all_id_data[3]['ecgValue']   \n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"bpms\\afib_1766471694144.json\"\n",
    "# # input_json = r\"bpms\\bigeminy_1766467666407.json\"\n",
    "# # input_json = r\"bpms\\pvc 6_1766467718685.json\"    ########\n",
    "# # input_json = r\"bpms\\tri_1766467618314.json\"\n",
    "# # input_json = r\"v01_prob/220_1767858669130.json\"\n",
    "# # input_json = r\"v01_prob/240bpm_1767858615562.json\"\n",
    "# # input_json = r\"v01_prob\\25 contec_1768375918389.json\"\n",
    "# # input_json = r\"v01_prob\\30bpm contec_1768375716454.json\"\n",
    "# # input_json = r\"v01_prob\\2d av_1754545008828.json\"  #####\n",
    "# input_json = r\"v01_prob\\3rd_davb_1768554217066.json\"  #####\n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# input_json = r\"exception\\L2_1759207950416.json\"  #####\n",
    "# # input_json = r\"0_bpm\\L2_1760767200872.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760254470484.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760354748658.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760770290911.json\"  \n",
    "# # input_json = r\"issues\\L2_1757064122874.json\"  #####\n",
    "# # input_json = r\"v01_prob\\run 5 pvc.json\"  #####\n",
    "# # input_json = r\"issues\\L2_1757579288752.json\"\n",
    "# # input_json = r\"issues\\L2_1757737806463.json\"  #####\n",
    "# # input_json = r\"v01_prob\\L2_1765984517025.json\"  #####\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "# doubles = []\n",
    "# with open(input_json, \"rb\") as f:\n",
    "#     while chunk := f.read(8):\n",
    "#         if len(chunk) < 8:\n",
    "#             break\n",
    "#         value = struct.unpack(\"<d\", chunk)[0]\n",
    "#         doubles.append(value)\n",
    "\n",
    "# file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "#     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "#     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     return json.loads(decoded_string)\n",
    "\n",
    "# # input_json = r\"NHF2\\DATA_1750689015865.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750689460556.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750851207409.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750858856842.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750862721789.json\"\n",
    "# input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "# input_json = r\"NHF\\DATA_1752067426678.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1752121970835.json\"  ########\n",
    "# # input_json = r\"NHF\\DATA_1754709586876.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted CSV file using AES ECB mode\"\"\"\n",
    "#     Private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    \n",
    "#     cipher = AES.new(Private_key.encode(), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     cipher = AES.new(Private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     data_list = decoded_string.split(\",\")\n",
    "#     float_list = [float(x) for x in data_list]\n",
    "\n",
    "#     return float_list\n",
    "\n",
    "# # selected_path = \"v01_prob\\ECG_1735798172211.csv\"  ####\n",
    "# selected_path = \"v01_prob\\ECG_L2_1738637533455.csv\"\n",
    "# file_data = decrypt(selected_path)\n",
    "# file_data = {'dataL2': file_data}\n",
    "\n",
    "\n",
    "def low_pass_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "\n",
    "def notch_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "    \n",
    "\n",
    "# data = data_process(file_data)\n",
    "processed_data, raw_global_stats, raw_ecg = data_process(\n",
    "    low_pass_filter(notch_filter(file_data))\n",
    ")\n",
    "\n",
    "ecg_full = processed_data[0, :15000, 0]\n",
    "sampling_rate = 500\n",
    "\n",
    "results = process_ecg_segments(\n",
    "    ecg_data=ecg_full,\n",
    "    sampling_rate=sampling_rate,\n",
    "    num_segments=4,\n",
    "    min_segment_length=4500\n",
    ")\n",
    "\n",
    "\n",
    "plot_ecg_segments(ecg_full, sampling_rate, results, \"ECG Analysis: 4 Segments with R-peak Detection\", raw_ecg=raw_ecg)\n",
    "\n",
    "print(\"\\n--- Plotting Full Data ---\")\n",
    "plot_full_ecg(ecg_full, sampling_rate, \"Final Full Data View\", raw_ecg=raw_ecg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5194db",
   "metadata": {},
   "source": [
    "## interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9306e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64    \n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "\n",
    "# ==========================================\n",
    "# 1. UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def decrypt(input_file):\n",
    "    \"\"\"Decrypt encrypted JSON file\"\"\"\n",
    "    private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    try:\n",
    "        cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "        with open(input_file, 'rb') as f:\n",
    "            encrypted_data = f.read()\n",
    "        enc = base64.b64decode(encrypted_data[24:])\n",
    "        data = unpad(cipher.decrypt(enc), 16)\n",
    "        decoded_string = data.decode('utf-8')\n",
    "        return json.loads(decoded_string)\n",
    "    except Exception as e:\n",
    "        print(f\"Decryption failed or file is plain JSON: {e}\")\n",
    "        with open(input_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def baseline_wander(X):\n",
    "    def get_median_filter_width(sampling_rate, duration):\n",
    "        res = int(sampling_rate * duration)\n",
    "        res += (res % 2) - 1\n",
    "        return res\n",
    "\n",
    "    ms_flt_array = [0.2, 0.6]\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype=\"int\")\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(500, ms_flt_array[i])\n",
    "    X0 = X\n",
    "    for mi in range(0, len(mfa)):\n",
    "        X0 = sp_signal.medfilt(X0, mfa[mi])\n",
    "    X0 = np.subtract(X, X0)\n",
    "    return X0\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    return normalize(signal_data, min_val, max_val)\n",
    "\n",
    "def data_process(filename):\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "   \n",
    "    if 'dataL2' in filename:\n",
    "        raw_data = filename['dataL2']\n",
    "    else:\n",
    "        raw_data = filename\n",
    "       \n",
    "    sig = np.array(raw_data)\n",
    "    datas.append(sig.astype('float32'))\n",
    "   \n",
    "    datas_array = np.array(datas)\n",
    "    min_val = np.min(datas_array)\n",
    "    max_val = np.max(datas_array)\n",
    "   \n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(process_signal(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    return final_data\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            mask[high_amp_indices] = True           # ← note: this line keeps outliers (maybe you meant False?)\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    # ────────────────────────────────────────────────\n",
    "    # Your full multi-strategy robust detection function\n",
    "    # (copy-pasted exactly as you provided)\n",
    "    # ────────────────────────────────────────────────\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated,\n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None):\n",
    "    \"\"\"\n",
    "    Enhanced QRS detection with Amplitude Guardrails for AV Blocks\n",
    "    ← This is the ONLY function you asked to replace\n",
    "    \"\"\"\n",
    "    # Apply baseline wander removal\n",
    "    # data_clean = baseline_wander(data)\n",
    "    data_clean = data  # Keeping your override\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "        \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # CRITICAL FIX: GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    if len(cleaned_r) >= 2:\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        if median_rr < 1.5:\n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                    \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    low_thresh = mean_val * 0.6\n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                        \n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "        \n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    min_expected_peaks = int(30/60 * segment_duration)\n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)]\n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "# ==========================================\n",
    "# 3. ENHANCED P-WAVE DETECTION\n",
    "# ==========================================\n",
    "\n",
    "def adaptive_noise_filter(segment, sampling_rate):\n",
    "    \"\"\"Apply stronger filtering in noisy regions\"\"\"\n",
    "    # Calculate local noise level\n",
    "    diff = np.diff(segment)\n",
    "    noise_std = np.std(diff)\n",
    "    \n",
    "    if noise_std > 0.05:  # Noisy segment\n",
    "        # Apply stronger low-pass filter\n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        low = 1 / nyquist\n",
    "        high = 25 / nyquist\n",
    "        b, a = sp_signal.butter(3, [low, high], btype='band')\n",
    "        return sp_signal.filtfilt(b, a, segment)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "def calculate_signal_quality(segment):\n",
    "    \"\"\"Calculate signal quality index (0-1, higher is better)\"\"\"\n",
    "    # Multiple quality metrics\n",
    "    \n",
    "    # 1. Noise level (based on high-frequency content)\n",
    "    diff = np.diff(segment)\n",
    "    noise_level = np.std(diff)\n",
    "    noise_score = np.exp(-noise_level / 0.1)  # Lower noise = higher score\n",
    "    \n",
    "    # 2. Baseline stability\n",
    "    baseline_drift = np.std(segment)\n",
    "    drift_score = np.exp(-baseline_drift / 0.3)\n",
    "    \n",
    "    # 3. Signal amplitude (not too high, not too low)\n",
    "    signal_range = np.max(segment) - np.min(segment)\n",
    "    if 0.2 <= signal_range <= 1.5:\n",
    "        amplitude_score = 1.0\n",
    "    else:\n",
    "        amplitude_score = 0.5\n",
    "    \n",
    "    # Combined quality index\n",
    "    quality = (noise_score * 0.4 + drift_score * 0.4 + amplitude_score * 0.2)\n",
    "    \n",
    "    return quality\n",
    "\n",
    "def enhanced_p_wave_detection(signal, r_peaks, sampling_rate, segment_num):\n",
    "    \"\"\"\n",
    "    Enhanced P-wave detection with adaptive handling for different heart rates\n",
    "    \"\"\"\n",
    "    if len(r_peaks) < 3:\n",
    "        return np.full(len(r_peaks), np.nan)\n",
    "    \n",
    "    # Calculate signal quality for adaptive thresholds\n",
    "    signal_quality = calculate_signal_quality(signal)\n",
    "    \n",
    "    # Initialize arrays\n",
    "    p_peaks = np.full(len(r_peaks), np.nan)\n",
    "    p_qualities = np.zeros(len(r_peaks))\n",
    "    \n",
    "    # Calculate average heart rate to adapt strategy\n",
    "    if len(r_peaks) > 1:\n",
    "        rr_intervals = np.diff(r_peaks) / sampling_rate\n",
    "        avg_rr = np.mean(rr_intervals)\n",
    "        avg_hr = 60 / avg_rr if avg_rr > 0 else 0\n",
    "    else:\n",
    "        avg_hr = 0\n",
    "    \n",
    "    print(f\"Segment {segment_num}: Signal Quality = {signal_quality:.2f}, Avg HR = {avg_hr:.0f} BPM\")\n",
    "    \n",
    "    # Adaptive parameters based on heart rate\n",
    "    if avg_hr > 180:  # Very high heart rate (tachycardia)\n",
    "        print(f\"  High heart rate detected - using adaptive short-cycle parameters\")\n",
    "        use_adaptive_short_cycle = True\n",
    "        min_quality_threshold = 20  # Lower threshold for high HR\n",
    "    elif avg_hr > 120:  # Elevated heart rate\n",
    "        use_adaptive_short_cycle = True\n",
    "        min_quality_threshold = 30\n",
    "    else:  # Normal to low heart rate\n",
    "        use_adaptive_short_cycle = False\n",
    "        if signal_quality > 0.7:\n",
    "            min_quality_threshold = 50\n",
    "        elif signal_quality > 0.5:\n",
    "            min_quality_threshold = 35\n",
    "        else:\n",
    "            min_quality_threshold = 25\n",
    "    \n",
    "    # Calculate global statistics from first pass\n",
    "    preliminary_pr_intervals = []\n",
    "    preliminary_p_amps = []\n",
    "    \n",
    "    # First pass: Estimate T-wave end positions adaptively\n",
    "    t_wave_ends = []\n",
    "    for i in range(len(r_peaks) - 1):\n",
    "        r_curr = int(r_peaks[i])\n",
    "        r_next = int(r_peaks[i + 1])\n",
    "        rr_interval = r_next - r_curr\n",
    "        \n",
    "        # ADAPTIVE T-wave end estimation based on RR interval\n",
    "        if rr_interval < 0.4 * sampling_rate:  # RR < 400ms (HR > 150 BPM)\n",
    "            # At high HR, T-wave is much shorter (ends at ~40-50% of RR)\n",
    "            estimated_t_end = r_curr + int(0.45 * rr_interval)\n",
    "        elif rr_interval < 0.6 * sampling_rate:  # RR < 600ms (HR > 100 BPM)\n",
    "            # Moderate HR, T-wave ends at ~55% of RR\n",
    "            estimated_t_end = r_curr + int(0.55 * rr_interval)\n",
    "        else:  # Normal to slow HR\n",
    "            # T-wave ends at ~60-70% of RR, but cap at 500ms\n",
    "            estimated_t_end = r_curr + min(int(0.5 * sampling_rate), int(0.65 * rr_interval))\n",
    "        \n",
    "        t_wave_ends.append(estimated_t_end)\n",
    "    \n",
    "    # Add a final T-wave end for the last R-peak\n",
    "    if len(r_peaks) > 0:\n",
    "        last_r = int(r_peaks[-1])\n",
    "        if len(r_peaks) > 1:\n",
    "            last_rr = r_peaks[-1] - r_peaks[-2]\n",
    "            if last_rr < 0.4 * sampling_rate:\n",
    "                t_wave_ends.append(last_r + int(0.45 * last_rr))\n",
    "            else:\n",
    "                t_wave_ends.append(last_r + int(0.5 * sampling_rate))\n",
    "        else:\n",
    "            t_wave_ends.append(last_r + int(0.5 * sampling_rate))\n",
    "    \n",
    "    # First pass: detect all possible P candidates\n",
    "    for i, r in enumerate(r_peaks):\n",
    "        if i == 0:\n",
    "            continue\n",
    "            \n",
    "        r = int(r)\n",
    "        rr_prev = r - int(r_peaks[i-1])\n",
    "        \n",
    "        # ADAPTIVE search window based on RR interval\n",
    "        if use_adaptive_short_cycle and rr_prev < 0.5 * sampling_rate:  # RR < 500ms\n",
    "            # Very short cycle - minimal T-wave time\n",
    "            # Start search immediately after estimated T-wave end\n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                t_end_prev = t_wave_ends[i - 1]\n",
    "                # Reduce safety margin for high HR (20ms instead of 50ms)\n",
    "                search_start = t_end_prev + int(0.02 * sampling_rate)\n",
    "            else:\n",
    "                # At high HR, P-wave can be very close to previous R\n",
    "                search_start = int(r_peaks[i-1] + 0.25 * rr_prev)\n",
    "            \n",
    "            # End search closer to QRS (20ms before instead of 30ms)\n",
    "            search_end = int(r - 0.02 * sampling_rate)\n",
    "            \n",
    "            # Minimum PR interval at high HR can be as short as 100ms\n",
    "            min_pr_ms = 80\n",
    "            max_pr_ms = 300\n",
    "            \n",
    "        else:  # Normal heart rate\n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                t_end_prev = t_wave_ends[i - 1]\n",
    "                search_start = t_end_prev + int(0.05 * sampling_rate)\n",
    "            else:\n",
    "                search_start = int(r_peaks[i-1] + 0.4 * sampling_rate)\n",
    "            \n",
    "            search_end = int(r - 0.03 * sampling_rate)\n",
    "            min_pr_ms = 80\n",
    "            max_pr_ms = 400\n",
    "        \n",
    "        # Ensure valid window\n",
    "        search_start = max(0, search_start)\n",
    "        search_end = min(len(signal)-1, search_end)\n",
    "        \n",
    "        # Check if window is valid\n",
    "        min_window_size = int(0.05 * sampling_rate) if use_adaptive_short_cycle else int(0.08 * sampling_rate)\n",
    "        \n",
    "        if search_end <= search_start:\n",
    "            # Window is invalid (negative or zero size)\n",
    "            # This can happen at very high HR - try a fallback approach\n",
    "            if use_adaptive_short_cycle:\n",
    "                # Use percentage-based window\n",
    "                search_start = int(r - 0.35 * rr_prev)\n",
    "                search_end = int(r - 0.02 * sampling_rate)\n",
    "                search_start = max(0, max(int(r_peaks[i-1] + 0.1 * sampling_rate), search_start))\n",
    "        \n",
    "        if search_end - search_start < min_window_size:\n",
    "            # Still too small - skip this beat\n",
    "            continue\n",
    "        \n",
    "        # Extract and filter search segment\n",
    "        segment = signal[search_start:search_end]\n",
    "        segment_filtered = adaptive_noise_filter(segment, sampling_rate)\n",
    "        \n",
    "        # Find all potential peaks with adaptive parameters\n",
    "        if use_adaptive_short_cycle:\n",
    "            # More lenient for high HR\n",
    "            min_prominence = 0.002\n",
    "            min_distance = int(0.05 * sampling_rate)  # Shorter distance\n",
    "            max_width = int(0.12 * sampling_rate)  # Narrower width expectation\n",
    "        else:\n",
    "            min_prominence = 0.003\n",
    "            min_distance = int(0.08 * sampling_rate)\n",
    "            max_width = int(0.15 * sampling_rate)\n",
    "        \n",
    "        try:\n",
    "            candidate_peaks, properties = sp_signal.find_peaks(\n",
    "                segment_filtered,\n",
    "                distance=min_distance,\n",
    "                prominence=min_prominence,\n",
    "                width=(int(0.02*sampling_rate), max_width)\n",
    "            )\n",
    "        except:\n",
    "            candidate_peaks = []\n",
    "        \n",
    "        if len(candidate_peaks) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Convert to absolute indices\n",
    "        candidate_peaks = search_start + candidate_peaks\n",
    "        \n",
    "        # Score each candidate\n",
    "        best_score = -np.inf\n",
    "        best_peak = None\n",
    "        \n",
    "        for cp in candidate_peaks:\n",
    "            cp = int(cp)\n",
    "            \n",
    "            # Check PR interval\n",
    "            pr_interval = (r - cp) / sampling_rate * 1000\n",
    "            if pr_interval < min_pr_ms or pr_interval > max_pr_ms:\n",
    "                continue\n",
    "            \n",
    "            # Double-check it's after T-wave (with tolerance for high HR)\n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                safety_margin = 0.01 * sampling_rate if use_adaptive_short_cycle else 0.02 * sampling_rate\n",
    "                if cp < (t_wave_ends[i - 1] - safety_margin):\n",
    "                    continue\n",
    "            \n",
    "            score = 0\n",
    "            \n",
    "            # 1. PR interval scoring (adjusted for HR)\n",
    "            if use_adaptive_short_cycle:\n",
    "                ideal_pr = 120  # Shorter PR at high HR\n",
    "                sigma = 40\n",
    "            else:\n",
    "                ideal_pr = 160\n",
    "                sigma = 60\n",
    "            \n",
    "            score += np.exp(-((pr_interval - ideal_pr) ** 2) / (2 * sigma ** 2)) * 150\n",
    "            \n",
    "            # 2. Amplitude scoring\n",
    "            p_amp = abs(signal[cp])\n",
    "            \n",
    "            # More lenient amplitude for high HR\n",
    "            if signal_quality < 0.5 or use_adaptive_short_cycle:\n",
    "                if p_amp < 0.015:\n",
    "                    continue\n",
    "                min_amp = 0.01\n",
    "            else:\n",
    "                if p_amp < 0.02:\n",
    "                    continue\n",
    "                min_amp = 0.015\n",
    "            \n",
    "            if min_amp <= p_amp <= 0.5:\n",
    "                ideal_amp = 0.08\n",
    "                amp_score = np.exp(-((p_amp - ideal_amp) ** 2) / (2 * 0.10 ** 2)) * 400\n",
    "                score += amp_score\n",
    "            elif p_amp > 0.5:\n",
    "                score += 50\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # 3. Morphology scoring\n",
    "            left_samples = min(5, cp)\n",
    "            right_samples = min(5, len(signal) - cp - 1)\n",
    "            if left_samples > 0 and right_samples > 0:\n",
    "                left_slope = signal[cp] - signal[cp - left_samples]\n",
    "                right_slope = signal[cp + right_samples] - signal[cp]\n",
    "                symmetry = 1 - abs(left_slope - right_slope) / (abs(left_slope) + abs(right_slope) + 1e-6)\n",
    "                score += symmetry * 60\n",
    "            \n",
    "            # 4. Width analysis\n",
    "            half_amp = p_amp / 2\n",
    "            left_width = 0\n",
    "            right_width = 0\n",
    "            \n",
    "            for offset in range(1, min(30, cp)):\n",
    "                if abs(signal[cp - offset]) >= half_amp:\n",
    "                    left_width += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            for offset in range(1, min(30, len(signal) - cp - 1)):\n",
    "                if abs(signal[cp + offset]) >= half_amp:\n",
    "                    right_width += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            total_width = left_width + right_width\n",
    "            width_ms = total_width / sampling_rate * 1000\n",
    "            \n",
    "            # Adaptive width expectations\n",
    "            if use_adaptive_short_cycle:\n",
    "                # At high HR, P-waves can be narrower\n",
    "                if 30 <= width_ms <= 100:\n",
    "                    score += 80\n",
    "                elif 100 < width_ms <= 120:\n",
    "                    score += 50\n",
    "                else:\n",
    "                    score -= 30\n",
    "            else:\n",
    "                if 40 <= width_ms <= 120:\n",
    "                    score += 80\n",
    "                elif 120 < width_ms <= 150:\n",
    "                    score += 40\n",
    "                else:\n",
    "                    score -= 50\n",
    "            \n",
    "            # 5. Distance from R-peak\n",
    "            distance_to_r = r - cp\n",
    "            min_distance_samples = int(0.06 * sampling_rate) if use_adaptive_short_cycle else int(0.08 * sampling_rate)\n",
    "            if distance_to_r > min_distance_samples:\n",
    "                score += 40\n",
    "            \n",
    "            # 6. Peak sharpness\n",
    "            if cp > 1 and cp < len(signal) - 2:\n",
    "                derivative = abs(signal[cp] - signal[cp-1]) + abs(signal[cp+1] - signal[cp])\n",
    "                second_derivative = abs(signal[cp-1] - 2*signal[cp] + signal[cp+1])\n",
    "                \n",
    "                if 0.01 < derivative < 0.3 and second_derivative < 0.2:\n",
    "                    score += 50\n",
    "                elif derivative > 0.5:\n",
    "                    score -= 30\n",
    "            \n",
    "            # 7. Local prominence\n",
    "            local_window = int(0.04 * sampling_rate) if use_adaptive_short_cycle else int(0.06 * sampling_rate)\n",
    "            local_start = max(0, cp - local_window)\n",
    "            local_end = min(len(signal), cp + local_window)\n",
    "            local_segment = signal[local_start:local_end]\n",
    "            \n",
    "            if len(local_segment) > 0:\n",
    "                local_median = np.median(np.abs(local_segment))\n",
    "                local_prominence = p_amp - local_median\n",
    "                \n",
    "                threshold = 0.01 if use_adaptive_short_cycle else 0.015\n",
    "                if local_prominence > threshold:\n",
    "                    score += min(local_prominence * 300, 60)\n",
    "                else:\n",
    "                    score -= 20\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_peak = cp\n",
    "        \n",
    "        # Accept peak if score is good enough\n",
    "        if best_peak is not None and best_score > min_quality_threshold:\n",
    "            # Final validation\n",
    "            accept_peak = True\n",
    "            \n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                # Very lenient check for high HR\n",
    "                margin = -int(0.02 * sampling_rate) if use_adaptive_short_cycle else 0\n",
    "                if best_peak < (t_wave_ends[i - 1] + margin):\n",
    "                    accept_peak = False\n",
    "            \n",
    "            if accept_peak:\n",
    "                p_peaks[i] = best_peak\n",
    "                p_qualities[i] = best_score\n",
    "                preliminary_pr_intervals.append((r - best_peak) / sampling_rate * 1000)\n",
    "                preliminary_p_amps.append(abs(signal[best_peak]))\n",
    "    \n",
    "    # Statistical refinement\n",
    "    if len(preliminary_pr_intervals) >= 5:\n",
    "        median_pr = np.median(preliminary_pr_intervals)\n",
    "        std_pr = np.std(preliminary_pr_intervals)\n",
    "        median_amp = np.median(preliminary_p_amps)\n",
    "        std_amp = np.std(preliminary_p_amps)\n",
    "        \n",
    "        # More lenient Z-score threshold for high HR\n",
    "        z_threshold = 5.0 if use_adaptive_short_cycle else 4.0\n",
    "        \n",
    "        for i in range(1, len(r_peaks)):\n",
    "            if np.isnan(p_peaks[i]) or p_qualities[i] == 0:\n",
    "                continue\n",
    "            \n",
    "            pr = (r_peaks[i] - p_peaks[i]) / sampling_rate * 1000\n",
    "            amp = abs(signal[int(p_peaks[i])])\n",
    "            \n",
    "            pr_z = abs(pr - median_pr) / (std_pr + 1e-6) if std_pr > 0 else 0\n",
    "            amp_z = abs(amp - median_amp) / (std_amp + 1e-6) if std_amp > 0 else 0\n",
    "            \n",
    "            if pr_z > z_threshold or amp_z > z_threshold:\n",
    "                p_peaks[i] = np.nan\n",
    "                p_qualities[i] = 0\n",
    "            else:\n",
    "                consistency_factor = (1.0 - 0.1 * pr_z) * (1.0 - 0.1 * amp_z)\n",
    "                p_qualities[i] *= max(0.5, consistency_factor)\n",
    "    \n",
    "    elif len(preliminary_pr_intervals) >= 2:\n",
    "        median_pr = np.median(preliminary_pr_intervals)\n",
    "        \n",
    "        for i in range(1, len(r_peaks)):\n",
    "            if np.isnan(p_peaks[i]):\n",
    "                continue\n",
    "            \n",
    "            pr = (r_peaks[i] - p_peaks[i]) / sampling_rate * 1000\n",
    "            \n",
    "            threshold = 120 if use_adaptive_short_cycle else 150\n",
    "            if abs(pr - median_pr) > threshold:\n",
    "                p_peaks[i] = np.nan\n",
    "                p_qualities[i] = 0\n",
    "    \n",
    "    # Print detection statistics\n",
    "    detected_count = np.sum(~np.isnan(p_peaks))\n",
    "    total_beats = len(r_peaks) - 1\n",
    "    detection_rate = detected_count / total_beats * 100 if total_beats > 0 else 0\n",
    "    \n",
    "    print(f\"  P-wave detection: {detected_count}/{total_beats} ({detection_rate:.1f}%)\")\n",
    "    if len(preliminary_pr_intervals) > 0:\n",
    "        avg_pr = np.mean(preliminary_pr_intervals)\n",
    "        print(f\"  Average PR interval: {avg_pr:.1f}ms\")\n",
    "    \n",
    "    return p_peaks\n",
    "\n",
    "def find_boundary(peak_idx, direction, max_search_samples, thresh_factor=0.05):\n",
    "    \"\"\"\n",
    "    Find boundary where slope flattens below thresh_factor * max_slope.\n",
    "    direction: -1 for onset (left), +1 for offset (right)\n",
    "    \"\"\"\n",
    "    signal_len = len(signal) if 'signal' in locals() else 0\n",
    "    if np.isnan(peak_idx) or signal_len == 0:\n",
    "        return np.nan\n",
    "    peak_idx = int(peak_idx)\n",
    "    limit = peak_idx + (direction * max_search_samples)\n",
    "    limit = max(0, min(signal_len, limit))\n",
    "   \n",
    "    if abs(limit - peak_idx) < 3:\n",
    "        return peak_idx\n",
    "   \n",
    "    start, end = sorted([peak_idx, limit])\n",
    "    segment = signal[start:end]\n",
    "    if direction == -1:\n",
    "        segment = segment[::-1]  # reverse so we always go \"forward\" from peak\n",
    "   \n",
    "    diff = np.diff(segment)\n",
    "    if len(diff) == 0:\n",
    "        return peak_idx\n",
    "   \n",
    "    max_slope = np.max(np.abs(diff))\n",
    "    thresh = max_slope * thresh_factor\n",
    "   \n",
    "    # Start from 1 to avoid stopping immediately at peak\n",
    "    for i in range(1, len(diff)):\n",
    "        if np.abs(diff[i]) < thresh:\n",
    "            return peak_idx + (direction * i)\n",
    "   \n",
    "    # If no flat region found → use the farthest point\n",
    "    return limit\n",
    "\n",
    "def improved_delineate_ecg_waves(signal, r_peaks, sampling_rate):\n",
    "    \"\"\"\n",
    "    Improved delineation with enhanced P-wave detection\n",
    "    \"\"\"\n",
    "    waves = {\n",
    "        'p_peak': [], 'p_onset': [], 'p_offset': [],\n",
    "        'q_peak': [], 'q_onset': [],\n",
    "        's_peak': [], 's_offset': [],\n",
    "        't_peak': [], 't_onset': [], 't_offset': []\n",
    "    }\n",
    "    \n",
    "    signal_len = len(signal)\n",
    "    \n",
    "    # Enhanced P-wave detection\n",
    "    p_peaks = enhanced_p_wave_detection(signal, r_peaks, sampling_rate, \"Segment\")\n",
    "    \n",
    "    # Helper function for boundary detection\n",
    "    def find_boundary_local(peak_idx, direction, max_search_samples, thresh_factor=0.05):\n",
    "        if np.isnan(peak_idx):\n",
    "            return np.nan\n",
    "        peak_idx = int(peak_idx)\n",
    "        limit = peak_idx + (direction * max_search_samples)\n",
    "        limit = max(0, min(signal_len, limit))\n",
    "       \n",
    "        if abs(limit - peak_idx) < 3:\n",
    "            return peak_idx\n",
    "       \n",
    "        start, end = sorted([peak_idx, limit])\n",
    "        segment = signal[start:end]\n",
    "        if direction == -1:\n",
    "            segment = segment[::-1]\n",
    "       \n",
    "        diff = np.diff(segment)\n",
    "        if len(diff) == 0:\n",
    "            return peak_idx\n",
    "       \n",
    "        max_slope = np.max(np.abs(diff))\n",
    "        thresh = max_slope * thresh_factor\n",
    "       \n",
    "        for i in range(1, len(diff)):\n",
    "            if np.abs(diff[i]) < thresh:\n",
    "                return peak_idx + (direction * i)\n",
    "       \n",
    "        return limit\n",
    "    \n",
    "    # For each R-peak, detect other waves\n",
    "    for i, r in enumerate(r_peaks):\n",
    "        r = int(r)\n",
    "        r_height = abs(signal[r]) if abs(signal[r]) > 0.05 else 1.0\n",
    "        \n",
    "        # Store P-wave results\n",
    "        p_peak_val = p_peaks[i] if i < len(p_peaks) else np.nan\n",
    "        waves['p_peak'].append(p_peak_val)\n",
    "        \n",
    "        # Calculate P boundaries if P exists\n",
    "        if not np.isnan(p_peak_val):\n",
    "            p_idx = int(p_peak_val)\n",
    "            waves['p_onset'].append(find_boundary_local(p_idx, -1, int(0.08 * sampling_rate)))  # Wider search (was 0.06)\n",
    "            waves['p_offset'].append(find_boundary_local(p_idx, 1, int(0.08 * sampling_rate)))\n",
    "        else:\n",
    "            waves['p_onset'].append(np.nan)\n",
    "            waves['p_offset'].append(np.nan)\n",
    "        \n",
    "        # Q peak & onset\n",
    "        win_q = int(0.05 * sampling_rate)\n",
    "        q_search_start = max(0, r - win_q)\n",
    "        q_window = signal[q_search_start:r]\n",
    "        q_idx = q_search_start + np.argmin(q_window) if len(q_window) > 0 else np.nan\n",
    "        waves['q_peak'].append(q_idx)\n",
    "        \n",
    "        anchor = q_idx if not np.isnan(q_idx) else r\n",
    "        waves['q_onset'].append(find_boundary_local(anchor, -1, int(0.04 * sampling_rate)))\n",
    "        \n",
    "        # S peak & offset\n",
    "        win_s = int(0.06 * sampling_rate)\n",
    "        s_search_end = min(signal_len, r + win_s)\n",
    "        s_window = signal[r:s_search_end]\n",
    "        s_idx = r + np.argmin(s_window) if len(s_window) > 0 else np.nan\n",
    "        waves['s_peak'].append(s_idx)\n",
    "        \n",
    "        anchor = s_idx if not np.isnan(s_idx) else r\n",
    "        waves['s_offset'].append(find_boundary_local(anchor, 1, int(0.04 * sampling_rate)))\n",
    "        \n",
    "        # T peak & boundaries\n",
    "        rr_next = (int(r_peaks[i+1]) - r) if i < len(r_peaks) - 1 else 1.0 * sampling_rate\n",
    "        \n",
    "        dyn_t_start = int(0.10 * sampling_rate)\n",
    "        dyn_t_end = int(min(0.600 * sampling_rate, 0.65 * rr_next))\n",
    "        \n",
    "        t_search_start = min(signal_len, r + dyn_t_start)\n",
    "        t_search_end = min(signal_len, r + dyn_t_end)\n",
    "        t_idx = np.nan\n",
    "        \n",
    "        if t_search_start < t_search_end:\n",
    "            t_window = signal[t_search_start:t_search_end]\n",
    "            if len(t_window) > 0:\n",
    "                local_peaks, _ = sp_signal.find_peaks(\n",
    "                    t_window,\n",
    "                    prominence=(0.05 * r_height)\n",
    "                )\n",
    "                if len(local_peaks) > 0:\n",
    "                    best_peak = local_peaks[np.argmax(t_window[local_peaks])]\n",
    "                    t_idx = t_search_start + best_peak\n",
    "                else:\n",
    "                    t_idx = t_search_start + np.argmax(t_window)\n",
    "        \n",
    "        waves['t_peak'].append(t_idx)\n",
    "        waves['t_onset'].append(find_boundary_local(t_idx, -1, int(0.08 * sampling_rate)))\n",
    "        \n",
    "        # Adaptive T-offset search\n",
    "        if rr_next > 1.0 * sampling_rate:\n",
    "            max_t_offset_search = int(0.280 * sampling_rate)\n",
    "            t_offset_thresh = 0.03\n",
    "        else:\n",
    "            max_t_offset_search = int(0.140 * sampling_rate)\n",
    "            t_offset_thresh = 0.05\n",
    "        \n",
    "        waves['t_offset'].append(find_boundary_local(\n",
    "            t_idx,\n",
    "            1,\n",
    "            max_t_offset_search,\n",
    "            thresh_factor=t_offset_thresh\n",
    "        ))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    for k in waves:\n",
    "        waves[k] = np.array(waves[k])\n",
    "    \n",
    "    return waves\n",
    "\n",
    "def calculate_intervals(waves, sampling_rate):\n",
    "    pr_intervals = (waves['q_onset'] - waves['p_onset']) / sampling_rate * 1000\n",
    "    qrs_durations = (waves['s_offset'] - waves['q_onset']) / sampling_rate * 1000\n",
    "    qt_intervals = (waves['t_offset'] - waves['q_onset']) / sampling_rate * 1000\n",
    "   \n",
    "    pr_intervals = np.where((pr_intervals > 40) & (pr_intervals < 600), pr_intervals, np.nan)\n",
    "    qrs_durations = np.where((qrs_durations > 30) & (qrs_durations < 200), qrs_durations, np.nan)\n",
    "    qt_intervals = np.where((qt_intervals > 100) & (qt_intervals < 600), qt_intervals, np.nan)\n",
    "   \n",
    "    return pr_intervals, qrs_durations, qt_intervals\n",
    "\n",
    "def process_ecg_segments(ecg_raw, ecg_filtered, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    \"\"\"\n",
    "    Modified function to process ECG segments with raw data for R-peak detection\n",
    "    and filtered data for wave delineation\n",
    "    \"\"\"\n",
    "    max_len = len(ecg_raw)\n",
    "    window_step = round((max_len - min_segment_length) / (num_segments - 1)) if num_segments > 1 else 0\n",
    "    results = []\n",
    "   \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = min(start_idx + min_segment_length, max_len)\n",
    "        if start_idx < 0: start_idx = 0\n",
    "       \n",
    "        # Use raw signal for R-peak detection\n",
    "        segment_raw = ecg_raw[start_idx:end_idx]\n",
    "        # Use filtered signal for wave delineation\n",
    "        segment_filtered = ecg_filtered[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment_raw) < 100: continue\n",
    "       \n",
    "        # Detect R-peaks using raw signal\n",
    "        _, r_peaks, bpm, _ = qrs_detect(segment_raw, sampling_rate, len(segment_raw)/sampling_rate)\n",
    "        \n",
    "        # Use filtered signal for wave delineation\n",
    "        waves = improved_delineate_ecg_waves(segment_filtered, r_peaks, sampling_rate)\n",
    "        pr, qrs, qt = calculate_intervals(waves, sampling_rate)\n",
    "       \n",
    "        def adj(arr):\n",
    "            if len(arr) == 0: return np.array([])\n",
    "            return arr + start_idx\n",
    "   \n",
    "        res = {\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_raw': segment_raw,  # Store raw signal for plotting\n",
    "            'ecg_filtered': segment_filtered,  # Store filtered signal for reference\n",
    "            'bpm': bpm,\n",
    "            'r_peaks': adj(r_peaks),\n",
    "            'avg_pr': np.nanmean(pr),\n",
    "            'avg_qrs': np.nanmean(qrs),\n",
    "            'avg_qt': np.nanmean(qt)\n",
    "        }\n",
    "        for k, v in waves.items():\n",
    "            res[k] = adj(v)\n",
    "           \n",
    "        results.append(res)\n",
    "   \n",
    "    return results\n",
    "\n",
    "# ==========================================\n",
    "# 4. PLOTTING FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def plot_ecg_segments(ecg_raw, ecg_filtered, sampling_rate, results, title=\"ECG Analysis\"):\n",
    "    \"\"\"\n",
    "    Modified plotting function to show both raw and filtered signals\n",
    "    \"\"\"\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(20, 4*num_segments))\n",
    "    if num_segments == 1: axes = [axes]\n",
    "   \n",
    "    time = np.arange(len(ecg_raw)) / sampling_rate\n",
    "   \n",
    "    def get_valid(indices):\n",
    "        if len(indices) == 0: return np.array([], dtype=int)\n",
    "        valid = indices[~np.isnan(indices)]\n",
    "        valid = valid[valid < len(ecg_raw)]\n",
    "        return valid.astype(int)\n",
    "\n",
    "    for i, (ax, res) in enumerate(zip(axes, results)):\n",
    "        seg_time = time[res['start_idx']:res['end_idx']]\n",
    "        \n",
    "        # Plot filtered signal (for wave visualization)\n",
    "        ax.plot(seg_time, res['ecg_filtered'], 'b-', alpha=0.7, linewidth=0.8, label='Filtered')\n",
    "        # Plot raw signal (thin line in background)\n",
    "        ax.plot(seg_time, res['ecg_raw'], 'k-', alpha=0.3, linewidth=0.5, label='Raw')\n",
    "       \n",
    "        peaks = [('r_peaks', 'ro', 'R'), ('p_peak', 'g^', 'P'), ('t_peak', 'bD', 'T')]\n",
    "        for key, style, lbl in peaks:\n",
    "            valid = get_valid(res[key])\n",
    "            if len(valid): \n",
    "                # Plot on filtered signal\n",
    "                ax.plot(time[valid], ecg_filtered[valid], style, markersize=6, label=lbl)\n",
    "\n",
    "        p_onsets = res['p_onset']\n",
    "        q_onsets = res['q_onset']\n",
    "        t_offsets = res['t_offset']\n",
    "       \n",
    "        y_min = np.min(res['ecg_filtered'])\n",
    "        bar_y_pr = y_min - 0.05\n",
    "        bar_y_qt = y_min - 0.10\n",
    "       \n",
    "        count = 0\n",
    "        for j in range(len(p_onsets)):\n",
    "            if j < len(q_onsets) and not np.isnan(p_onsets[j]) and not np.isnan(q_onsets[j]):\n",
    "                pon = int(p_onsets[j])\n",
    "                qon = int(q_onsets[j])\n",
    "                if pon < qon:\n",
    "                    ax.hlines(y=bar_y_pr, xmin=time[pon], xmax=time[qon], colors='green', linewidth=4, alpha=0.7)\n",
    "                    if count == 0: ax.text(time[pon], bar_y_pr, 'PR', color='green', fontsize=8, ha='right', va='center')\n",
    "\n",
    "            if j < len(t_offsets) and not np.isnan(q_onsets[j]) and not np.isnan(t_offsets[j]):\n",
    "                qon = int(q_onsets[j])\n",
    "                toff = int(t_offsets[j])\n",
    "                if qon < toff:\n",
    "                    ax.hlines(y=bar_y_qt, xmin=time[qon], xmax=time[toff], colors='blue', linewidth=4, alpha=0.7)\n",
    "                    if count == 0: ax.text(time[toff], bar_y_qt, 'QT', color='blue', fontsize=8, ha='left', va='center')\n",
    "            count += 1\n",
    "\n",
    "        valid = get_valid(res['p_onset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'gx', markersize=8, label='P-start')\n",
    "        valid = get_valid(res['q_onset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'm|', markersize=12, markeredgewidth=2, label='QRS-start')\n",
    "        valid = get_valid(res['t_offset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'b|', markersize=12, markeredgewidth=2, label='T-end')\n",
    "\n",
    "        info = f\"Seg {res['segment_num']} | BPM: {res['bpm']:.0f} | \"\n",
    "        info += f\"PR: {res['avg_pr']:.0f}ms | QRS: {res['avg_qrs']:.0f}ms | QT: {res['avg_qt']:.0f}ms\"\n",
    "       \n",
    "        ax.set_title(info, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlim([seg_time[0], seg_time[-1]])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if i == 0: ax.legend(loc='upper right', ncol=6, fontsize='small')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "   \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'SEGMENT':<10} {'BPM':<10} {'PR (ms)':<10} {'QRS (ms)':<10} {'QT (ms)':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for res in results:\n",
    "        print(f\"{res['segment_num']:<10} {res['bpm']:<10.1f} {res['avg_pr']:<10.0f} {res['avg_qrs']:<10.0f} {res['avg_qt']:<10.0f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def plot_p_wave_quality(signal, r_peaks, p_peaks, sampling_rate, segment_num=\"\"):\n",
    "    \"\"\"Visualize P-wave detection quality with enhanced diagnostics\"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))  # Added one more subplot\n",
    "    \n",
    "    # Plot 1: Full signal with P and R peaks\n",
    "    time = np.arange(len(signal)) / sampling_rate\n",
    "    axes[0].plot(time, signal, 'k-', alpha=0.6, linewidth=0.8)\n",
    "    \n",
    "    valid_r = r_peaks[~np.isnan(r_peaks)].astype(int)\n",
    "    axes[0].plot(time[valid_r], signal[valid_r], 'ro', markersize=6, label='R-peaks')\n",
    "    \n",
    "    valid_p = p_peaks[~np.isnan(p_peaks)].astype(int)\n",
    "    axes[0].plot(time[valid_p], signal[valid_p], 'g^', markersize=6, label='P-waves')\n",
    "    \n",
    "    # Mark missing P-waves\n",
    "    missing_p = np.where(np.isnan(p_peaks[1:]))[0] + 1  # Skip first beat\n",
    "    if len(missing_p) > 0:\n",
    "        missing_r = r_peaks[missing_p].astype(int)\n",
    "        axes[0].plot(time[missing_r], signal[missing_r], 'rx', markersize=10, \n",
    "                    markeredgewidth=2, label=f'Missing P ({len(missing_p)})')\n",
    "    \n",
    "    axes[0].set_title(f'P-wave Detection - Segment {segment_num}')\n",
    "    axes[0].set_ylabel('Amplitude (mV)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: PR intervals\n",
    "    pr_intervals = []\n",
    "    beat_numbers = []\n",
    "    beat_idx = 0\n",
    "    for i, (r, p) in enumerate(zip(r_peaks, p_peaks)):\n",
    "        if not np.isnan(p) and not np.isnan(r):\n",
    "            pr = (r - p) / sampling_rate * 1000\n",
    "            pr_intervals.append(pr)\n",
    "            beat_numbers.append(beat_idx)\n",
    "        beat_idx += 1\n",
    "    \n",
    "    if pr_intervals:\n",
    "        axes[1].plot(beat_numbers, pr_intervals, 'bo-', markersize=4)\n",
    "        axes[1].axhline(y=120, color='g', linestyle='--', alpha=0.5, label='Normal PR min (120ms)')\n",
    "        axes[1].axhline(y=200, color='orange', linestyle='--', alpha=0.5, label='1st° AVB threshold (200ms)')\n",
    "        axes[1].axhline(y=np.mean(pr_intervals), color='b', linestyle='-', alpha=0.7, \n",
    "                       label=f'Mean: {np.mean(pr_intervals):.1f}ms')\n",
    "        axes[1].set_ylabel('PR Interval (ms)')\n",
    "        axes[1].set_xlabel('Beat Number')\n",
    "        axes[1].set_title(f'PR Intervals (mean: {np.mean(pr_intervals):.1f}ms ± {np.std(pr_intervals):.1f}ms)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend()\n",
    "    \n",
    "    # Plot 3: P-wave amplitudes\n",
    "    p_amplitudes = []\n",
    "    beat_numbers_amp = []\n",
    "    beat_idx = 0\n",
    "    for p in valid_p:\n",
    "        p_amplitudes.append(abs(signal[p]))\n",
    "        beat_numbers_amp.append(beat_idx)\n",
    "        beat_idx += 1\n",
    "    \n",
    "    if p_amplitudes:\n",
    "        axes[2].plot(beat_numbers_amp, p_amplitudes, 'go-', markersize=4)\n",
    "        axes[2].axhline(y=0.1, color='g', linestyle='--', alpha=0.5, label='Typical P (0.1mV)')\n",
    "        axes[2].axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='Low amplitude threshold')\n",
    "        axes[2].axhline(y=np.mean(p_amplitudes), color='g', linestyle='-', alpha=0.7,\n",
    "                       label=f'Mean: {np.mean(p_amplitudes):.3f}mV')\n",
    "        axes[2].set_ylabel('Amplitude (mV)')\n",
    "        axes[2].set_xlabel('Beat Number')\n",
    "        axes[2].set_title(f'P-wave Amplitudes (mean: {np.mean(p_amplitudes):.3f}mV ± {np.std(p_amplitudes):.3f}mV)')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend()\n",
    "    \n",
    "    # Plot 4: Signal Quality Over Time (NEW)\n",
    "    # Calculate quality in windows\n",
    "    window_size = int(2 * sampling_rate)  # 2-second windows\n",
    "    num_windows = len(signal) // window_size\n",
    "    quality_over_time = []\n",
    "    time_points = []\n",
    "    \n",
    "    for w in range(num_windows):\n",
    "        start = w * window_size\n",
    "        end = min((w + 1) * window_size, len(signal))\n",
    "        window_signal = signal[start:end]\n",
    "        quality = calculate_signal_quality(window_signal)\n",
    "        quality_over_time.append(quality)\n",
    "        time_points.append((start + end) / 2 / sampling_rate)\n",
    "    \n",
    "    if quality_over_time:\n",
    "        axes[3].plot(time_points, quality_over_time, 'r-', linewidth=2, label='Signal Quality')\n",
    "        axes[3].axhline(y=0.7, color='g', linestyle='--', alpha=0.5, label='Good (>0.7)')\n",
    "        axes[3].axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate (>0.5)')\n",
    "        axes[3].axhline(y=0.3, color='r', linestyle='--', alpha=0.5, label='Poor (<0.3)')\n",
    "        axes[3].fill_between(time_points, 0, quality_over_time, alpha=0.3, color='red')\n",
    "        axes[3].set_ylabel('Quality Index')\n",
    "        axes[3].set_xlabel('Time (s)')\n",
    "        axes[3].set_title('Signal Quality Over Time')\n",
    "        axes[3].set_ylim([0, 1])\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        axes[3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection summary\n",
    "    total_beats = len(r_peaks) - 1  # Exclude first\n",
    "    detected_p = np.sum(~np.isnan(p_peaks[1:]))\n",
    "    detection_rate = detected_p / total_beats * 100 if total_beats > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- P-Wave Detection Summary (Segment {segment_num}) ---\")\n",
    "    print(f\"Total R-peaks: {len(r_peaks)}\")\n",
    "    print(f\"P-waves detected: {detected_p}/{total_beats} ({detection_rate:.1f}%)\")\n",
    "    print(f\"Mean signal quality: {np.mean(quality_over_time):.2f}\")\n",
    "    if pr_intervals:\n",
    "        print(f\"PR interval: {np.mean(pr_intervals):.1f} ± {np.std(pr_intervals):.1f} ms\")\n",
    "    if p_amplitudes:\n",
    "        print(f\"P amplitude: {np.mean(p_amplitudes):.3f} ± {np.std(p_amplitudes):.3f} mV\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION BLOCK\n",
    "# ==========================================\n",
    "\n",
    "input_json = r\"exception\\L2_1759207950416.json\" \n",
    "# input_json = r\"issues\\L2_1757064122874.json\"    #####\n",
    "# input_json = r\"v01_prob\\run 5 pvc.json\"    #####  xxxx\n",
    "# input_json = r\"issues\\L2_1757579288752.json\"\n",
    "# input_json = r\"issues\\L2_1757737806463.json\" \n",
    "# input_json = r\"v01_prob\\L2_1765984517025.json\"    ##  \n",
    "# input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "# input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "doubles = []\n",
    "with open(input_json, \"rb\") as f:\n",
    "    while chunk := f.read(8):\n",
    "        if len(chunk) < 8:\n",
    "            break\n",
    "        value = struct.unpack(\"<d\", chunk)[0]\n",
    "        doubles.append(value)\n",
    "\n",
    "file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "#     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "#     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     return json.loads(decoded_string)\n",
    "\n",
    "# # # # input_json = r\"NHF2\\DATA_1750689015865.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750689460556.json\"    #####\n",
    "# # # # input_json = r\"NHF2\\DATA_1750851207409.json\"   \n",
    "# # # input_json = r\"NHF2\\DATA_1750858856842.json\"    ##\n",
    "# # # # input_json = r\"NHF2\\DATA_1750862721789.json\"\n",
    "# # # # input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "# # # file_data = decrypt(input_json)\n",
    "\n",
    "# input_json = r\"NHF\\DATA_1752067426678.json\"    #### \n",
    "# # input_json = r\"NHF\\DATA_1752121970835.json\"    ####  \n",
    "# # input_json = r\"NHF\\DATA_1754709586876.json\"    ###   \n",
    "# # input_json = r\"NHF\\DATA_1754729551054.json\"    #\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "\n",
    "# Filter setup\n",
    "freq = 500\n",
    "low_pass_cutoff = 40\n",
    "low_pass_order = 7\n",
    "b_lp, a_lp = sp_signal.butter(low_pass_order, low_pass_cutoff / (freq / 2), btype=\"low\")\n",
    "b_notch, a_notch = sp_signal.iirnotch(50, 50 / 20, freq)  \n",
    "\n",
    "def low_pass_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "def notch_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "try:\n",
    "    # Process raw data for R-peak detection\n",
    "    print(\"Processing raw data for R-peak detection...\")\n",
    "    raw_data = data_process(file_data)\n",
    "    ecg_raw = raw_data[0, :15000, 0]\n",
    "    \n",
    "    # Process filtered data for P, Q, S, T wave detection\n",
    "    print(\"Processing filtered data for wave delineation...\")\n",
    "    filtered_data = data_process(low_pass_filter(notch_filter(baseline_wander(np.array(file_data[\"dataL2\"])))))\n",
    "    ecg_filtered = filtered_data[0, :15000, 0]\n",
    "    \n",
    "    sampling_rate = 500\n",
    "\n",
    "    print(\"\\n--- ENHANCED ECG ANALYSIS WITH P-WAVE DETECTION ---\")\n",
    "    print(\"Using raw data for R-peak detection\")\n",
    "    print(\"Using filtered data for P, Q, S, T wave delineation\")\n",
    "    \n",
    "    segment_results = process_ecg_segments(\n",
    "        ecg_raw=ecg_raw,\n",
    "        ecg_filtered=ecg_filtered,\n",
    "        sampling_rate=sampling_rate,\n",
    "        num_segments=4,\n",
    "        min_segment_length=5000\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    plot_ecg_segments(\n",
    "        ecg_raw,\n",
    "        ecg_filtered,\n",
    "        sampling_rate,\n",
    "        segment_results,\n",
    "        \"Enhanced Clinical Interval Analysis\"\n",
    "    )\n",
    "    \n",
    "    # Additional P-wave quality visualization (using filtered data)\n",
    "    print(\"\\n--- P-WAVE QUALITY ANALYSIS ---\")\n",
    "    for i, res in enumerate(segment_results):\n",
    "        segment_filtered = res['ecg_filtered']\n",
    "        segment_r_peaks = res['r_peaks'] - res['start_idx']\n",
    "        segment_p_peaks = res['p_peak'] - res['start_idx']\n",
    "        \n",
    "        plot_p_wave_quality(\n",
    "            segment_filtered,\n",
    "            segment_r_peaks,\n",
    "            segment_p_peaks,\n",
    "            sampling_rate,\n",
    "            segment_num=res['segment_num']\n",
    "        )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32d2de",
   "metadata": {},
   "source": [
    "## flat line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pywt\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64   \n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "\n",
    "# Define filter coefficients if not defined\n",
    "fs = 500  # sampling rate\n",
    "nyq = 0.5 * fs\n",
    "\n",
    "# Example low pass filter (cutoff 40 Hz)\n",
    "low_cutoff = 40 / nyq\n",
    "b_lp, a_lp = sp_signal.butter(4, low_cutoff, btype='low')\n",
    "\n",
    "# Example notch filter (50 Hz)\n",
    "q = 30\n",
    "w0 = 50 / nyq\n",
    "b_notch, a_notch = sp_signal.iirnotch(w0, q)\n",
    "\n",
    "def decrypt(input_file):\n",
    "    \"\"\"Decrypt encrypted JSON file\"\"\"\n",
    "    private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    with open(input_file, 'rb') as f:\n",
    "        encrypted_data = f.read()\n",
    "    enc = base64.b64decode(encrypted_data[24:])\n",
    "    data = unpad(cipher.decrypt(enc), 16)\n",
    "    decoded_string = data.decode('utf-8')\n",
    "    return json.loads(decoded_string)\n",
    "\n",
    "    \n",
    "def baseline_wander(X):\n",
    "    def get_median_filter_width(sampling_rate, duration):\n",
    "        res = int(sampling_rate * duration)\n",
    "        res += (res % 2) - 1\n",
    "        return res\n",
    "\n",
    "    ms_flt_array = [0.2, 0.6]\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype=\"int\")\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(500, ms_flt_array[i])\n",
    "    X0 = X\n",
    "    for mi in range(0, len(mfa)):\n",
    "        X0 = sp_signal.medfilt(X0, mfa[mi])\n",
    "    X0 = np.subtract(X, X0)\n",
    "    return X0\n",
    "\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    \"\"\"Normalize signal to range [0, 1].\"\"\"\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    data = normalize(signal_data, min_val, max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_process(filename):\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "    \n",
    "    for key in keys:\n",
    "        sig = np.array(filename[key])\n",
    "        datas.append(sig.astype('float32'))\n",
    "    \n",
    "    datas_array = np.array(datas)               # shape: (1, length) or (channels, length)\n",
    "    \n",
    "    # ── Compute real (raw) statistics here ───────────────────────────────\n",
    "    raw_min   = np.min(datas_array)\n",
    "    raw_max   = np.max(datas_array)\n",
    "    raw_mean  = np.mean(datas_array)\n",
    "    raw_std   = np.std(datas_array)\n",
    "    raw_var   = np.var(datas_array)\n",
    "    raw_median = np.median(datas_array)\n",
    "    \n",
    "    print(\"\\nRaw (pre-normalized) signal statistics:\")\n",
    "    print(f\"  Min    = {raw_min:12.4f}\")\n",
    "    print(f\"  Max    = {raw_max:12.4f}\")\n",
    "    print(f\"  Mean   = {raw_mean:12.4f}\")\n",
    "    print(f\"  Std    = {raw_std:12.4f}\")\n",
    "    print(f\"  Var    = {raw_var:14.6f}\")\n",
    "    print(f\"  Median = {raw_median:12.4f}\")\n",
    "    print(f\"  Range  = {raw_max - raw_min:.4f}\\n\")\n",
    "    \n",
    "    # Now do normalization (your existing code)\n",
    "    min_val = raw_min\n",
    "    max_val = raw_max\n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(normalize(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    \n",
    "    return final_data, {\n",
    "        'raw_min': raw_min, 'raw_max': raw_max, 'raw_mean': raw_mean,\n",
    "        'raw_std': raw_std, 'raw_var': raw_var, 'raw_median': raw_median,\n",
    "        'raw_range': raw_max - raw_min\n",
    "    }, datas_array[0]  # return flattened raw for simplicity\n",
    "\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0: \n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        # If we have 2-3 outlier peaks, remove them\n",
    "        # if 2 <= high_amp_count <= 3:\n",
    "        # if 3 <= high_amp_count <= 4 and len(peaks) - high_amp_count > 0:\n",
    "        # if 5 <= high_amp_count <= 6 and len(peaks) - high_amp_count > 0:\n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            # mask[high_amp_indices] = False\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            # Check if this could be a T-wave (160-450ms after R-peak)\n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                # T-waves are typically smaller and wider\n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    # Measure width at half maximum\n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    # T-waves are wider than QRS complexes\n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    \"\"\"Multi-strategy robust QRS detection for difficult cases\"\"\"\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated, \n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None, raw_segment=None):\n",
    "    if raw_segment is not None:\n",
    "        var_raw = np.var(raw_segment)\n",
    "        # if var_raw < 0.0095:                  \n",
    "        if var_raw < 0.005:                  \n",
    "            print(f\"Raw variance {var_raw:.6f} < 0.0095 → treating as asystole / flatline\")\n",
    "            return data, np.array([]), 0.0, np.array([])\n",
    "    # else:\n",
    "    #     var = np.var(data)\n",
    "    #     if var < 0.00015:                     \n",
    "    #         print(f\"Normalized variance {var:.6f} too low → possible asystole\")\n",
    "    #         return data, np.array([]), 0.0, np.array([])\n",
    "\n",
    "    # data_clean = baseline_wander(data) \n",
    "\n",
    "    data_clean = data \n",
    "\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "            \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # =================================================================\n",
    "    # CRITICAL FIX: GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    # =================================================================\n",
    "    if len(cleaned_r) >= 2:\n",
    "        # Calculate reference height (Median of existing peaks)\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        # Only fill gaps if median_rr suggests a normal rhythm (< 1.5s).\n",
    "        # If median_rr is already 2.0s (bradycardia), huge gaps are normal.\n",
    "        if median_rr < 1.5: \n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                        \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    # Lower threshold slightly for gap search\n",
    "                    low_thresh = mean_val * 0.6 \n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                            \n",
    "                        # 1. Check Sharpness\n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            # 2. Refine Position\n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            # 3. AMPLITUDE CHECK (The Fix)\n",
    "                            # Even if it's sharp, is it tall enough?\n",
    "                            # AV Block P-waves are sharp but short.\n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            # Must be at least 40-50% of the median R-peak height\n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "\n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # =================================================================\n",
    "\n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    # Relaxed expectations for Bradycardia/AV Block\n",
    "    min_expected_peaks = int(30/60 * segment_duration) \n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    # Fallback to robust only if counts are extremely off\n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        # Valid intervals widened to account for Bradycardia/Pauses\n",
    "        # valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 3.5)] \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)] \n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_data, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    max_len = len(ecg_data)\n",
    "    \n",
    "    if num_segments > 1:\n",
    "        window_step = (max_len - min_segment_length) / (num_segments - 1)\n",
    "        window_step = round(window_step)\n",
    "    else:\n",
    "        window_step = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = start_idx + min_segment_length\n",
    "        \n",
    "        if end_idx > max_len:\n",
    "            start_idx = max_len - min_segment_length\n",
    "            end_idx = max_len\n",
    "            \n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            end_idx = min(min_segment_length, max_len)\n",
    "        \n",
    "        segment = ecg_data[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment) < 100:\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        segment_duration = len(segment) / sampling_rate\n",
    "        # ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(segment, sampling_rate, segment_duration)\n",
    "\n",
    "        raw_segment = raw_ecg[start_idx:end_idx]   # ← the real raw amplitudes\n",
    "\n",
    "        quality, sqi, details = assess_ecg_quality(raw_segment, sampling_rate)\n",
    "\n",
    "        print(f\"Segment {i+1} SQI: {sqi:.2f} → {quality}\")\n",
    "\n",
    "        # if quality == \"BAD\" or quality == \"MARGINAL\":\n",
    "        if quality == \"BADDDDDDDDDDDDDDDDD\":\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(\n",
    "            segment,\n",
    "            sampling_rate,\n",
    "            segment_duration,\n",
    "            raw_segment=raw_segment                # ← pass raw here\n",
    "        )\n",
    "\n",
    "        print(f\"Segment {i+1}: Detected {len(r_peaks)} R-peaks, BPM: {bpm:.1f}\")\n",
    "        \n",
    "        adjusted_r_peaks = r_peaks + start_idx if len(r_peaks) > 0 else np.array([])\n",
    "        adjusted_cleaned_r = np.array(cleaned_r) + start_idx if len(cleaned_r) > 0 else np.array([])\n",
    "        \n",
    "        results.append({\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_filtered': ecg_filtered,\n",
    "            'r_peaks': adjusted_r_peaks,\n",
    "            'bpm': bpm,\n",
    "            'cleaned_r': adjusted_cleaned_r,\n",
    "            'ecg_raw': segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_ecg_stats(signal, fs=500):\n",
    "    \"\"\"Compute common statistics for an ECG segment\"\"\"\n",
    "    if len(signal) == 0:\n",
    "        return {\n",
    "            'nsamples': 0,\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'var': np.nan,\n",
    "            'min': np.nan,\n",
    "            'max': np.nan,\n",
    "            'median': np.nan,\n",
    "            'rms': np.nan,\n",
    "            'duration_s': 0.0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'nsamples': len(signal),\n",
    "        'mean': float(np.mean(signal)),\n",
    "        'std': float(np.std(signal)),\n",
    "        'var': float(np.var(signal)),\n",
    "        'min': float(np.min(signal)),\n",
    "        'max': float(np.max(signal)),\n",
    "        'median': float(np.median(signal)),\n",
    "        'rms': float(np.sqrt(np.mean(signal**2))),\n",
    "        'duration_s': len(signal) / fs\n",
    "    }\n",
    "\n",
    "\n",
    "def format_stats_text(stats, prefix=\"\"):\n",
    "    \"\"\"Create a compact multi-line stats string for plotting\"\"\"\n",
    "    lines = [\n",
    "        f\"{prefix}Duration: {stats['duration_s']:.2f} s\",\n",
    "        f\"Samples:   {stats['nsamples']}\",\n",
    "        f\"Mean:      {stats['mean']:.4f}\",\n",
    "        f\"Std:       {stats['std']:.4f}\",\n",
    "        f\"Var:       {stats['var']:.6f}\",\n",
    "        f\"Min / Max: {stats['min']:.4f} / {stats['max']:.4f}\",\n",
    "        f\"Median:    {stats['median']:.4f}\",\n",
    "        f\"RMS:       {stats['rms']:.4f}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def plot_ecg_segments(ecg_data, sampling_rate, results, title=\"ECG Segments with R-peaks and BPM\", raw_ecg=None):\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(15, 3.5 * num_segments), sharex=False)\n",
    "    \n",
    "    if num_segments == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    global_stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "    fig.suptitle(f\"{title}\\nFull signal stats: {global_stats['duration_s']:.1f}s | \"\n",
    "                 f\"mean={global_stats['mean']:.4f}  std={global_stats['std']:.4f}\", \n",
    "                 fontsize=13, y=0.98)\n",
    "    \n",
    "    for i, (ax, result) in enumerate(zip(axes, results)):\n",
    "        segment_num = result['segment_num']\n",
    "        start_idx = result['start_idx']\n",
    "        end_idx = result['end_idx']\n",
    "        bpm = result['bpm']\n",
    "        r_peaks = result['r_peaks']\n",
    "        \n",
    "        segment_time = time[start_idx:end_idx]\n",
    "        segment_data = result['ecg_raw']\n",
    "        \n",
    "        ax.plot(segment_time, segment_data, 'b-', alpha=0.8, linewidth=1.1, label='ECG')\n",
    "        \n",
    "        if len(r_peaks) > 0:\n",
    "            r_times = r_peaks / sampling_rate\n",
    "            r_values = ecg_data[r_peaks.astype(int)]\n",
    "            ax.plot(r_times, r_values, 'ro', markersize=7, label='R-peaks', alpha=0.85)\n",
    "        \n",
    "        # ── Statistics box per segment (use raw if available) ───────────────────────────────\n",
    "        if raw_ecg is not None:\n",
    "            raw_segment = raw_ecg[start_idx:end_idx]\n",
    "            seg_stats = compute_ecg_stats(raw_segment, sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            seg_stats = compute_ecg_stats(segment_data, sampling_rate)\n",
    "            prefix = \"\"\n",
    "        stats_text = format_stats_text(seg_stats, prefix + f\"Seg {segment_num}  \")\n",
    "        stats_text += f\"\\nBPM:       {bpm:.1f}\"\n",
    "        \n",
    "        ax.text(0.02, 0.98, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=9.5,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.82, edgecolor='gray', boxstyle='round,pad=0.4'))\n",
    "        \n",
    "        segment_duration = (end_idx - start_idx) / sampling_rate\n",
    "        ax.set_title(f'Segment {segment_num}: {start_idx:,} – {end_idx:,}  |  BPM: {bpm:.1f}')\n",
    "        ax.set_ylabel('Amplitude (norm)')\n",
    "        ax.grid(True, alpha=0.35, linestyle='--')\n",
    "        ax.set_xlim([segment_time[0], segment_time[-1]])\n",
    "        ax.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (seconds)')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])   # make room for suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # ── Console summary ───────────────────────────────────────────────\n",
    "    print(\"═\" * 70)\n",
    "    print(\"ECG SEGMENT STATISTICS SUMMARY\")\n",
    "    print(\"═\" * 70)\n",
    "    for res in results:\n",
    "        if raw_ecg is not None:\n",
    "            s = compute_ecg_stats(raw_ecg[res['start_idx']:res['end_idx']], sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            s = compute_ecg_stats(res['ecg_raw'], sampling_rate)\n",
    "            prefix = \"Norm \"\n",
    "        print(f\"Segment {res['segment_num']:2d} | {s['duration_s']:5.2f}s | \"\n",
    "              f\"mean={s['mean']:8.4f}  std={s['std']:7.4f}  BPM={res['bpm']:5.1f} ({prefix.strip()})\"\n",
    "            )\n",
    "    print(\"═\" * 70)\n",
    "    \n",
    "    \n",
    "def plot_full_ecg(ecg_data, sampling_rate, title=\"Full ECG Signal Analysis\", raw_ecg=None):\n",
    "    # _, r_peaks, global_bpm, _ = qrs_detect(ecg_data, sampling_rate)\n",
    "    _, r_peaks, global_bpm, _ = qrs_detect(\n",
    "        ecg_data,\n",
    "        sampling_rate,\n",
    "        raw_segment=raw_ecg[:len(ecg_data)]    # pass corresponding raw part\n",
    "    )\n",
    "        \n",
    "    if raw_ecg is not None:\n",
    "        stats = compute_ecg_stats(raw_ecg[:len(ecg_data)], sampling_rate)\n",
    "        prefix = \"Raw \"\n",
    "    else:\n",
    "        stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "        prefix = \"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 6)) # Width of 20 makes the 15k samples readable\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    # Plot the signal\n",
    "    plt.plot(time_axis, ecg_data, 'b-', linewidth=0.8, alpha=0.8, label='Filtered ECG')\n",
    "    \n",
    "    # Plot the peaks\n",
    "    if len(r_peaks) > 0:\n",
    "        # Filter out peaks that might be out of bounds (safety check)\n",
    "        valid_peaks = r_peaks[r_peaks < len(ecg_data)].astype(int)\n",
    "        \n",
    "        peak_times = valid_peaks / sampling_rate\n",
    "        peak_values = ecg_data[valid_peaks]\n",
    "        \n",
    "        plt.plot(peak_times, peak_values, 'ro', markersize=4, label='R-peaks')\n",
    "        \n",
    "        # Optional: Annotate every 5th peak to help navigation\n",
    "        for i, (t, v) in enumerate(zip(peak_times, peak_values)):\n",
    "            if i % 5 == 0:\n",
    "                plt.annotate(f'{t:.1f}s', (t, v), xytext=(0, 10), \n",
    "                             textcoords='offset points', ha='center', fontsize=8, color='red')\n",
    "\n",
    "    plt.title(f\"{title} | Global BPM: {global_bpm:.1f} | Total Peaks: {len(r_peaks)}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Normalized Amplitude\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Global Analysis: {len(r_peaks)} peaks detected over {len(ecg_data)/sampling_rate:.2f} seconds.\")\n",
    "    print(f\"{prefix}Full signal stats →  mean={stats['mean']:.4f}  std={stats['std']:.4f}  var={stats['var']:.6f}\")\n",
    "\n",
    "\n",
    "# def assess_ecg_quality(ecg_raw, fs=500):\n",
    "#     ecg = np.asarray(ecg_raw)\n",
    "\n",
    "#     if len(ecg) < fs:\n",
    "#         return \"BADDDDDDDDDDDDDDDDD\", 0.0, {\"reason\": \"Too short\"}\n",
    "\n",
    "#     # ── 1. Variance (flatline / saturation)\n",
    "#     var = np.var(ecg)\n",
    "\n",
    "#     # ── 2. Baseline wander (0–0.5 Hz)\n",
    "#     f, pxx = sp_signal.welch(ecg, fs=fs, nperseg=2048)\n",
    "#     baseline_power = np.sum(pxx[(f >= 0.05) & (f <= 0.5)])\n",
    "#     total_power = np.sum(pxx)\n",
    "#     baseline_ratio = baseline_power / total_power if total_power > 0 else 1.0\n",
    "\n",
    "#     # ── 3. Powerline noise (50 Hz)\n",
    "#     power_50hz = np.sum(pxx[(f >= 48) & (f <= 52)])\n",
    "#     power_5_40hz = np.sum(pxx[(f >= 5) & (f <= 40)])\n",
    "#     powerline_ratio = power_50hz / power_5_40hz if power_5_40hz > 0 else 1.0\n",
    "\n",
    "#     # ── 4. QRS energy dominance\n",
    "#     qrs_band_power = np.sum(pxx[(f >= 8) & (f <= 25)])\n",
    "#     qrs_ratio = qrs_band_power / total_power if total_power > 0 else 0\n",
    "\n",
    "#     # ── 5. Clipping detection\n",
    "#     clip_ratio = np.mean((ecg == np.max(ecg)) | (ecg == np.min(ecg)))\n",
    "\n",
    "#     # ── Scoring (0–1)\n",
    "#     score = 1.0\n",
    "#     score -= 0.4 if var < 0.005 else 0\n",
    "#     score -= min(baseline_ratio * 2, 0.3)\n",
    "#     score -= min(powerline_ratio * 2, 0.3)\n",
    "#     score -= 0.2 if clip_ratio > 0.02 else 0\n",
    "#     score += min(qrs_ratio * 1.5, 0.3)\n",
    "\n",
    "#     score = np.clip(score, 0, 1)\n",
    "\n",
    "#     if score >= 0.75:  # 75\n",
    "#         quality = \"GOOD\"\n",
    "#     # elif score >= 0.45:\n",
    "#     #     quality = \"MARGINAL\"\n",
    "#     else:\n",
    "#         quality = \"BADDDDDDDDDDDDDDDDD\"\n",
    "\n",
    "#     details = {\n",
    "#         \"variance\": float(var),\n",
    "#         \"baseline_ratio\": float(baseline_ratio),\n",
    "#         \"powerline_ratio\": float(powerline_ratio),\n",
    "#         \"qrs_ratio\": float(qrs_ratio),\n",
    "#         \"clip_ratio\": float(clip_ratio)\n",
    "#     }\n",
    "\n",
    "#     return quality, score, details\n",
    "\n",
    "# BADDDDDDDDDDDDDDDDD\n",
    "\n",
    "\n",
    "def assess_ecg_quality(ecg_raw, fs=500):\n",
    "    ecg = np.asarray(ecg_raw)\n",
    "\n",
    "    if len(ecg) < fs:\n",
    "        return \"BAD\", 0.0, {\"reason\": \"Too short\"}\n",
    "\n",
    "    # ── 1. Variance (flatline / saturation)\n",
    "    var = np.var(ecg)\n",
    "    \n",
    "    # ── 2. Check for monotonic rising/drowning patterns using peak differences ──\n",
    "    # Find all local maxima and minima (peaks and valleys)\n",
    "    maxima, _ = sp_signal.find_peaks(ecg, distance=50)  # local peaks\n",
    "    minima, _ = sp_signal.find_peaks(-ecg, distance=50)  # local valleys\n",
    "    \n",
    "    # Combine and sort all extrema\n",
    "    all_extrema = np.sort(np.concatenate([maxima, minima]))\n",
    "    \n",
    "    monotonic_score = 1.0\n",
    "    monotonic_details = {\"peak_difference_pattern\": \"variable\"}\n",
    "    \n",
    "    if len(all_extrema) >= 3:  # Need at least 3 extrema to analyze pattern\n",
    "        # Calculate amplitude differences between consecutive extrema\n",
    "        extrema_values = ecg[all_extrema]\n",
    "        amplitude_diffs = np.abs(np.diff(extrema_values))\n",
    "        \n",
    "        # Calculate time differences between consecutive extrema\n",
    "        time_diffs = np.diff(all_extrema) / fs  # in seconds\n",
    "        \n",
    "        # Analyze patterns:\n",
    "        # 1. Check if amplitude differences are consistently small (suggests drowning/smooth signal)\n",
    "        if len(amplitude_diffs) > 2:\n",
    "            amp_std = np.std(amplitude_diffs)\n",
    "            amp_mean = np.mean(amplitude_diffs)\n",
    "            \n",
    "            # Low standard deviation of amplitude differences suggests consistent pattern\n",
    "            if amp_std < 0.1 * amp_mean and amp_mean < 0.05 * (np.max(ecg) - np.min(ecg)):\n",
    "                monotonic_score -= 0.3\n",
    "                monotonic_details[\"peak_difference_pattern\"] = \"consistent_small\"\n",
    "        \n",
    "        # 2. Check for consistent rising/falling trend by analyzing peak-valley sequences\n",
    "        trend_strength = 0\n",
    "        trend_direction = 0\n",
    "        \n",
    "        # Analyze sequence of peaks and valleys\n",
    "        peak_valley_values = ecg[all_extrema]\n",
    "        peak_valley_signs = np.sign(np.diff(peak_valley_values))\n",
    "        \n",
    "        # Count consecutive same-sign differences\n",
    "        if len(peak_valley_signs) > 3:\n",
    "            same_sign_count = 0\n",
    "            max_same_sign = 0\n",
    "            \n",
    "            for i in range(1, len(peak_valley_signs)):\n",
    "                if peak_valley_signs[i] == peak_valley_signs[i-1]:\n",
    "                    same_sign_count += 1\n",
    "                    max_same_sign = max(max_same_sign, same_sign_count)\n",
    "                else:\n",
    "                    same_sign_count = 0\n",
    "            \n",
    "            # If many consecutive same-sign differences, strong trend exists\n",
    "            if max_same_sign >= 4:  # 4+ consecutive same-sign changes\n",
    "                monotonic_score -= 0.4\n",
    "                monotonic_details[\"peak_difference_pattern\"] = \"strong_trend\"\n",
    "        \n",
    "        # 3. Check if time intervals between extrema are too regular (suggests artificial pattern)\n",
    "        if len(time_diffs) > 3:\n",
    "            time_std = np.std(time_diffs)\n",
    "            time_mean = np.mean(time_diffs)\n",
    "            \n",
    "            if time_std < 0.1 * time_mean:  # Very regular timing\n",
    "                monotonic_score -= 0.2\n",
    "                monotonic_details[\"time_regularity\"] = \"high\"\n",
    "    \n",
    "    # ── 3. Baseline wander (0–0.5 Hz)\n",
    "    f, pxx = sp_signal.welch(ecg, fs=fs, nperseg=2048)\n",
    "    baseline_power = np.sum(pxx[(f >= 0.05) & (f <= 0.5)])\n",
    "    total_power = np.sum(pxx)\n",
    "    baseline_ratio = baseline_power / total_power if total_power > 0 else 1.0\n",
    "\n",
    "    # ── 4. Powerline noise (50 Hz)\n",
    "    power_50hz = np.sum(pxx[(f >= 48) & (f <= 52)])\n",
    "    power_5_40hz = np.sum(pxx[(f >= 5) & (f <= 40)])\n",
    "    powerline_ratio = power_50hz / power_5_40hz if power_5_40hz > 0 else 1.0\n",
    "\n",
    "    # ── 5. QRS energy dominance\n",
    "    qrs_band_power = np.sum(pxx[(f >= 8) & (f <= 25)])\n",
    "    qrs_ratio = qrs_band_power / total_power if total_power > 0 else 0\n",
    "\n",
    "    # ── 6. Clipping detection\n",
    "    clip_ratio = np.mean((ecg == np.max(ecg)) | (ecg == np.min(ecg)))\n",
    "    \n",
    "    # ── 7. Additional checks for extreme monotonic patterns ──\n",
    "    # Check overall trend using linear regression\n",
    "    x = np.arange(len(ecg))\n",
    "    slope, intercept = np.polyfit(x, ecg, 1)\n",
    "    trend_line = slope * x + intercept\n",
    "    trend_residuals = ecg - trend_line\n",
    "    trend_strength = np.abs(slope) * len(ecg) / (np.max(ecg) - np.min(ecg) + 1e-10)\n",
    "    \n",
    "    # If strong linear trend with low residuals\n",
    "    if trend_strength > 0.5 and np.std(trend_residuals) < 0.2 * np.std(ecg):\n",
    "        monotonic_score -= 0.5\n",
    "        monotonic_details[\"linear_trend\"] = \"strong\"\n",
    "\n",
    "    # ── Combined Scoring (0–1) ──\n",
    "    score = 1.0\n",
    "    \n",
    "    # Apply monotonic pattern penalty\n",
    "    score = max(0, score - (1 - monotonic_score))\n",
    "    \n",
    "    # Apply other penalties\n",
    "    score -= 0.4 if var < 0.005 else 0\n",
    "    score -= min(baseline_ratio * 2, 0.3)\n",
    "    score -= min(powerline_ratio * 2, 0.3)\n",
    "    score -= 0.2 if clip_ratio > 0.02 else 0\n",
    "    \n",
    "    score = np.clip(score, 0, 1)\n",
    "\n",
    "    # Quality classification\n",
    "    if score >= 0.7:\n",
    "        quality = \"GOOD\"\n",
    "    elif score >= 0.4:\n",
    "        quality = \"MARGINAL\"\n",
    "    else:\n",
    "        quality = \"BAD\"\n",
    "\n",
    "    details = {\n",
    "        \"variance\": float(var),\n",
    "        \"baseline_ratio\": float(baseline_ratio),\n",
    "        \"powerline_ratio\": float(powerline_ratio),\n",
    "        \"qrs_ratio\": float(qrs_ratio),\n",
    "        \"clip_ratio\": float(clip_ratio),\n",
    "        \"monotonic_pattern\": monotonic_details[\"peak_difference_pattern\"],\n",
    "        \"score\": float(score),\n",
    "        \"extrema_count\": len(all_extrema),\n",
    "        \"trend_strength\": float(trend_strength) if 'trend_strength' in locals() else 0.0\n",
    "    }\n",
    "\n",
    "    return quality, score, details\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    " \n",
    "# # input_json = r\"simulator\\contec\\trigeminy_1756103085272.json\" \n",
    "# # input_json = r\"simulator\\contec\\asystl_1756103447146.json\" \n",
    "# # input_json = r\"simulator\\contec\\1d av_1756104504294.json\"  \n",
    "# # input_json = r\"simulator\\contec\\3d av_1756104633918.json\"  \n",
    "# # input_json = r\"simulator\\contec\\280bpm_1756100716422.json\" \n",
    "# # input_json = r\"simulator\\contec\\av sequence_1756106676125.json\"  \n",
    "# # input_json = r\"simulator\\contec\\dmnd freq_1756106571373.json\"  \n",
    "# #    \n",
    "# # input_json = r\"simulator\\fluke\\trigeminy_1754543043205.json\"   \n",
    "# # input_json = r\"simulator\\fluke\\3d av_1754545068278.json\"   \n",
    "# # input_json = r\"simulator\\fluke\\asystole_1754544406847.json\"   \n",
    "\n",
    "# # input_json = r\"simulator\\fluke\\80bpm_1754288606201.json\"   \n",
    "# input_json = r\"0_bpm\\asystole_jan_2_1770026113214.json\"   \n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# input_json = r\"v01_prob\\teton_ecg.ecgdatas.json\"  \n",
    "# with open(input_json, 'r') as file:\n",
    "#     all_id_data = json.load(file)\n",
    "\n",
    "# file_data = all_id_data[3]['ecgValue']   \n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"bpms\\afib_1766471694144.json\"\n",
    "# # input_json = r\"bpms\\bigeminy_1766467666407.json\"\n",
    "# # input_json = r\"bpms\\pvc 6_1766467718685.json\"    \n",
    "# # input_json = r\"bpms\\tri_1766467618314.json\"\n",
    "# # input_json = r\"v01_prob/220_1767858669130.json\"\n",
    "# # input_json = r\"v01_prob/240bpm_1767858615562.json\"\n",
    "# # input_json = r\"v01_prob\\25 contec_1768375918389.json\"\n",
    "# # input_json = r\"v01_prob\\30bpm contec_1768375716454.json\"\n",
    "# # input_json = r\"v01_prob\\2d av_1754545008828.json\"  \n",
    "# input_json = r\"v01_prob\\3rd_davb_1768554217066.json\"  \n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_json = r\"exception\\L2_1759207950416.json\"   \n",
    "# input_json = r\"issues\\L2_1757064122874.json\"  \n",
    "# input_json = r\"v01_prob\\run 5 pvc.json\"  \n",
    "# input_json = r\"issues\\L2_1757579288752.json\"\n",
    "# input_json = r\"issues\\L2_1757737806463.json\"  \n",
    "# input_json = r\"v01_prob\\L2_1765984517025.json\"  \n",
    "# input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "# input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "# input_json = r\"issues2\\1757998097068\\L2_1757998097068.json\"\n",
    "# input_json = r\"issues2\\1758943573744\\L2_1758943573744.json\"\n",
    "input_json = r\"issues2\\1759059066184\\L2_1759059066184.json\"     ###########\n",
    "# input_json = r\"issues2\\1759117739887\\L2_1759117739887.json\"\n",
    "# input_json = r\"issues2\\1759118709079\\L2_1759118709079.json\"\n",
    "# input_json = r\"issues2\\1759202739736\\L2_1759202739736.json\"\n",
    "# input_json = r\"issues2\\1759639059357\\L2_1759639059357.json\"\n",
    "# input_json = r\"issues2\\L2_1759208381248.json\"\n",
    "\n",
    "doubles = []\n",
    "with open(input_json, \"rb\") as f:\n",
    "    while chunk := f.read(8):\n",
    "        if len(chunk) < 8:\n",
    "            break\n",
    "        value = struct.unpack(\"<d\", chunk)[0]\n",
    "        doubles.append(value)\n",
    "\n",
    "file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "#     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "#     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     return json.loads(decoded_string)\n",
    "\n",
    "# # # input_json = r\"NHF2\\DATA_1750689015865.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750689460556.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750851207409.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750858856842.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750862721789.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "# # file_data = decrypt(input_json)\n",
    "\n",
    "# # input_json = r\"NHF\\DATA_1752067426678.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1752121970835.json\"  ########\n",
    "# input_json = r\"NHF\\DATA_1754709586876.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted CSV file using AES ECB mode\"\"\"\n",
    "#     Private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    \n",
    "#     cipher = AES.new(Private_key.encode(), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     cipher = AES.new(Private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     data_list = decoded_string.split(\",\")\n",
    "#     float_list = [float(x) for x in data_list]\n",
    "\n",
    "#     return float_list\n",
    "\n",
    "# selected_path = \"v01_prob\\ECG_1735798172211.csv\"  ####\n",
    "# # selected_path = \"v01_prob\\ECG_L2_1738637533455.csv\"\n",
    "# file_data = decrypt(selected_path)\n",
    "# file_data = {'dataL2': file_data}\n",
    "\n",
    "\n",
    "\n",
    "def low_pass_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "\n",
    "def notch_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "     \n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "# processed_data, raw_global_stats, raw_ecg = data_process(\n",
    "#     low_pass_filter(notch_filter(file_data))\n",
    "# )\n",
    "\n",
    "# ecg_full = processed_data[0, :15000, 0]\n",
    "\n",
    "\n",
    "# processed_data, raw_global_stats, raw_ecg = data_process(\n",
    "#     low_pass_filter(notch_filter(file_data))\n",
    "# )\n",
    "# ecg_full = processed_data[0, :15000, 0]\n",
    "\n",
    "# data = data_process(file_data)\n",
    "# ecg_full = data[0, :15000, 0]\n",
    "\n",
    "processed, stats, raw_ecg = data_process(file_data)\n",
    "ecg_full = processed[0, :15000, 0]\n",
    "\n",
    "sampling_rate = 500\n",
    "\n",
    "results = process_ecg_segments(\n",
    "    ecg_data=ecg_full,\n",
    "    sampling_rate=sampling_rate,\n",
    "    num_segments=20,\n",
    "    min_segment_length=1500\n",
    ")\n",
    "\n",
    "plot_ecg_segments(ecg_full, sampling_rate, results, \"ECG Analysis: 4 Segments with R-peak Detection\", raw_ecg=raw_ecg)\n",
    "\n",
    "print(\"\\n--- Plotting Full Data ---\")\n",
    "plot_full_ecg(ecg_full, sampling_rate, \"Final Full Data View\", raw_ecg=raw_ecg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319eb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

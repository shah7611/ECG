{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c28d8d",
   "metadata": {},
   "source": [
    "## bpm normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75410ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pywt\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64   \n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "\n",
    "\n",
    "def decrypt(input_file):\n",
    "    \"\"\"Decrypt encrypted JSON file\"\"\"\n",
    "    private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    with open(input_file, 'rb') as f:\n",
    "        encrypted_data = f.read()\n",
    "    enc = base64.b64decode(encrypted_data[24:])\n",
    "    data = unpad(cipher.decrypt(enc), 16)\n",
    "    decoded_string = data.decode('utf-8')\n",
    "    return json.loads(decoded_string)\n",
    "\n",
    "    \n",
    "def baseline_wander(X):\n",
    "    def get_median_filter_width(sampling_rate, duration):\n",
    "        res = int(sampling_rate * duration)\n",
    "        res += (res % 2) - 1\n",
    "        return res\n",
    "\n",
    "    ms_flt_array = [0.2, 0.6]\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype=\"int\")\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(500, ms_flt_array[i])\n",
    "    X0 = X\n",
    "    for mi in range(0, len(mfa)):\n",
    "        X0 = sp_signal.medfilt(X0, mfa[mi])\n",
    "    X0 = np.subtract(X, X0)\n",
    "    return X0\n",
    "\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    \"\"\"Normalize signal to range [0, 1].\"\"\"\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    data = normalize(signal_data, min_val, max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_process(filename):\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "    \n",
    "    # print(f\"Type of filename['dataL2']: {type(filename['dataL2'])}\")\n",
    "    # print(f\"Content of filename['dataL2']: {filename['dataL2']}\")\n",
    "    \n",
    "    for key in keys:\n",
    "        sig = np.array(filename[key])\n",
    "        datas.append(sig.astype('float32'))\n",
    "    \n",
    "    datas_array = np.array(datas)\n",
    "    print(datas_array.shape)\n",
    "    min_val = np.min(datas_array)\n",
    "    max_val = np.max(datas_array)\n",
    "    \n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(process_signal(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    print(final_data)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0: \n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        # If we have 2-3 outlier peaks, remove them\n",
    "        # if 2 <= high_amp_count <= 3:\n",
    "        # if 3 <= high_amp_count <= 4 and len(peaks) - high_amp_count > 0:\n",
    "        # if 5 <= high_amp_count <= 6 and len(peaks) - high_amp_count > 0:\n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            # mask[high_amp_indices] = False\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            # Check if this could be a T-wave (150-450ms after R-peak)\n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                # T-waves are typically smaller and wider\n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    # Measure width at half maximum\n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    # T-waves are wider than QRS complexes\n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    \"\"\"Multi-strategy robust QRS detection for difficult cases\"\"\"\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated, \n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None):\n",
    "    \"\"\"\n",
    "    Enhanced QRS detection with Amplitude Guardrails for AV Blocks\n",
    "    \"\"\"\n",
    "    # Apply baseline wander removal\n",
    "    # data_clean = baseline_wander(data) \n",
    "    data_clean = data # Keeping your override\n",
    "\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "            \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # =================================================================\n",
    "    # CRITICAL FIX: GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    # =================================================================\n",
    "    if len(cleaned_r) >= 2:\n",
    "        # Calculate reference height (Median of existing peaks)\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        # Only fill gaps if median_rr suggests a normal rhythm (< 1.5s).\n",
    "        # If median_rr is already 2.0s (bradycardia), huge gaps are normal.\n",
    "        if median_rr < 1.5: \n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                        \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    # Lower threshold slightly for gap search\n",
    "                    low_thresh = mean_val * 0.6 \n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                            \n",
    "                        # 1. Check Sharpness\n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            # 2. Refine Position\n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            # 3. AMPLITUDE CHECK (The Fix)\n",
    "                            # Even if it's sharp, is it tall enough?\n",
    "                            # AV Block P-waves are sharp but short.\n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            # Must be at least 40-50% of the median R-peak height\n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "\n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # =================================================================\n",
    "\n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    # Relaxed expectations for Bradycardia/AV Block\n",
    "    min_expected_peaks = int(30/60 * segment_duration) \n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    # Fallback to robust only if counts are extremely off\n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        # Valid intervals widened to account for Bradycardia/Pauses\n",
    "        # valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 3.5)] \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)] \n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_data, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    max_len = len(ecg_data)\n",
    "    \n",
    "    if num_segments > 1:\n",
    "        window_step = (max_len - min_segment_length) / (num_segments - 1)\n",
    "        window_step = round(window_step)\n",
    "    else:\n",
    "        window_step = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = start_idx + min_segment_length\n",
    "        \n",
    "        if end_idx > max_len:\n",
    "            start_idx = max_len - min_segment_length\n",
    "            end_idx = max_len\n",
    "            \n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            end_idx = min(min_segment_length, max_len)\n",
    "        \n",
    "        segment = ecg_data[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment) < 100:\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        segment_duration = len(segment) / sampling_rate\n",
    "        ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(segment, sampling_rate, segment_duration)\n",
    "        print(f\"Segment {i+1}: Detected {len(r_peaks)} R-peaks, BPM: {bpm:.1f}\")\n",
    "        \n",
    "        adjusted_r_peaks = r_peaks + start_idx if len(r_peaks) > 0 else np.array([])\n",
    "        adjusted_cleaned_r = np.array(cleaned_r) + start_idx if len(cleaned_r) > 0 else np.array([])\n",
    "        \n",
    "        results.append({\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_filtered': ecg_filtered,\n",
    "            'r_peaks': adjusted_r_peaks,\n",
    "            'bpm': bpm,\n",
    "            'cleaned_r': adjusted_cleaned_r,\n",
    "            'ecg_raw': segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_ecg_segments(ecg_data, sampling_rate, results, title=\"ECG Segments with R-peaks and BPM\"):\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(15, 3*num_segments))\n",
    "    \n",
    "    if num_segments == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    for i, (ax, result) in enumerate(zip(axes, results)):\n",
    "        segment_num = result['segment_num']\n",
    "        start_idx = result['start_idx']\n",
    "        end_idx = result['end_idx']\n",
    "        bpm = result['bpm']\n",
    "        r_peaks = result['r_peaks']\n",
    "        \n",
    "        segment_time = time[start_idx:end_idx]\n",
    "        \n",
    "        ax.plot(segment_time, result['ecg_raw'], 'b-', alpha=0.7, linewidth=1, label='ECG Raw')\n",
    "        \n",
    "        if len(r_peaks) > 0:\n",
    "            r_times = r_peaks / sampling_rate\n",
    "            r_values = ecg_data[r_peaks.astype(int)]\n",
    "            ax.plot(r_times, r_values, 'ro', markersize=8, label='R-peaks', alpha=0.7)\n",
    "        \n",
    "        segment_duration = (end_idx - start_idx) / sampling_rate\n",
    "        ax.set_title(f'Segment {segment_num}: {start_idx}-{end_idx} samples '\n",
    "                    f'({segment_duration:.2f}s), BPM: {bpm:.1f}')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([segment_time[0], segment_time[-1]])\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ECG SEGMENT ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nSegment {result['segment_num']}:\")\n",
    "        print(f\"  Samples: {result['start_idx']}-{result['end_idx']}\")\n",
    "        print(f\"  Duration: {(result['end_idx']-result['start_idx'])/sampling_rate:.2f}s\")\n",
    "        print(f\"  BPM: {result['bpm']:.1f}\")\n",
    "        print(f\"  R-peaks detected: {len(result['r_peaks'])}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "def plot_full_ecg(ecg_data, sampling_rate, title=\"Full ECG Signal Analysis\"):\n",
    "    \"\"\"\n",
    "    Runs detection on the entire dataset and plots a single continuous view.\n",
    "    \"\"\"\n",
    "    # Run detection on the full unsegmented data\n",
    "    # Note: We ignore segment_duration to let the function calculate it automatically\n",
    "    _, r_peaks, global_bpm, _ = qrs_detect(ecg_data, sampling_rate)\n",
    "    \n",
    "    plt.figure(figsize=(20, 6)) # Width of 20 makes the 15k samples readable\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    # Plot the signal\n",
    "    plt.plot(time_axis, ecg_data, 'b-', linewidth=0.8, alpha=0.8, label='Filtered ECG')\n",
    "    \n",
    "    # Plot the peaks\n",
    "    if len(r_peaks) > 0:\n",
    "        # Filter out peaks that might be out of bounds (safety check)\n",
    "        valid_peaks = r_peaks[r_peaks < len(ecg_data)].astype(int)\n",
    "        \n",
    "        peak_times = valid_peaks / sampling_rate\n",
    "        peak_values = ecg_data[valid_peaks]\n",
    "        \n",
    "        plt.plot(peak_times, peak_values, 'ro', markersize=4, label='R-peaks')\n",
    "        \n",
    "        # Optional: Annotate every 5th peak to help navigation\n",
    "        for i, (t, v) in enumerate(zip(peak_times, peak_values)):\n",
    "            if i % 5 == 0:\n",
    "                plt.annotate(f'{t:.1f}s', (t, v), xytext=(0, 10), \n",
    "                             textcoords='offset points', ha='center', fontsize=8, color='red')\n",
    "\n",
    "    plt.title(f\"{title} | Global BPM: {global_bpm:.1f} | Total Peaks: {len(r_peaks)}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Normalized Amplitude\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Global Analysis: {len(r_peaks)} peaks detected over {len(ecg_data)/sampling_rate:.2f} seconds.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    " \n",
    "# # input_json = r\"simulator\\contec\\bigeminy_1756103016311.json\" \n",
    "# input_json = r\"simulator\\contec\\trigeminy_1756103085272.json\" \n",
    "# input_json = r\"simulator\\contec\\asystl_1756103447146.json\" \n",
    "# # input_json = r\"simulator\\contec\\1d av_1756104504294.json\" \n",
    "# input_json = r\"simulator\\contec\\3d av_1756104633918.json\" \n",
    "# input_json = r\"simulator\\contec\\280bpm_1756100716422.json\" \n",
    "# input_json = r\"simulator\\contec\\av sequence_1756106676125.json\"   #####\n",
    "# input_json = r\"simulator\\contec\\dmnd freq_1756106571373.json\"\n",
    "#    \n",
    "# input_json = r\"simulator\\fluke\\trigeminy_1754543043205.json\"   \n",
    "# input_json = r\"simulator\\fluke\\3d av_1754545068278.json\"   \n",
    "input_json = r\"simulator\\fluke\\asystole_1754544406847.json\"   \n",
    "\n",
    "with open(input_json, 'r') as file:\n",
    "    file_data = json.load(file)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# input_json = r\"v01_prob\\teton_ecg.ecgdatas.json\"  ####\n",
    "# with open(input_json, 'r') as file:\n",
    "#     all_id_data = json.load(file)\n",
    "\n",
    "# file_data = all_id_data[3]['ecgValue']   \n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"bpms\\afib_1766471694144.json\"\n",
    "# # input_json = r\"bpms\\bigeminy_1766467666407.json\"\n",
    "# input_json = r\"bpms\\pvc 6_1766467718685.json\"    ########\n",
    "# # input_json = r\"bpms\\tri_1766467618314.json\"\n",
    "# # input_json = r\"v01_prob/220_1767858669130.json\"\n",
    "# # input_json = r\"v01_prob/240bpm_1767858615562.json\"\n",
    "# # input_json = r\"v01_prob\\25 contec_1768375918389.json\"\n",
    "# # input_json = r\"v01_prob\\30bpm contec_1768375716454.json\"\n",
    "# # input_json = r\"v01_prob\\2d av_1754545008828.json\"  #####\n",
    "# # input_json = r\"v01_prob\\3rd_davb_1768554217066.json\"  #####\n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"exception\\L2_1759207950416.json\"  #####\n",
    "# input_json = r\"0_bpm\\L2_1760767200872.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760254470484.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760354748658.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760770290911.json\"  \n",
    "# # input_json = r\"issues\\L2_1757064122874.json\"  #####\n",
    "# # input_json = r\"v01_prob\\run 5 pvc.json\"  #####\n",
    "# # input_json = r\"issues\\L2_1757579288752.json\"\n",
    "# # input_json = r\"issues\\L2_1757737806463.json\"  #####\n",
    "# # input_json = r\"v01_prob\\L2_1765984517025.json\"  #####\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "# doubles = []\n",
    "# with open(input_json, \"rb\") as f:\n",
    "#     while chunk := f.read(8):\n",
    "#         if len(chunk) < 8:\n",
    "#             break\n",
    "#         value = struct.unpack(\"<d\", chunk)[0]\n",
    "#         doubles.append(value)\n",
    "\n",
    "# file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "#     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "#     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     return json.loads(decoded_string)\n",
    "\n",
    "# # input_json = r\"NHF2\\DATA_1750689015865.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750689460556.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750851207409.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750858856842.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750862721789.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "# # file_data = decrypt(input_json)\n",
    "\n",
    "# # input_json = r\"NHF\\DATA_1752067426678.json\"  #####\n",
    "# input_json = r\"NHF\\DATA_1752121970835.json\"  ########\n",
    "# # input_json = r\"NHF\\DATA_1754709586876.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted CSV file using AES ECB mode\"\"\"\n",
    "#     Private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    \n",
    "#     cipher = AES.new(Private_key.encode(), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     cipher = AES.new(Private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     data_list = decoded_string.split(\",\")\n",
    "#     float_list = [float(x) for x in data_list]\n",
    "\n",
    "#     return float_list\n",
    "\n",
    "# selected_path = \"v01_prob\\ECG_1735798172211.csv\"  ####\n",
    "# # selected_path = \"v01_prob\\ECG_L2_1738637533455.csv\"\n",
    "# file_data = decrypt(selected_path)\n",
    "# file_data = {'dataL2': file_data}\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def low_pass_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "\n",
    "def notch_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "    \n",
    "\n",
    "# data = data_process(file_data)\n",
    "data = data_process(low_pass_filter(notch_filter(file_data)))\n",
    "\n",
    "ecg_full = data[0, :15000, 0]\n",
    "# ecg_full = data[0, :15000, 0]\n",
    "sampling_rate = 500\n",
    "\n",
    "results = process_ecg_segments(\n",
    "    ecg_data=ecg_full,\n",
    "    sampling_rate=sampling_rate,\n",
    "    num_segments=4,\n",
    "    min_segment_length=4500\n",
    ")\n",
    "\n",
    "\n",
    "plot_ecg_segments(ecg_full, sampling_rate, results, \"ECG Analysis: 7 Segments with R-peak Detection\")\n",
    "\n",
    "# 2. Run the Full Data Plot (New logic)\n",
    "\n",
    "print(\"\\n--- Plotting Full Data ---\")\n",
    "plot_full_ecg(ecg_full, sampling_rate, \"Final Full Data View\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a296b3",
   "metadata": {},
   "source": [
    "## bpm normal & death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pywt\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64   \n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "\n",
    "# Define filter coefficients if not defined\n",
    "fs = 500  # sampling rate\n",
    "nyq = 0.5 * fs\n",
    "\n",
    "# Example low pass filter (cutoff 40 Hz)\n",
    "low_cutoff = 40 / nyq\n",
    "b_lp, a_lp = sp_signal.butter(4, low_cutoff, btype='low')\n",
    "\n",
    "# Example notch filter (50 Hz)\n",
    "q = 30\n",
    "w0 = 50 / nyq\n",
    "b_notch, a_notch = sp_signal.iirnotch(w0, q)\n",
    "\n",
    "def decrypt(input_file):\n",
    "    \"\"\"Decrypt encrypted JSON file\"\"\"\n",
    "    private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    with open(input_file, 'rb') as f:\n",
    "        encrypted_data = f.read()\n",
    "    enc = base64.b64decode(encrypted_data[24:])\n",
    "    data = unpad(cipher.decrypt(enc), 16)\n",
    "    decoded_string = data.decode('utf-8')\n",
    "    return json.loads(decoded_string)\n",
    "\n",
    "    \n",
    "def baseline_wander(X):\n",
    "    def get_median_filter_width(sampling_rate, duration):\n",
    "        res = int(sampling_rate * duration)\n",
    "        res += (res % 2) - 1\n",
    "        return res\n",
    "\n",
    "    ms_flt_array = [0.2, 0.6]\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype=\"int\")\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(500, ms_flt_array[i])\n",
    "    X0 = X\n",
    "    for mi in range(0, len(mfa)):\n",
    "        X0 = sp_signal.medfilt(X0, mfa[mi])\n",
    "    X0 = np.subtract(X, X0)\n",
    "    return X0\n",
    "\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    \"\"\"Normalize signal to range [0, 1].\"\"\"\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    data = normalize(signal_data, min_val, max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_process(filename):\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "    \n",
    "    for key in keys:\n",
    "        sig = np.array(filename[key])\n",
    "        datas.append(sig.astype('float32'))\n",
    "    \n",
    "    datas_array = np.array(datas)               # shape: (1, length) or (channels, length)\n",
    "    \n",
    "    # ── Compute real (raw) statistics here ───────────────────────────────\n",
    "    raw_min   = np.min(datas_array)\n",
    "    raw_max   = np.max(datas_array)\n",
    "    raw_mean  = np.mean(datas_array)\n",
    "    raw_std   = np.std(datas_array)\n",
    "    raw_var   = np.var(datas_array)\n",
    "    raw_median = np.median(datas_array)\n",
    "    \n",
    "    print(\"\\nRaw (pre-normalized) signal statistics:\")\n",
    "    print(f\"  Min    = {raw_min:12.4f}\")\n",
    "    print(f\"  Max    = {raw_max:12.4f}\")\n",
    "    print(f\"  Mean   = {raw_mean:12.4f}\")\n",
    "    print(f\"  Std    = {raw_std:12.4f}\")\n",
    "    print(f\"  Var    = {raw_var:14.6f}\")\n",
    "    print(f\"  Median = {raw_median:12.4f}\")\n",
    "    print(f\"  Range  = {raw_max - raw_min:.4f}\\n\")\n",
    "    \n",
    "    # Now do normalization (your existing code)\n",
    "    min_val = raw_min\n",
    "    max_val = raw_max\n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(normalize(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    \n",
    "    return final_data, {\n",
    "        'raw_min': raw_min, 'raw_max': raw_max, 'raw_mean': raw_mean,\n",
    "        'raw_std': raw_std, 'raw_var': raw_var, 'raw_median': raw_median,\n",
    "        'raw_range': raw_max - raw_min\n",
    "    }, datas_array[0]  # return flattened raw for simplicity\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0: \n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        # If we have 2-3 outlier peaks, remove them\n",
    "        # if 2 <= high_amp_count <= 3:\n",
    "        # if 3 <= high_amp_count <= 4 and len(peaks) - high_amp_count > 0:\n",
    "        # if 5 <= high_amp_count <= 6 and len(peaks) - high_amp_count > 0:\n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            # mask[high_amp_indices] = False\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            # Check if this could be a T-wave (160-450ms after R-peak)\n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                # T-waves are typically smaller and wider\n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    # Measure width at half maximum\n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    # T-waves are wider than QRS complexes\n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    \"\"\"Multi-strategy robust QRS detection for difficult cases\"\"\"\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated, \n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None, raw_segment=None):\n",
    "    if raw_segment is not None:\n",
    "        var_raw = np.var(raw_segment)\n",
    "        # if var_raw < 0.0095:                  \n",
    "        if var_raw < 0.005:                  \n",
    "            print(f\"Raw variance {var_raw:.6f} < 0.0095 → treating as asystole / flatline\")\n",
    "            return data, np.array([]), 0.0, np.array([])\n",
    "    else:\n",
    "        var = np.var(data)\n",
    "        if var < 0.00015:                     \n",
    "            print(f\"Normalized variance {var:.6f} too low → possible asystole\")\n",
    "            return data, np.array([]), 0.0, np.array([])\n",
    "\n",
    "    # data_clean = baseline_wander(data) \n",
    "\n",
    "    data_clean = data \n",
    "\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "            \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # =================================================================\n",
    "    # CRITICAL FIX: GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    # =================================================================\n",
    "    if len(cleaned_r) >= 2:\n",
    "        # Calculate reference height (Median of existing peaks)\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        # Only fill gaps if median_rr suggests a normal rhythm (< 1.5s).\n",
    "        # If median_rr is already 2.0s (bradycardia), huge gaps are normal.\n",
    "        if median_rr < 1.5: \n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                        \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    # Lower threshold slightly for gap search\n",
    "                    low_thresh = mean_val * 0.6 \n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                            \n",
    "                        # 1. Check Sharpness\n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            # 2. Refine Position\n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            # 3. AMPLITUDE CHECK (The Fix)\n",
    "                            # Even if it's sharp, is it tall enough?\n",
    "                            # AV Block P-waves are sharp but short.\n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            # Must be at least 40-50% of the median R-peak height\n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "\n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # =================================================================\n",
    "\n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    # Relaxed expectations for Bradycardia/AV Block\n",
    "    min_expected_peaks = int(30/60 * segment_duration) \n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    # Fallback to robust only if counts are extremely off\n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        # Valid intervals widened to account for Bradycardia/Pauses\n",
    "        # valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 3.5)] \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)] \n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_data, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    max_len = len(ecg_data)\n",
    "    \n",
    "    if num_segments > 1:\n",
    "        window_step = (max_len - min_segment_length) / (num_segments - 1)\n",
    "        window_step = round(window_step)\n",
    "    else:\n",
    "        window_step = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = start_idx + min_segment_length\n",
    "        \n",
    "        if end_idx > max_len:\n",
    "            start_idx = max_len - min_segment_length\n",
    "            end_idx = max_len\n",
    "            \n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            end_idx = min(min_segment_length, max_len)\n",
    "        \n",
    "        segment = ecg_data[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment) < 100:\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        segment_duration = len(segment) / sampling_rate\n",
    "        # ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(segment, sampling_rate, segment_duration)\n",
    "\n",
    "        raw_segment = raw_ecg[start_idx:end_idx]   # ← the real raw amplitudes\n",
    "        ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(\n",
    "            segment,\n",
    "            sampling_rate,\n",
    "            segment_duration,\n",
    "            raw_segment=raw_segment                # ← pass raw here\n",
    "        )\n",
    "\n",
    "        print(f\"Segment {i+1}: Detected {len(r_peaks)} R-peaks, BPM: {bpm:.1f}\")\n",
    "        \n",
    "        adjusted_r_peaks = r_peaks + start_idx if len(r_peaks) > 0 else np.array([])\n",
    "        adjusted_cleaned_r = np.array(cleaned_r) + start_idx if len(cleaned_r) > 0 else np.array([])\n",
    "        \n",
    "        results.append({\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_filtered': ecg_filtered,\n",
    "            'r_peaks': adjusted_r_peaks,\n",
    "            'bpm': bpm,\n",
    "            'cleaned_r': adjusted_cleaned_r,\n",
    "            'ecg_raw': segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_ecg_stats(signal, fs=500):\n",
    "    \"\"\"Compute common statistics for an ECG segment\"\"\"\n",
    "    if len(signal) == 0:\n",
    "        return {\n",
    "            'nsamples': 0,\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'var': np.nan,\n",
    "            'min': np.nan,\n",
    "            'max': np.nan,\n",
    "            'median': np.nan,\n",
    "            'rms': np.nan,\n",
    "            'duration_s': 0.0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'nsamples': len(signal),\n",
    "        'mean': float(np.mean(signal)),\n",
    "        'std': float(np.std(signal)),\n",
    "        'var': float(np.var(signal)),\n",
    "        'min': float(np.min(signal)),\n",
    "        'max': float(np.max(signal)),\n",
    "        'median': float(np.median(signal)),\n",
    "        'rms': float(np.sqrt(np.mean(signal**2))),\n",
    "        'duration_s': len(signal) / fs\n",
    "    }\n",
    "\n",
    "\n",
    "def format_stats_text(stats, prefix=\"\"):\n",
    "    \"\"\"Create a compact multi-line stats string for plotting\"\"\"\n",
    "    lines = [\n",
    "        f\"{prefix}Duration: {stats['duration_s']:.2f} s\",\n",
    "        f\"Samples:   {stats['nsamples']}\",\n",
    "        f\"Mean:      {stats['mean']:.4f}\",\n",
    "        f\"Std:       {stats['std']:.4f}\",\n",
    "        f\"Var:       {stats['var']:.6f}\",\n",
    "        f\"Min / Max: {stats['min']:.4f} / {stats['max']:.4f}\",\n",
    "        f\"Median:    {stats['median']:.4f}\",\n",
    "        f\"RMS:       {stats['rms']:.4f}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def plot_ecg_segments(ecg_data, sampling_rate, results, title=\"ECG Segments with R-peaks and BPM\", raw_ecg=None):\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(15, 3.5 * num_segments), sharex=False)\n",
    "    \n",
    "    if num_segments == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    global_stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "    fig.suptitle(f\"{title}\\nFull signal stats: {global_stats['duration_s']:.1f}s | \"\n",
    "                 f\"mean={global_stats['mean']:.4f}  std={global_stats['std']:.4f}\", \n",
    "                 fontsize=13, y=0.98)\n",
    "    \n",
    "    for i, (ax, result) in enumerate(zip(axes, results)):\n",
    "        segment_num = result['segment_num']\n",
    "        start_idx = result['start_idx']\n",
    "        end_idx = result['end_idx']\n",
    "        bpm = result['bpm']\n",
    "        r_peaks = result['r_peaks']\n",
    "        \n",
    "        segment_time = time[start_idx:end_idx]\n",
    "        segment_data = result['ecg_raw']\n",
    "        \n",
    "        ax.plot(segment_time, segment_data, 'b-', alpha=0.8, linewidth=1.1, label='ECG')\n",
    "        \n",
    "        if len(r_peaks) > 0:\n",
    "            r_times = r_peaks / sampling_rate\n",
    "            r_values = ecg_data[r_peaks.astype(int)]\n",
    "            ax.plot(r_times, r_values, 'ro', markersize=7, label='R-peaks', alpha=0.85)\n",
    "        \n",
    "        # ── Statistics box per segment (use raw if available) ───────────────────────────────\n",
    "        if raw_ecg is not None:\n",
    "            raw_segment = raw_ecg[start_idx:end_idx]\n",
    "            seg_stats = compute_ecg_stats(raw_segment, sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            seg_stats = compute_ecg_stats(segment_data, sampling_rate)\n",
    "            prefix = \"\"\n",
    "        stats_text = format_stats_text(seg_stats, prefix + f\"Seg {segment_num}  \")\n",
    "        stats_text += f\"\\nBPM:       {bpm:.1f}\"\n",
    "        \n",
    "        ax.text(0.02, 0.98, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=9.5,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.82, edgecolor='gray', boxstyle='round,pad=0.4'))\n",
    "        \n",
    "        segment_duration = (end_idx - start_idx) / sampling_rate\n",
    "        ax.set_title(f'Segment {segment_num}: {start_idx:,} – {end_idx:,}  |  BPM: {bpm:.1f}')\n",
    "        ax.set_ylabel('Amplitude (norm)')\n",
    "        ax.grid(True, alpha=0.35, linestyle='--')\n",
    "        ax.set_xlim([segment_time[0], segment_time[-1]])\n",
    "        ax.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (seconds)')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])   # make room for suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # ── Console summary ───────────────────────────────────────────────\n",
    "    print(\"═\" * 70)\n",
    "    print(\"ECG SEGMENT STATISTICS SUMMARY\")\n",
    "    print(\"═\" * 70)\n",
    "    for res in results:\n",
    "        if raw_ecg is not None:\n",
    "            s = compute_ecg_stats(raw_ecg[res['start_idx']:res['end_idx']], sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            s = compute_ecg_stats(res['ecg_raw'], sampling_rate)\n",
    "            prefix = \"Norm \"\n",
    "        print(f\"Segment {res['segment_num']:2d} | {s['duration_s']:5.2f}s | \"\n",
    "              f\"mean={s['mean']:8.4f}  std={s['std']:7.4f}  BPM={res['bpm']:5.1f} ({prefix.strip()})\"\n",
    "            )\n",
    "    print(\"═\" * 70)\n",
    "    \n",
    "    \n",
    "def plot_full_ecg(ecg_data, sampling_rate, title=\"Full ECG Signal Analysis\", raw_ecg=None):\n",
    "    \"\"\"\n",
    "    Runs detection on the entire dataset and plots a single continuous view.\n",
    "    \"\"\"\n",
    "    # _, r_peaks, global_bpm, _ = qrs_detect(ecg_data, sampling_rate)\n",
    "    _, r_peaks, global_bpm, _ = qrs_detect(\n",
    "        ecg_data,\n",
    "        sampling_rate,\n",
    "        raw_segment=raw_ecg[:len(ecg_data)]    # pass corresponding raw part\n",
    "    )\n",
    "        \n",
    "    if raw_ecg is not None:\n",
    "        stats = compute_ecg_stats(raw_ecg[:len(ecg_data)], sampling_rate)\n",
    "        prefix = \"Raw \"\n",
    "    else:\n",
    "        stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "        prefix = \"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 6)) # Width of 20 makes the 15k samples readable\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    # Plot the signal\n",
    "    plt.plot(time_axis, ecg_data, 'b-', linewidth=0.8, alpha=0.8, label='Filtered ECG')\n",
    "    \n",
    "    # Plot the peaks\n",
    "    if len(r_peaks) > 0:\n",
    "        # Filter out peaks that might be out of bounds (safety check)\n",
    "        valid_peaks = r_peaks[r_peaks < len(ecg_data)].astype(int)\n",
    "        \n",
    "        peak_times = valid_peaks / sampling_rate\n",
    "        peak_values = ecg_data[valid_peaks]\n",
    "        \n",
    "        plt.plot(peak_times, peak_values, 'ro', markersize=4, label='R-peaks')\n",
    "        \n",
    "        # Optional: Annotate every 5th peak to help navigation\n",
    "        for i, (t, v) in enumerate(zip(peak_times, peak_values)):\n",
    "            if i % 5 == 0:\n",
    "                plt.annotate(f'{t:.1f}s', (t, v), xytext=(0, 10), \n",
    "                             textcoords='offset points', ha='center', fontsize=8, color='red')\n",
    "\n",
    "    plt.title(f\"{title} | Global BPM: {global_bpm:.1f} | Total Peaks: {len(r_peaks)}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Normalized Amplitude\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Global Analysis: {len(r_peaks)} peaks detected over {len(ecg_data)/sampling_rate:.2f} seconds.\")\n",
    "    print(f\"{prefix}Full signal stats →  mean={stats['mean']:.4f}  std={stats['std']:.4f}  var={stats['var']:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    " \n",
    "# input_json = r\"simulator\\contec\\trigeminy_1756103085272.json\" \n",
    "input_json = r\"simulator\\contec\\asystl_1756103447146.json\" \n",
    "# input_json = r\"simulator\\contec\\1d av_1756104504294.json\"  \n",
    "# input_json = r\"simulator\\contec\\3d av_1756104633918.json\"  \n",
    "# input_json = r\"simulator\\contec\\280bpm_1756100716422.json\" \n",
    "# input_json = r\"simulator\\contec\\av sequence_1756106676125.json\"  #####\n",
    "# input_json = r\"simulator\\contec\\dmnd freq_1756106571373.json\"  \n",
    "#    \n",
    "# input_json = r\"simulator\\fluke\\trigeminy_1754543043205.json\"   \n",
    "# input_json = r\"simulator\\fluke\\3d av_1754545068278.json\"   \n",
    "# input_json = r\"simulator\\fluke\\asystole_1754544406847.json\"   \n",
    "\n",
    "with open(input_json, 'r') as file:\n",
    "    file_data = json.load(file)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# input_json = r\"v01_prob\\teton_ecg.ecgdatas.json\"  ####\n",
    "# with open(input_json, 'r') as file:\n",
    "#     all_id_data = json.load(file)\n",
    "\n",
    "# file_data = all_id_data[3]['ecgValue']   \n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"bpms\\afib_1766471694144.json\"\n",
    "# # input_json = r\"bpms\\bigeminy_1766467666407.json\"\n",
    "# # input_json = r\"bpms\\pvc 6_1766467718685.json\"    ########\n",
    "# # input_json = r\"bpms\\tri_1766467618314.json\"\n",
    "# # input_json = r\"v01_prob/220_1767858669130.json\"\n",
    "# # input_json = r\"v01_prob/240bpm_1767858615562.json\"\n",
    "# # input_json = r\"v01_prob\\25 contec_1768375918389.json\"\n",
    "# # input_json = r\"v01_prob\\30bpm contec_1768375716454.json\"\n",
    "# # input_json = r\"v01_prob\\2d av_1754545008828.json\"  #####\n",
    "# input_json = r\"v01_prob\\3rd_davb_1768554217066.json\"  #####\n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# input_json = r\"exception\\L2_1759207950416.json\"  #####\n",
    "# # input_json = r\"0_bpm\\L2_1760767200872.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760254470484.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760354748658.json\"  \n",
    "# # input_json = r\"0_bpm\\L2_1760770290911.json\"  \n",
    "# # input_json = r\"issues\\L2_1757064122874.json\"  #####\n",
    "# # input_json = r\"v01_prob\\run 5 pvc.json\"  #####\n",
    "# # input_json = r\"issues\\L2_1757579288752.json\"\n",
    "# # input_json = r\"issues\\L2_1757737806463.json\"  #####\n",
    "# # input_json = r\"v01_prob\\L2_1765984517025.json\"  #####\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "# # input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "# doubles = []\n",
    "# with open(input_json, \"rb\") as f:\n",
    "#     while chunk := f.read(8):\n",
    "#         if len(chunk) < 8:\n",
    "#             break\n",
    "#         value = struct.unpack(\"<d\", chunk)[0]\n",
    "#         doubles.append(value)\n",
    "\n",
    "# file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "#     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "#     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     return json.loads(decoded_string)\n",
    "\n",
    "# # input_json = r\"NHF2\\DATA_1750689015865.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750689460556.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750851207409.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750858856842.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750862721789.json\"\n",
    "# input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "# input_json = r\"NHF\\DATA_1752067426678.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1752121970835.json\"  ########\n",
    "# # input_json = r\"NHF\\DATA_1754709586876.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted CSV file using AES ECB mode\"\"\"\n",
    "#     Private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    \n",
    "#     cipher = AES.new(Private_key.encode(), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     cipher = AES.new(Private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     data_list = decoded_string.split(\",\")\n",
    "#     float_list = [float(x) for x in data_list]\n",
    "\n",
    "#     return float_list\n",
    "\n",
    "# # selected_path = \"v01_prob\\ECG_1735798172211.csv\"  ####\n",
    "# selected_path = \"v01_prob\\ECG_L2_1738637533455.csv\"\n",
    "# file_data = decrypt(selected_path)\n",
    "# file_data = {'dataL2': file_data}\n",
    "\n",
    "\n",
    "def low_pass_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "\n",
    "def notch_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "    \n",
    "\n",
    "# data = data_process(file_data)\n",
    "processed_data, raw_global_stats, raw_ecg = data_process(\n",
    "    low_pass_filter(notch_filter(file_data))\n",
    ")\n",
    "\n",
    "ecg_full = processed_data[0, :15000, 0]\n",
    "sampling_rate = 500\n",
    "\n",
    "results = process_ecg_segments(\n",
    "    ecg_data=ecg_full,\n",
    "    sampling_rate=sampling_rate,\n",
    "    num_segments=4,\n",
    "    min_segment_length=4500\n",
    ")\n",
    "\n",
    "\n",
    "plot_ecg_segments(ecg_full, sampling_rate, results, \"ECG Analysis: 4 Segments with R-peak Detection\", raw_ecg=raw_ecg)\n",
    "\n",
    "print(\"\\n--- Plotting Full Data ---\")\n",
    "plot_full_ecg(ecg_full, sampling_rate, \"Final Full Data View\", raw_ecg=raw_ecg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5194db",
   "metadata": {},
   "source": [
    "## interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9306e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# ==========================================\n",
    "# 1. UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def baseline_wander(data, sampling_rate=500, knot_spacing=0.4):\n",
    "    \"\"\"\n",
    "    Cubic spline interpolation - fits smooth curve through evenly spaced points.\n",
    "    knot_spacing: distance between knots in seconds\n",
    "    \"\"\"\n",
    "    n_samples = len(data)\n",
    "    knot_interval = int(sampling_rate * knot_spacing)\n",
    "    \n",
    "    # Create knot points at regular intervals\n",
    "    knot_indices = np.arange(0, n_samples, knot_interval)\n",
    "    if knot_indices[-1] != n_samples - 1:\n",
    "        knot_indices = np.append(knot_indices, n_samples - 1)\n",
    "    \n",
    "    # Use percentile at each knot region to estimate baseline (robust to QRS)\n",
    "    knot_values = []\n",
    "    half_window = knot_interval // 2\n",
    "    for idx in knot_indices:\n",
    "        start = max(0, idx - half_window)\n",
    "        end = min(n_samples, idx + half_window)\n",
    "        knot_values.append(np.percentile(data[start:end], 50))\n",
    "    \n",
    "    # Fit cubic spline and subtract\n",
    "    spline = CubicSpline(knot_indices, knot_values)\n",
    "    baseline = spline(np.arange(n_samples))\n",
    "    \n",
    "    return data - baseline\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    return normalize(signal_data, min_val, max_val)\n",
    "\n",
    "def data_process(input_data):\n",
    "    \"\"\"\n",
    "    Robust data processing that handles both dictionary inputs and direct array inputs.\n",
    "    Fixed the FutureWarning issue.\n",
    "    \"\"\"\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "   \n",
    "    # Check if input is a dictionary and has the key\n",
    "    if isinstance(input_data, dict) and 'dataL2' in input_data:\n",
    "        raw_data = input_data['dataL2']\n",
    "    else:\n",
    "        # Assume it's already the data array\n",
    "        raw_data = input_data\n",
    "        \n",
    "    sig = np.array(raw_data)\n",
    "    datas.append(sig.astype('float32'))\n",
    "   \n",
    "    datas_array = np.array(datas)\n",
    "    min_val = np.min(datas_array)\n",
    "    max_val = np.max(datas_array)\n",
    "   \n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(process_signal(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated,\n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None):\n",
    "    \"\"\"Enhanced QRS detection with Amplitude Guardrails for AV Blocks\"\"\"\n",
    "    data_clean = data\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "        \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    if len(cleaned_r) >= 2:\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        if median_rr < 1.5:\n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                    \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    low_thresh = mean_val * 0.6\n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                        \n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "        \n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    min_expected_peaks = int(30/60 * segment_duration)\n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)]\n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "# ==========================================\n",
    "# 2. P-WAVE DETECTION\n",
    "# ==========================================\n",
    "\n",
    "def adaptive_noise_filter(segment, sampling_rate):\n",
    "    \"\"\"Apply stronger filtering in noisy regions\"\"\"\n",
    "    diff = np.diff(segment)\n",
    "    noise_std = np.std(diff)\n",
    "    \n",
    "    if noise_std > 0.05:\n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        low = 1 / nyquist\n",
    "        high = 25 / nyquist\n",
    "        b, a = sp_signal.butter(3, [low, high], btype='band')\n",
    "        return sp_signal.filtfilt(b, a, segment)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "def calculate_signal_quality(segment):\n",
    "    \"\"\"Calculate signal quality index (0-1, higher is better)\"\"\"\n",
    "    diff = np.diff(segment)\n",
    "    noise_level = np.std(diff)\n",
    "    noise_score = np.exp(-noise_level / 0.1)\n",
    "    \n",
    "    baseline_drift = np.std(segment)\n",
    "    drift_score = np.exp(-baseline_drift / 0.3)\n",
    "    \n",
    "    signal_range = np.max(segment) - np.min(segment)\n",
    "    if 0.2 <= signal_range <= 1.5:\n",
    "        amplitude_score = 1.0\n",
    "    else:\n",
    "        amplitude_score = 0.5\n",
    "    \n",
    "    quality = (noise_score * 0.4 + drift_score * 0.4 + amplitude_score * 0.2)\n",
    "    \n",
    "    return quality\n",
    "\n",
    "def enhanced_p_wave_detection(signal, r_peaks, sampling_rate, segment_num):\n",
    "    \"\"\"Enhanced P-wave detection with Bradycardia fix\"\"\"\n",
    "    if len(r_peaks) < 3:\n",
    "        return np.full(len(r_peaks), np.nan)\n",
    "    \n",
    "    signal_quality = calculate_signal_quality(signal)\n",
    "    \n",
    "    p_peaks = np.full(len(r_peaks), np.nan)\n",
    "    p_qualities = np.zeros(len(r_peaks))\n",
    "    \n",
    "    if len(r_peaks) > 1:\n",
    "        rr_intervals = np.diff(r_peaks) / sampling_rate\n",
    "        avg_rr = np.mean(rr_intervals)\n",
    "        avg_hr = 60 / avg_rr if avg_rr > 0 else 0\n",
    "    else:\n",
    "        avg_hr = 0\n",
    "    \n",
    "    print(f\"Segment {segment_num}: Signal Quality = {signal_quality:.2f}, Avg HR = {avg_hr:.0f} BPM\")\n",
    "    \n",
    "    if avg_hr > 180:\n",
    "        # print(f\"  High heart rate detected - using adaptive short-cycle parameters\")\n",
    "        use_adaptive_short_cycle = True\n",
    "        min_quality_threshold = 20  \n",
    "    elif avg_hr > 120:\n",
    "        use_adaptive_short_cycle = True\n",
    "        min_quality_threshold = 30\n",
    "    else:\n",
    "        use_adaptive_short_cycle = False\n",
    "        if signal_quality > 0.7:\n",
    "            min_quality_threshold = 50\n",
    "        elif signal_quality > 0.5:\n",
    "            min_quality_threshold = 35\n",
    "        else:\n",
    "            min_quality_threshold = 25\n",
    "    \n",
    "    preliminary_pr_intervals = []\n",
    "    preliminary_p_amps = []\n",
    "    \n",
    "    t_wave_ends = []\n",
    "    for i in range(len(r_peaks) - 1):\n",
    "        r_curr = int(r_peaks[i])\n",
    "        r_next = int(r_peaks[i + 1])\n",
    "        rr_interval = r_next - r_curr\n",
    "        \n",
    "        if rr_interval < 0.4 * sampling_rate:  \n",
    "            estimated_t_end = r_curr + int(0.45 * rr_interval)\n",
    "        elif rr_interval < 0.6 * sampling_rate: \n",
    "            estimated_t_end = r_curr + int(0.55 * rr_interval)\n",
    "        else:  \n",
    "            estimated_t_end = r_curr + min(int(0.5 * sampling_rate), int(0.65 * rr_interval))\n",
    "        \n",
    "        t_wave_ends.append(estimated_t_end)\n",
    "    \n",
    "    if len(r_peaks) > 0:\n",
    "        last_r = int(r_peaks[-1])\n",
    "        t_wave_ends.append(last_r + int(0.5 * sampling_rate))\n",
    "    \n",
    "    for i, r in enumerate(r_peaks):\n",
    "        if i == 0: continue\n",
    "            \n",
    "        r = int(r)\n",
    "        rr_prev = r - int(r_peaks[i-1])\n",
    "        \n",
    "        if use_adaptive_short_cycle and rr_prev < 0.5 * sampling_rate:  \n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                t_end_prev = t_wave_ends[i - 1]\n",
    "                search_start = t_end_prev + int(0.02 * sampling_rate)\n",
    "            else:\n",
    "                search_start = int(r_peaks[i-1] + 0.25 * rr_prev)\n",
    "            \n",
    "            search_end = int(r - 0.02 * sampling_rate)\n",
    "            min_pr_ms = 80\n",
    "            max_pr_ms = 300\n",
    "            \n",
    "        else:\n",
    "            max_lookback_samples = int(0.40 * sampling_rate)\n",
    "            earliest_allowed_start = r - max_lookback_samples\n",
    "            \n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                t_end_prev = t_wave_ends[i - 1]\n",
    "                search_start = max(t_end_prev + int(0.05 * sampling_rate), earliest_allowed_start)\n",
    "            else:\n",
    "                search_start = max(int(r_peaks[i-1] + 0.4 * sampling_rate), earliest_allowed_start)\n",
    "            \n",
    "            search_end = int(r - 0.03 * sampling_rate)\n",
    "            min_pr_ms = 80\n",
    "            max_pr_ms = 400\n",
    "        \n",
    "        search_start = max(0, search_start)\n",
    "        search_end = min(len(signal)-1, search_end)\n",
    "        \n",
    "        min_window_size = int(0.05 * sampling_rate)\n",
    "        if search_end - search_start < min_window_size:\n",
    "            continue\n",
    "        \n",
    "        segment = signal[search_start:search_end]\n",
    "        segment_filtered = adaptive_noise_filter(segment, sampling_rate)\n",
    "        \n",
    "        if use_adaptive_short_cycle:\n",
    "            min_prominence = 0.002\n",
    "            min_distance = int(0.05 * sampling_rate)\n",
    "            max_width = int(0.12 * sampling_rate)\n",
    "        else:\n",
    "            min_prominence = 0.003\n",
    "            min_distance = int(0.08 * sampling_rate)\n",
    "            max_width = int(0.15 * sampling_rate)\n",
    "        \n",
    "        try:\n",
    "            candidate_peaks, properties = sp_signal.find_peaks(\n",
    "                segment_filtered,\n",
    "                distance=min_distance,\n",
    "                prominence=min_prominence,\n",
    "                width=(int(0.02*sampling_rate), max_width)\n",
    "            )\n",
    "        except:\n",
    "            candidate_peaks = []\n",
    "        \n",
    "        if len(candidate_peaks) == 0:\n",
    "            continue\n",
    "        \n",
    "        candidate_peaks = search_start + candidate_peaks\n",
    "        \n",
    "        best_score = -np.inf\n",
    "        best_peak = None\n",
    "        \n",
    "        for cp in candidate_peaks:\n",
    "            cp = int(cp)\n",
    "            \n",
    "            pr_interval = (r - cp) / sampling_rate * 1000\n",
    "            if pr_interval < min_pr_ms or pr_interval > max_pr_ms:\n",
    "                continue\n",
    "            \n",
    "            score = 0\n",
    "            \n",
    "            if use_adaptive_short_cycle:\n",
    "                ideal_pr = 120\n",
    "                sigma = 40\n",
    "            else:\n",
    "                ideal_pr = 160\n",
    "                sigma = 60\n",
    "            \n",
    "            score += np.exp(-((pr_interval - ideal_pr) ** 2) / (2 * sigma ** 2)) * 150\n",
    "            \n",
    "            p_amp = abs(signal[cp])\n",
    "            if p_amp > 0.5:\n",
    "                score += 50 \n",
    "            elif 0.015 <= p_amp <= 0.5:\n",
    "                ideal_amp = 0.08\n",
    "                score += np.exp(-((p_amp - ideal_amp) ** 2) / (2 * 0.10 ** 2)) * 400\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                left_slope = signal[cp] - signal[cp - 5]\n",
    "                right_slope = signal[cp + 5] - signal[cp]\n",
    "                symmetry = 1 - abs(left_slope - right_slope) / (abs(left_slope) + abs(right_slope) + 1e-6)\n",
    "                score += symmetry * 60\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            score += 50 \n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_peak = cp\n",
    "        \n",
    "        if best_peak is not None and best_score > min_quality_threshold:\n",
    "             p_peaks[i] = best_peak\n",
    "             p_qualities[i] = best_score\n",
    "             preliminary_pr_intervals.append((r - best_peak) / sampling_rate * 1000)\n",
    "             preliminary_p_amps.append(abs(signal[best_peak]))\n",
    "\n",
    "    if len(preliminary_pr_intervals) >= 3:\n",
    "        median_pr = np.median(preliminary_pr_intervals)\n",
    "        \n",
    "        for i in range(1, len(r_peaks)):\n",
    "            if np.isnan(p_peaks[i]): continue\n",
    "            \n",
    "            pr = (r_peaks[i] - p_peaks[i]) / sampling_rate * 1000\n",
    "            \n",
    "            tolerance = 100 if use_adaptive_short_cycle else 120\n",
    "            if abs(pr - median_pr) > tolerance:\n",
    "                p_peaks[i] = np.nan\n",
    "                p_qualities[i] = 0\n",
    "\n",
    "    return p_peaks\n",
    "\n",
    "# ==========================================\n",
    "# 4. P-WAVE ONSET DETECTION\n",
    "# ==========================================\n",
    "\n",
    "def find_p_onset_constrained(signal, p_peak_idx, sampling_rate, prev_t_offset=None):\n",
    "    \"\"\"\n",
    "    Finds P-onset with strict constraints\n",
    "    \"\"\"\n",
    "    if np.isnan(p_peak_idx): return np.nan\n",
    "    p_idx = int(p_peak_idx)\n",
    "    \n",
    "    min_dist_samples = int(0.012 * sampling_rate)\n",
    "    start_search = p_idx - min_dist_samples\n",
    "    \n",
    "    max_lookback = int(0.12 * sampling_rate)\n",
    "    default_limit = p_idx - max_lookback\n",
    "    \n",
    "    if prev_t_offset is not None and not np.isnan(prev_t_offset):\n",
    "        t_end_buffer = int(prev_t_offset) + int(0.02 * sampling_rate)\n",
    "        limit_idx = max(default_limit, t_end_buffer)\n",
    "    else:\n",
    "        limit_idx = default_limit\n",
    "\n",
    "    if limit_idx >= start_search:\n",
    "        return start_search\n",
    "        \n",
    "    limit_idx = max(0, limit_idx)\n",
    "    \n",
    "    segment = signal[limit_idx:start_search + 1]\n",
    "    \n",
    "    if len(segment) < 3:\n",
    "        return limit_idx\n",
    "        \n",
    "    grads = np.gradient(segment)\n",
    "    \n",
    "    max_slope = np.max(np.abs(grads))\n",
    "    threshold = 0.10 * max_slope\n",
    "    \n",
    "    for i in range(len(grads) - 1, -1, -1):\n",
    "        if np.abs(grads[i]) < threshold:\n",
    "            found_idx = limit_idx + i\n",
    "            return found_idx\n",
    "            \n",
    "    return limit_idx\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. WAVE DELINEATION\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def find_t_wave_offset_with_stability(signal, t_peak_idx, sampling_rate, \n",
    "                                      next_r_peak=None, limit_idx=None):\n",
    "    if np.isnan(t_peak_idx):\n",
    "        return np.nan\n",
    "    \n",
    "    t_peak_idx = int(t_peak_idx)\n",
    "    t_peak_value = signal[t_peak_idx]\n",
    "    \n",
    "    # ===== STAGE 1: Determine Search Limits =====\n",
    "    if next_r_peak is not None:\n",
    "        max_search = int(0.70 * (next_r_peak - t_peak_idx))\n",
    "        max_search = max(int(0.100 * sampling_rate), max_search)\n",
    "    else:\n",
    "        max_search = int(0.200 * sampling_rate)\n",
    "    \n",
    "    if limit_idx is not None and not np.isnan(limit_idx):\n",
    "        limit_idx = int(limit_idx)\n",
    "        dist_to_limit = limit_idx - t_peak_idx\n",
    "        if dist_to_limit <= 5:\n",
    "            return t_peak_idx + 5\n",
    "        max_search = min(max_search, dist_to_limit - 3)\n",
    "    \n",
    "    max_idx = min(len(signal) - 1, t_peak_idx + max_search)\n",
    "    \n",
    "    if max_idx <= t_peak_idx + 8:\n",
    "        return t_peak_idx + 5\n",
    "    \n",
    "    # ===== STAGE 2: Setup Parameters =====\n",
    "    skip_samples = max(3, int(0.008 * sampling_rate))  # Skip 8ms from peak\n",
    "    \n",
    "    if max_idx <= t_peak_idx + skip_samples + 5:\n",
    "        return t_peak_idx + skip_samples\n",
    "    \n",
    "    segment = signal[t_peak_idx:max_idx]\n",
    "    \n",
    "    # Window size for variance calculation (30 samples ≈ 60ms at 500Hz)\n",
    "    window_size = min(15, int(0.060 * sampling_rate))                                                      \n",
    "    \n",
    "    # Detect T-wave polarity\n",
    "    if t_peak_value > 0:\n",
    "        target_descent_sign = -1\n",
    "    else:\n",
    "        target_descent_sign = 1\n",
    "    \n",
    "    # ===== STAGE 3: Calculate Derivatives (for inflection backup) =====\n",
    "    derivative_1st = np.gradient(segment)\n",
    "    \n",
    "    if len(derivative_1st) > 5:\n",
    "        kernel_size = 3\n",
    "        derivative_1st_smooth = np.convolve(derivative_1st, \n",
    "                                            np.ones(kernel_size)/kernel_size, \n",
    "                                            mode='same')\n",
    "    else:\n",
    "        derivative_1st_smooth = derivative_1st\n",
    "    \n",
    "    derivative_2nd = np.gradient(derivative_1st_smooth)\n",
    "    \n",
    "    # Adaptive thresholds for slope-based methods\n",
    "    slope_magnitudes = np.abs(derivative_1st_smooth[skip_samples:])\n",
    "    \n",
    "    if len(slope_magnitudes) > 0:\n",
    "        slope_75th = np.percentile(slope_magnitudes, 75)\n",
    "        slope_median = np.median(slope_magnitudes)\n",
    "        steep_threshold = max(slope_75th * 0.4, 0.002)\n",
    "        flat_threshold = max(slope_median * 0.15, 0.0008)\n",
    "    else:\n",
    "        steep_threshold = 0.003\n",
    "        flat_threshold = 0.001\n",
    "    \n",
    "    # ===== STAGE 4: NEW METHOD - Local Variance Stability Detection =====\n",
    "    stability_idx = None\n",
    "    \n",
    "    # Start search after skip_samples\n",
    "    search_start = skip_samples + int(0.010 * sampling_rate)  # At least 10ms from peak\n",
    "    search_end = len(segment) - window_size\n",
    "    \n",
    "    if search_start < search_end:\n",
    "        variance_ratios = []\n",
    "        candidate_points = []\n",
    "        \n",
    "        for i in range(search_start, search_end):\n",
    "            # Get windows before and after current point\n",
    "            before_window = segment[max(0, i-window_size):i]\n",
    "            after_window = segment[i:min(len(segment), i+window_size)]\n",
    "            \n",
    "            if len(before_window) < 10 or len(after_window) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Calculate variance (spread of values)\n",
    "            var_before = np.var(before_window)\n",
    "            var_after = np.var(after_window)\n",
    "            \n",
    "            # Also calculate standard deviation for robustness\n",
    "            std_before = np.std(before_window)\n",
    "            std_after = np.std(after_window)\n",
    "            \n",
    "            # Calculate ratio (how much does variance drop?)\n",
    "            if var_before > 0:\n",
    "                var_ratio = var_after / var_before\n",
    "                std_ratio = std_after / std_before\n",
    "                \n",
    "                # Store results\n",
    "                variance_ratios.append(var_ratio)\n",
    "                candidate_points.append(i)\n",
    "        \n",
    "        # Find where variance drops significantly (baseline is more stable)\n",
    "        if len(variance_ratios) > 0:\n",
    "            variance_ratios = np.array(variance_ratios)\n",
    "            candidate_points = np.array(candidate_points)\n",
    "            \n",
    "            # Threshold: variance after should be < 40% of variance before\n",
    "            # This means we've transitioned from T-wave to stable baseline\n",
    "            stability_mask = variance_ratios < 0.15                                                      \n",
    "            \n",
    "            if np.any(stability_mask):\n",
    "                # Take the FIRST point where stability is achieved\n",
    "                first_stable = candidate_points[stability_mask][0]\n",
    "                stability_idx = first_stable\n",
    "    \n",
    "    # ===== STAGE 5: METHOD 2 - Inflection Point Detection (Backup) =====\n",
    "    inflection_idx = None\n",
    "    in_steep_descent = False\n",
    "    \n",
    "    for i in range(skip_samples, len(derivative_1st_smooth) - 2):\n",
    "        current_slope = derivative_1st_smooth[i]\n",
    "        \n",
    "        if target_descent_sign * current_slope < -steep_threshold:\n",
    "            if not in_steep_descent:\n",
    "                in_steep_descent = True\n",
    "        \n",
    "        elif in_steep_descent:\n",
    "            next_slopes = derivative_1st_smooth[i:min(i+4, len(derivative_1st_smooth))]\n",
    "            \n",
    "            # Option A: Slope becomes flat\n",
    "            if np.all(np.abs(next_slopes) < flat_threshold * 1.5):\n",
    "                inflection_idx = i\n",
    "                break\n",
    "            \n",
    "            # Option B: Slope reverses direction\n",
    "            if target_descent_sign * current_slope > 0:\n",
    "                if i + 2 < len(derivative_1st_smooth):\n",
    "                    if target_descent_sign * derivative_1st_smooth[i+1] > 0:\n",
    "                        inflection_idx = i\n",
    "                        break\n",
    "            \n",
    "            # Option C: Slope magnitude drops below threshold\n",
    "            if np.abs(current_slope) < flat_threshold:\n",
    "                if i + 3 < len(derivative_1st_smooth):\n",
    "                    if np.mean(np.abs(derivative_1st_smooth[i:i+3])) < flat_threshold * 1.3:\n",
    "                        inflection_idx = i\n",
    "                        break\n",
    "                else:\n",
    "                    inflection_idx = i\n",
    "                    break\n",
    "    \n",
    "    # ===== STAGE 6: METHOD 3 - Second Derivative Zero-Crossing =====\n",
    "    curvature_inflection_idx = None\n",
    "    \n",
    "    if len(derivative_2nd) > skip_samples + 10:\n",
    "        for i in range(skip_samples + 5, len(derivative_2nd) - 1):\n",
    "            if derivative_2nd[i] * derivative_2nd[i+1] <= 0:\n",
    "                if i > skip_samples + int(0.015 * sampling_rate):\n",
    "                    if i + 3 < len(derivative_1st_smooth):\n",
    "                        avg_slope_after = np.mean(np.abs(derivative_1st_smooth[i:i+3]))\n",
    "                        if avg_slope_after < steep_threshold * 0.6:\n",
    "                            curvature_inflection_idx = i\n",
    "                            break\n",
    "    \n",
    "    # ===== STAGE 7: METHOD 4 - Minimum Slope Magnitude =====\n",
    "    min_slope_idx = None\n",
    "    \n",
    "    if stability_idx is None and inflection_idx is None and curvature_inflection_idx is None:\n",
    "        search_start_min = skip_samples + int(0.020 * sampling_rate)\n",
    "        search_end_min = min(len(derivative_1st_smooth), skip_samples + int(0.100 * sampling_rate))\n",
    "        \n",
    "        if search_start_min < search_end_min:\n",
    "            slope_window = np.abs(derivative_1st_smooth[search_start_min:search_end_min])\n",
    "            if len(slope_window) > 0:\n",
    "                local_min = np.argmin(slope_window)\n",
    "                min_slope_idx = search_start_min + local_min\n",
    "    \n",
    "    # ===== STAGE 8: Select Best Detection (Prioritized) =====\n",
    "    candidates = []\n",
    "    \n",
    "    # HIGHEST PRIORITY: Variance stability (your method!)\n",
    "    if stability_idx is not None:\n",
    "        candidates.append(('variance_stability', t_peak_idx + stability_idx, 120))\n",
    "    \n",
    "    # HIGH PRIORITY: Inflection point\n",
    "    if inflection_idx is not None:\n",
    "        candidates.append(('inflection', t_peak_idx + inflection_idx, 100))\n",
    "    \n",
    "    # MEDIUM PRIORITY: Curvature change\n",
    "    if curvature_inflection_idx is not None:\n",
    "        candidates.append(('curvature', t_peak_idx + curvature_inflection_idx, 80))\n",
    "    \n",
    "    # LOW PRIORITY: Minimum slope\n",
    "    if min_slope_idx is not None:\n",
    "        candidates.append(('min_slope', t_peak_idx + min_slope_idx, 60))\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        # Ultimate fallback\n",
    "        return min(t_peak_idx + int(0.040 * sampling_rate), max_idx)\n",
    "    \n",
    "    # Use the highest priority detection\n",
    "    best_method, best_offset, best_score = max(candidates, key=lambda x: x[2])\n",
    "    \n",
    "    # Debug info (optional - can be removed in production)\n",
    "    # print(f\"  T-offset method used: {best_method} (score: {best_score})\")\n",
    "    \n",
    "    # ===== STAGE 9: Validation =====\n",
    "    best_offset = max(best_offset, t_peak_idx + skip_samples)\n",
    "    best_offset = min(best_offset, max_idx)\n",
    "    \n",
    "    # Sanity check on duration\n",
    "    duration_ms = (best_offset - t_peak_idx) / sampling_rate * 1000\n",
    "    \n",
    "    if duration_ms < 15:\n",
    "        best_offset = t_peak_idx + int(0.030 * sampling_rate)\n",
    "    elif duration_ms > 300:\n",
    "        best_offset = t_peak_idx + int(0.100 * sampling_rate)\n",
    "    \n",
    "    return int(best_offset)\n",
    "\n",
    "def improved_delineate_ecg_waves(signal, r_peaks, sampling_rate):\n",
    "    \"\"\"\n",
    "    Complete ECG wave delineation with ultra-accurate T-offset detection\n",
    "    \"\"\"\n",
    "    waves = {\n",
    "        'p_peak': [], 'p_onset': [], 'p_offset': [],\n",
    "        'q_peak': [], 'q_onset': [],\n",
    "        's_peak': [], 's_offset': [],\n",
    "        't_peak': [], 't_onset': [], 't_offset': []\n",
    "    }\n",
    "    \n",
    "    signal_len = len(signal)\n",
    "    \n",
    "    # Detect P-peaks\n",
    "    p_peaks = enhanced_p_wave_detection(signal, r_peaks, sampling_rate, \"Segment\")\n",
    "    \n",
    "    # Helper for generic boundaries\n",
    "    def find_boundary_local(peak_idx, direction, max_search_samples, thresh_factor=0.05):\n",
    "        if np.isnan(peak_idx): return np.nan\n",
    "        peak_idx = int(peak_idx)\n",
    "        limit = peak_idx + (direction * max_search_samples)\n",
    "        limit = max(0, min(signal_len, limit))\n",
    "        if abs(limit - peak_idx) < 3: return peak_idx\n",
    "        start, end = sorted([peak_idx, limit])\n",
    "        segment = signal[start:end]\n",
    "        if direction == -1: segment = segment[::-1] \n",
    "        diff = np.diff(segment)\n",
    "        if len(diff) == 0: return peak_idx\n",
    "        max_slope = np.max(np.abs(diff))\n",
    "        thresh = max_slope * thresh_factor\n",
    "        for i in range(1, len(diff)):\n",
    "            if np.abs(diff[i]) < thresh:\n",
    "                return peak_idx + (direction * i)\n",
    "        return limit\n",
    "\n",
    "    last_t_offset = None\n",
    "\n",
    "    for i, r in enumerate(r_peaks):\n",
    "        r = int(r)\n",
    "        r_height = abs(signal[r]) if abs(signal[r]) > 0.05 else 1.0\n",
    "        \n",
    "        # --- P-WAVE ---\n",
    "        p_peak_val = p_peaks[i] if i < len(p_peaks) else np.nan\n",
    "        waves['p_peak'].append(p_peak_val)\n",
    "        \n",
    "        if not np.isnan(p_peak_val):\n",
    "            p_onset = find_p_onset_constrained(signal, p_peak_val, sampling_rate, last_t_offset)\n",
    "            waves['p_onset'].append(p_onset)\n",
    "            waves['p_offset'].append(find_boundary_local(int(p_peak_val), 1, int(0.08 * sampling_rate)))\n",
    "        else:\n",
    "            waves['p_onset'].append(np.nan)\n",
    "            waves['p_offset'].append(np.nan)\n",
    "\n",
    "        # --- Q-WAVE ---\n",
    "        win_q = int(0.05 * sampling_rate)\n",
    "        q_search_start = max(0, r - win_q)\n",
    "        q_window = signal[q_search_start:r]\n",
    "        q_idx = q_search_start + np.argmin(q_window) if len(q_window) > 0 else np.nan\n",
    "        waves['q_peak'].append(q_idx)\n",
    "        anchor_q = q_idx if not np.isnan(q_idx) else r\n",
    "        waves['q_onset'].append(find_boundary_local(anchor_q, -1, int(0.04 * sampling_rate)))\n",
    "        \n",
    "        # --- S-WAVE ---\n",
    "        win_s = int(0.06 * sampling_rate)\n",
    "        s_search_end = min(signal_len, r + win_s)\n",
    "        s_window = signal[r:s_search_end]\n",
    "        s_idx = r + np.argmin(s_window) if len(s_window) > 0 else np.nan\n",
    "        waves['s_peak'].append(s_idx)\n",
    "        anchor_s = s_idx if not np.isnan(s_idx) else r\n",
    "        waves['s_offset'].append(find_boundary_local(anchor_s, 1, int(0.04 * sampling_rate)))\n",
    "        \n",
    "        # --- T-WAVE ---\n",
    "        rr_next = (int(r_peaks[i+1]) - r) if i < len(r_peaks) - 1 else 1.0 * sampling_rate\n",
    "        dyn_t_start = int(0.10 * sampling_rate)\n",
    "        dyn_t_end = int(min(0.600 * sampling_rate, 0.65 * rr_next))\n",
    "        t_search_start = min(signal_len, r + dyn_t_start)\n",
    "        t_search_end = min(signal_len, r + dyn_t_end)\n",
    "        t_idx = np.nan\n",
    "        \n",
    "        if t_search_start < t_search_end:\n",
    "            t_window = signal[t_search_start:t_search_end]\n",
    "            if len(t_window) > 0:\n",
    "                local_peaks, _ = sp_signal.find_peaks(t_window, prominence=(0.05 * r_height))\n",
    "                if len(local_peaks) > 0:\n",
    "                    best_peak = local_peaks[np.argmax(t_window[local_peaks])]\n",
    "                    t_idx = t_search_start + best_peak\n",
    "                else:\n",
    "                    t_idx = t_search_start + np.argmax(t_window)\n",
    "        \n",
    "        waves['t_peak'].append(t_idx)\n",
    "        waves['t_onset'].append(find_boundary_local(t_idx, -1, int(0.08 * sampling_rate)))\n",
    "        \n",
    "        # --- T-OFFSET (ULTRA ACCURATE) ---\n",
    "        next_r = int(r_peaks[i+1]) if i < len(r_peaks) - 1 else None\n",
    "        next_p_onset_limit = None\n",
    "        if i + 1 < len(p_peaks):\n",
    "            next_p_peak = p_peaks[i+1]\n",
    "            if not np.isnan(next_p_peak):\n",
    "                next_p_onset_limit = find_boundary_local(int(next_p_peak), -1, int(0.08 * sampling_rate))\n",
    "        \n",
    "        t_offset = find_t_wave_offset_with_stability(\n",
    "            signal, \n",
    "            t_idx, \n",
    "            sampling_rate, \n",
    "            next_r_peak=next_r,\n",
    "            limit_idx=next_p_onset_limit\n",
    "        )\n",
    "\n",
    "        waves['t_offset'].append(t_offset)\n",
    "        \n",
    "        last_t_offset = t_offset\n",
    "\n",
    "    for k in waves:\n",
    "        waves[k] = np.array(waves[k])\n",
    "    \n",
    "    return waves\n",
    "\n",
    "\n",
    "def calculate_intervals(waves, sampling_rate):\n",
    "    \"\"\"Calculate PR, QRS, and QT intervals\"\"\"\n",
    "    pr_intervals = (waves['q_onset'] - waves['p_onset']) / sampling_rate * 1000\n",
    "    qrs_durations = (waves['s_offset'] - waves['q_onset']) / sampling_rate * 1000\n",
    "    qt_intervals = (waves['t_offset'] - waves['q_onset']) / sampling_rate * 1000\n",
    "   \n",
    "    pr_intervals = np.where((pr_intervals > 40) & (pr_intervals < 600), pr_intervals, np.nan)\n",
    "    qrs_durations = np.where((qrs_durations > 30) & (qrs_durations < 200), qrs_durations, np.nan)\n",
    "    qt_intervals = np.where((qt_intervals > 100) & (qt_intervals < 600), qt_intervals, np.nan)\n",
    "   \n",
    "    return pr_intervals, qrs_durations, qt_intervals\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_raw, ecg_filtered, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    \"\"\"Process ECG into segments for analysis\"\"\"\n",
    "    max_len = len(ecg_raw)\n",
    "    window_step = round((max_len - min_segment_length) / (num_segments - 1)) if num_segments > 1 else 0\n",
    "    results = []\n",
    "   \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = min(start_idx + min_segment_length, max_len)\n",
    "        if start_idx < 0: start_idx = 0\n",
    "        \n",
    "        segment_raw = ecg_raw[start_idx:end_idx]\n",
    "        segment_filtered = ecg_filtered[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment_raw) < 100: continue\n",
    "        \n",
    "        _, r_peaks, bpm, _ = qrs_detect(segment_raw, sampling_rate, len(segment_raw)/sampling_rate)\n",
    "        \n",
    "        waves = improved_delineate_ecg_waves(segment_filtered, r_peaks, sampling_rate)\n",
    "        pr, qrs, qt = calculate_intervals(waves, sampling_rate)\n",
    "        \n",
    "        def adj(arr):\n",
    "            if len(arr) == 0: return np.array([])\n",
    "            return arr + start_idx\n",
    "   \n",
    "        res = {\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_raw': segment_raw,\n",
    "            'ecg_filtered': segment_filtered,\n",
    "            'bpm': bpm,\n",
    "            'r_peaks': adj(r_peaks),\n",
    "            'avg_pr': np.nanmean(pr),\n",
    "            'avg_qrs': np.nanmean(qrs),\n",
    "            'avg_qt': np.nanmean(qt)\n",
    "        }\n",
    "        for k, v in waves.items():\n",
    "            res[k] = adj(v)\n",
    "            \n",
    "        results.append(res)\n",
    "   \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6. PLOTTING FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def plot_ecg_segments(ecg_raw, ecg_filtered, sampling_rate, results, title=\"ECG Analysis\"):\n",
    "    \"\"\"Plot ECG segments with detected waves and intervals\"\"\"\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(20, 4*num_segments))\n",
    "    if num_segments == 1: axes = [axes]\n",
    "   \n",
    "    time = np.arange(len(ecg_raw)) / sampling_rate\n",
    "   \n",
    "    def get_valid(indices):\n",
    "        if len(indices) == 0: return np.array([], dtype=int)\n",
    "        valid = indices[~np.isnan(indices)]\n",
    "        valid = valid[valid < len(ecg_raw)]\n",
    "        return valid.astype(int)\n",
    "\n",
    "    t_wave_issues = 0\n",
    "    \n",
    "    for i, (ax, res) in enumerate(zip(axes, results)):\n",
    "        seg_time = time[res['start_idx']:res['end_idx']]\n",
    "        \n",
    "        ax.plot(seg_time, res['ecg_filtered'], 'b-', alpha=0.7, linewidth=0.8, label='Filtered')\n",
    "        ax.plot(seg_time, res['ecg_raw'], 'k-', alpha=0.3, linewidth=0.5, label='Raw')\n",
    "        \n",
    "        peaks = [('r_peaks', 'ro', 'R'), ('p_peak', 'g^', 'P'), ('t_peak', 'bD', 'T')]\n",
    "        for key, style, lbl in peaks:\n",
    "            valid = get_valid(res[key])\n",
    "            if len(valid): \n",
    "                ax.plot(time[valid], ecg_filtered[valid], style, markersize=6, label=lbl)\n",
    "\n",
    "        p_onsets = res['p_onset']\n",
    "        q_onsets = res['q_onset']\n",
    "        t_offsets = res['t_offset']\n",
    "        t_peaks_arr = res['t_peak']\n",
    "        \n",
    "        for j in range(len(t_peaks_arr)):\n",
    "            if not np.isnan(t_peaks_arr[j]) and not np.isnan(t_offsets[j]):\n",
    "                if abs(t_peaks_arr[j] - t_offsets[j]) < 2:\n",
    "                    t_wave_issues += 1\n",
    "        \n",
    "        y_min = np.min(res['ecg_filtered'])\n",
    "        bar_y_pr = y_min - 0.05\n",
    "        bar_y_qt = y_min - 0.10\n",
    "        \n",
    "        count = 0\n",
    "        for j in range(len(p_onsets)):\n",
    "            if j < len(q_onsets) and not np.isnan(p_onsets[j]) and not np.isnan(q_onsets[j]):\n",
    "                pon = int(p_onsets[j])\n",
    "                qon = int(q_onsets[j])\n",
    "                if pon < qon:\n",
    "                    ax.hlines(y=bar_y_pr, xmin=time[pon], xmax=time[qon], colors='green', linewidth=4, alpha=0.7)\n",
    "                    if count == 0: ax.text(time[pon], bar_y_pr, 'PR', color='green', fontsize=8, ha='right', va='center')\n",
    "\n",
    "            if j < len(t_offsets) and not np.isnan(q_onsets[j]) and not np.isnan(t_offsets[j]):\n",
    "                qon = int(q_onsets[j])\n",
    "                toff = int(t_offsets[j])\n",
    "                if qon < toff:\n",
    "                    ax.hlines(y=bar_y_qt, xmin=time[qon], xmax=time[toff], colors='blue', linewidth=4, alpha=0.7)\n",
    "                    if count == 0: ax.text(time[toff], bar_y_qt, 'QT', color='blue', fontsize=8, ha='left', va='center')\n",
    "            count += 1\n",
    "\n",
    "        valid = get_valid(res['p_onset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'gx', markersize=8, label='P-start')\n",
    "        \n",
    "        valid = get_valid(res['q_onset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'm|', markersize=12, markeredgewidth=2, label='QRS-start')\n",
    "        \n",
    "        # ==========================================\n",
    "        # ADDED: QRS END PLOTTING (S-OFFSET)\n",
    "        # ==========================================\n",
    "        valid = get_valid(res['s_offset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'c|', markersize=12, markeredgewidth=2, label='QRS-end')\n",
    "        # ==========================================\n",
    "\n",
    "        valid = get_valid(res['t_offset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'b|', markersize=12, markeredgewidth=2, label='T-end')\n",
    "\n",
    "        info = f\"Seg {res['segment_num']} | BPM: {res['bpm']:.0f} | \"\n",
    "        info += f\"PR: {res['avg_pr']:.0f}ms | QRS: {res['avg_qrs']:.0f}ms | QT: {res['avg_qt']:.0f}ms\"\n",
    "        \n",
    "        ax.set_title(info, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlim([seg_time[0], seg_time[-1]])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if i == 0: ax.legend(loc='upper right', ncol=7, fontsize='small') # Increased ncol for new item\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_p_wave_quality(signal, r_peaks, p_peaks, sampling_rate, segment_num=\"\"):\n",
    "    \"\"\"Visualize P-wave detection quality\"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    \n",
    "    time = np.arange(len(signal)) / sampling_rate\n",
    "    axes[0].plot(time, signal, 'k-', alpha=0.6, linewidth=0.8)\n",
    "    \n",
    "    valid_r = r_peaks[~np.isnan(r_peaks)].astype(int)\n",
    "    axes[0].plot(time[valid_r], signal[valid_r], 'ro', markersize=6, label='R-peaks')\n",
    "    \n",
    "    valid_p = p_peaks[~np.isnan(p_peaks)].astype(int)\n",
    "    axes[0].plot(time[valid_p], signal[valid_p], 'g^', markersize=6, label='P-waves')\n",
    "    \n",
    "    missing_p = np.where(np.isnan(p_peaks[1:]))[0] + 1\n",
    "    if len(missing_p) > 0:\n",
    "        missing_r = r_peaks[missing_p].astype(int)\n",
    "        axes[0].plot(time[missing_r], signal[missing_r], 'rx', markersize=10, \n",
    "                    markeredgewidth=2, label=f'Missing P ({len(missing_p)})')\n",
    "    \n",
    "    axes[0].set_title(f'P-wave Detection - Segment {segment_num}')\n",
    "    axes[0].set_ylabel('Amplitude (mV)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    pr_intervals = []\n",
    "    beat_numbers = []\n",
    "    beat_idx = 0\n",
    "    for i, (r, p) in enumerate(zip(r_peaks, p_peaks)):\n",
    "        if not np.isnan(p) and not np.isnan(r):\n",
    "            pr = (r - p) / sampling_rate * 1000\n",
    "            pr_intervals.append(pr)\n",
    "            beat_numbers.append(beat_idx)\n",
    "        beat_idx += 1\n",
    "    \n",
    "    if pr_intervals:\n",
    "        axes[1].plot(beat_numbers, pr_intervals, 'bo-', markersize=4)\n",
    "        axes[1].axhline(y=120, color='g', linestyle='--', alpha=0.5, label='Normal PR min (120ms)')\n",
    "        axes[1].axhline(y=200, color='orange', linestyle='--', alpha=0.5, label='1st° AVB threshold (200ms)')\n",
    "        axes[1].axhline(y=np.mean(pr_intervals), color='b', linestyle='-', alpha=0.7, \n",
    "                               label=f'Mean: {np.mean(pr_intervals):.1f}ms')\n",
    "        axes[1].set_ylabel('PR Interval (ms)')\n",
    "        axes[1].set_xlabel('Beat Number')\n",
    "        axes[1].set_title(f'PR Intervals (mean: {np.mean(pr_intervals):.1f}ms ± {np.std(pr_intervals):.1f}ms)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend()\n",
    "    \n",
    "    p_amplitudes = []\n",
    "    beat_numbers_amp = []\n",
    "    beat_idx = 0\n",
    "    for p in valid_p:\n",
    "        p_amplitudes.append(abs(signal[p]))\n",
    "        beat_numbers_amp.append(beat_idx)\n",
    "        beat_idx += 1\n",
    "    \n",
    "    if p_amplitudes:\n",
    "        axes[2].plot(beat_numbers_amp, p_amplitudes, 'go-', markersize=4)\n",
    "        axes[2].axhline(y=0.1, color='g', linestyle='--', alpha=0.5, label='Typical P (0.1mV)')\n",
    "        axes[2].axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='Low amplitude threshold')\n",
    "        axes[2].axhline(y=np.mean(p_amplitudes), color='g', linestyle='-', alpha=0.7,\n",
    "                               label=f'Mean: {np.mean(p_amplitudes):.3f}mV')\n",
    "        axes[2].set_ylabel('Amplitude (mV)')\n",
    "        axes[2].set_xlabel('Beat Number')\n",
    "        axes[2].set_title(f'P-wave Amplitudes (mean: {np.mean(p_amplitudes):.3f}mV ± {np.std(p_amplitudes):.3f}mV)')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend()\n",
    "    \n",
    "    window_size = int(2 * sampling_rate)\n",
    "    num_windows = len(signal) // window_size\n",
    "    quality_over_time = []\n",
    "    time_points = []\n",
    "    \n",
    "    for w in range(num_windows):\n",
    "        start = w * window_size\n",
    "        end = min((w + 1) * window_size, len(signal))\n",
    "        window_signal = signal[start:end]\n",
    "        quality = calculate_signal_quality(window_signal)\n",
    "        quality_over_time.append(quality)\n",
    "        time_points.append((start + end) / 2 / sampling_rate)\n",
    "    \n",
    "    if quality_over_time:\n",
    "        axes[3].plot(time_points, quality_over_time, 'r-', linewidth=2, label='Signal Quality')\n",
    "        axes[3].axhline(y=0.7, color='g', linestyle='--', alpha=0.5, label='Good (>0.7)')\n",
    "        axes[3].axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate (>0.5)')\n",
    "        axes[3].axhline(y=0.3, color='r', linestyle='--', alpha=0.5, label='Poor (<0.3)')\n",
    "        axes[3].fill_between(time_points, 0, quality_over_time, alpha=0.3, color='red')\n",
    "        axes[3].set_ylabel('Quality Index')\n",
    "        axes[3].set_xlabel('Time (s)')\n",
    "        axes[3].set_title('Signal Quality Over Time')\n",
    "        axes[3].set_ylim([0, 1])\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        axes[3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('/mnt/user-data/outputs/p_wave_quality.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    total_beats = len(r_peaks) - 1\n",
    "    detected_p = np.sum(~np.isnan(p_peaks[1:]))\n",
    "    detection_rate = detected_p / total_beats * 100 if total_beats > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- P-Wave Detection Summary (Segment {segment_num}) ---\")\n",
    "    print(f\"Total R-peaks: {len(r_peaks)}\")\n",
    "    print(f\"P-waves detected: {detected_p}/{total_beats} ({detection_rate:.1f}%)\")\n",
    "    print(f\"Mean signal quality: {np.mean(quality_over_time):.2f}\")\n",
    "    if pr_intervals:\n",
    "        print(f\"PR interval: {np.mean(pr_intervals):.1f} ± {np.std(pr_intervals):.1f} ms\")\n",
    "    if p_amplitudes:\n",
    "        print(f\"P amplitude: {np.mean(p_amplitudes):.3f} ± {np.std(p_amplitudes):.3f} mV\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def plot_full_ecg(ecg_raw, ecg_filtered, sampling_rate, waves, r_peaks, bpm, title=\"Full ECG Analysis\"):\n",
    "    # --- 1. Setup Figure (Dynamic Width) ---\n",
    "    duration_sec = len(ecg_raw) / sampling_rate\n",
    "    fig_width = max(15, min(100, int(duration_sec * 0.8))) \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(fig_width, 6))\n",
    "    time = np.arange(len(ecg_raw)) / sampling_rate\n",
    "\n",
    "    # --- 2. Plot Signals ---\n",
    "    ax.plot(time, ecg_filtered, 'b-', alpha=0.7, linewidth=0.8, label='Filtered')\n",
    "    ax.plot(time, ecg_raw, 'k-', alpha=0.3, linewidth=0.5, label='Raw')\n",
    "\n",
    "    # Helper to clean indices\n",
    "    def get_valid(indices):\n",
    "        if len(indices) == 0: return np.array([], dtype=int)\n",
    "        valid = indices[~np.isnan(indices)]\n",
    "        valid = valid[(valid >= 0) & (valid < len(ecg_raw))]\n",
    "        return valid.astype(int)\n",
    "\n",
    "    # --- 3. Plot Peaks (R, P, T) ---\n",
    "    valid_r = get_valid(r_peaks)\n",
    "    if len(valid_r):\n",
    "        ax.plot(time[valid_r], ecg_filtered[valid_r], 'ro', markersize=6, label='R_peak')\n",
    "\n",
    "    if 'p_peak' in waves:\n",
    "        valid_p = get_valid(waves['p_peak'])\n",
    "        if len(valid_p):\n",
    "            ax.plot(time[valid_p], ecg_filtered[valid_p], 'g^', markersize=6, label='P_peak')\n",
    "\n",
    "    if 't_peak' in waves:\n",
    "        valid_t = get_valid(waves['t_peak'])\n",
    "        if len(valid_t):\n",
    "            ax.plot(time[valid_t], ecg_filtered[valid_t], 'bD', markersize=6, label='T_peak')\n",
    "\n",
    "    # --- 4. Plot Interval Bars (PR & QT) ---\n",
    "    y_min = np.min(ecg_filtered)\n",
    "    bar_y_pr = y_min - (np.ptp(ecg_filtered) * 0.05)\n",
    "    bar_y_qt = y_min - (np.ptp(ecg_filtered) * 0.10)\n",
    "\n",
    "    p_onsets = waves.get('p_onset', [])\n",
    "    q_onsets = waves.get('q_onset', [])\n",
    "    t_offsets = waves.get('t_offset', [])\n",
    "\n",
    "    count_pr = 0\n",
    "    count_qt = 0\n",
    "    \n",
    "    num_beats = len(r_peaks)\n",
    "    \n",
    "    for j in range(num_beats):\n",
    "        if j < len(p_onsets) and j < len(q_onsets):\n",
    "            pon = p_onsets[j]\n",
    "            qon = q_onsets[j]\n",
    "            \n",
    "            if not np.isnan(pon) and not np.isnan(qon):\n",
    "                pon, qon = int(pon), int(qon)\n",
    "                if pon < qon and qon < len(time):\n",
    "                    ax.hlines(y=bar_y_pr, xmin=time[pon], xmax=time[qon], \n",
    "                             colors='green', linewidth=4, alpha=0.7)\n",
    "                    if count_pr == 0: \n",
    "                        ax.text(time[pon], bar_y_pr, 'PR', color='green', \n",
    "                               fontsize=8, ha='right', va='center', fontweight='bold')\n",
    "                    count_pr += 1\n",
    "\n",
    "        if j < len(q_onsets) and j < len(t_offsets):\n",
    "            qon = q_onsets[j]\n",
    "            toff = t_offsets[j]\n",
    "            \n",
    "            if not np.isnan(qon) and not np.isnan(toff):\n",
    "                qon, toff = int(qon), int(toff)\n",
    "                if qon < toff and toff < len(time):\n",
    "                    ax.hlines(y=bar_y_qt, xmin=time[qon], xmax=time[toff], \n",
    "                             colors='blue', linewidth=4, alpha=0.7)\n",
    "                    if count_qt == 0: \n",
    "                        ax.text(time[toff], bar_y_qt, 'QT', color='blue', \n",
    "                               fontsize=8, ha='left', va='center', fontweight='bold')\n",
    "                    count_qt += 1\n",
    "\n",
    "    # --- 5. Plot Specific Markers (Onsets/Offsets) ---\n",
    "    if 'p_onset' in waves:\n",
    "        valid_pon = get_valid(waves['p_onset'])\n",
    "        if len(valid_pon):\n",
    "            ax.plot(time[valid_pon], ecg_filtered[valid_pon], 'gx', markersize=8, label='P-start')\n",
    "\n",
    "    if 'q_onset' in waves:\n",
    "        valid_qon = get_valid(waves['q_onset'])\n",
    "        if len(valid_qon):\n",
    "            ax.plot(time[valid_qon], ecg_filtered[valid_qon], 'm|', markersize=12, markeredgewidth=2, label='QRS-start')\n",
    "\n",
    "    # ==========================================\n",
    "    # ADDED: QRS END PLOTTING (S-OFFSET)\n",
    "    # ==========================================\n",
    "    if 's_offset' in waves:\n",
    "        valid_soff = get_valid(waves['s_offset'])\n",
    "        if len(valid_soff):\n",
    "            ax.plot(time[valid_soff], ecg_filtered[valid_soff], 'c|', markersize=12, markeredgewidth=2, label='QRS-end')\n",
    "    # ==========================================\n",
    "\n",
    "    if 't_offset' in waves:\n",
    "        valid_toff = get_valid(waves['t_offset'])\n",
    "        if len(valid_toff):\n",
    "            ax.plot(time[valid_toff], ecg_filtered[valid_toff], 'b|', markersize=12, markeredgewidth=2, label='T-end')\n",
    "\n",
    "    # --- 6. Add Statistics Box ---\n",
    "    pr, qrs, qt = calculate_intervals(waves, sampling_rate)\n",
    "    stats_text = (\n",
    "        f\"HEART RATE: {bpm:.0f} BPM\\n\"\n",
    "        f\"Avg PR:  {np.nanmean(pr):.0f} ms\\n\"\n",
    "        f\"Avg QRS: {np.nanmean(qrs):.0f} ms\\n\"\n",
    "        f\"Avg QT:  {np.nanmean(qt):.0f} ms\"\n",
    "    )\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    ax.text(0.005, 0.95, stats_text, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', bbox=props, fontfamily='monospace')\n",
    "\n",
    "    # --- 7. Formatting ---\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.set_xlim([0, time[-1]])\n",
    "    ax.grid(True, which='both', alpha=0.3)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', ncol=5, fontsize='small')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ==========================================\n",
    "# 7. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Filter setup\n",
    "    freq = 500\n",
    "    low_pass_cutoff = 40\n",
    "    low_pass_order = 7\n",
    "    b_lp, a_lp = sp_signal.butter(low_pass_order, low_pass_cutoff / (freq / 2), btype=\"low\")\n",
    "    b_notch, a_notch = sp_signal.iirnotch(50, 50 / 20, freq)  \n",
    "\n",
    "    def low_pass_filter(data):\n",
    "        try:\n",
    "            return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "    def notch_filter(data):\n",
    "        try:\n",
    "            return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "    # # input_json = r\"simulator\\contec\\30bpm_1756099931979.json\"   \n",
    "    # input_json = r\"simulator\\contec\\40bpm_1756099996286.json\"   \n",
    "    # # input_json = r\"simulator\\contec\\60bpm_1756100055600.json\"   \n",
    "    # # input_json = r\"simulator\\contec\\80bpm_1756100133843.json\"   \n",
    "    # # input_json = r\"simulator\\contec\\100bpm_1756100188299.json\"   \n",
    "    # input_json = r\"simulator\\contec\\120bpm_1756100243822.json\"     \n",
    "    # # input_json = r\"simulator\\contec\\140bpm_1756100303625.json\"   \n",
    "    # # input_json = r\"simulator\\contec\\180bpm_1756100430492.json\"   \n",
    "    # # input_json = r\"simulator\\contec\\200bpm_1756100489086.json\"   \n",
    "    # # input_json = r\"simulator\\contec\\220bpm_1756100542987.json\"   \n",
    "\n",
    "    # input_json = r\"intervals\\Antor_1769669515402.json\"   \n",
    "    # input_json = r\"intervals\\Asif_1769673034525.json\"   \n",
    "    # input_json = r\"intervals\\Atiur_1769684477237.json\"     #### 89\n",
    "    input_json = r\"intervals\\Aupo_1769678570784.json\"     #### 98\n",
    "    # input_json = r\"intervals\\Faizur_1769672384762.json\"    \n",
    "    # input_json = r\"intervals\\Alauddin_1769763819330.json\"   \n",
    "\n",
    "    with open(input_json, 'r') as file:\n",
    "        file_data = json.load(file)\n",
    "\n",
    "\n",
    "    # # input_json = r\"exception\\L2_1759207950416.json\"   \n",
    "    # input_json = r\"issues\\L2_1757064122874.json\"  \n",
    "    # # input_json = r\"issues\\L2_1757579288752.json\"\n",
    "    # # input_json = r\"issues\\L2_1757737806463.json\"  \n",
    "    # # input_json = r\"v01_prob\\L2_1765984517025.json\"  \n",
    "    # # input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "    # # input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "    # # input_json = r\"issues2\\1757998097068\\L2_1757998097068.json\"\n",
    "    # # input_json = r\"issues2\\1758943573744\\L2_1758943573744.json\"\n",
    "    # # input_json = r\"issues2\\1759059066184\\L2_1759059066184.json\"     \n",
    "    # # input_json = r\"issues2\\1759117739887\\L2_1759117739887.json\"\n",
    "    # # input_json = r\"issues2\\1759118709079\\L2_1759118709079.json\"\n",
    "    # # input_json = r\"issues2\\1759202739736\\L2_1759202739736.json\"  \n",
    "    # # input_json = r\"issues2\\1759639059357\\L2_1759639059357.json\"  ####\n",
    "\n",
    "    # doubles = []\n",
    "    # with open(input_json, \"rb\") as f:\n",
    "    #     while chunk := f.read(8):\n",
    "    #         if len(chunk) < 8:\n",
    "    #             break\n",
    "    #         value = struct.unpack(\"<d\", chunk)[0]\n",
    "    #         doubles.append(value)\n",
    "\n",
    "    # file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "    # # def decrypt(input_file):\n",
    "    # #     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "    # #     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    # #     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    # #     with open(input_file, 'rb') as f:\n",
    "    # #         encrypted_data = f.read()\n",
    "    # #     enc = base64.b64decode(encrypted_data[24:])\n",
    "    # #     data = unpad(cipher.decrypt(enc), 16)\n",
    "    # #     decoded_string = data.decode('utf-8')\n",
    "    # #     return json.loads(decoded_string)\n",
    "\n",
    "    # input_json = r\"NHF2\\DATA_1750689015865.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750689460556.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750851207409.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750858856842.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750862721789.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "    # file_data = decrypt(input_json)\n",
    "\n",
    "    # # input_json = r\"NHF\\DATA_1752067426678.json\"  \n",
    "    # # input_json = r\"NHF\\DATA_1752121970835.json\"  \n",
    "    # # input_json = r\"NHF\\DATA_1754709586876.json\"  \n",
    "    # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "    # file_data = decrypt(input_json)  \n",
    "        \n",
    "    try:\n",
    "        # Process raw data for R-peak detection\n",
    "        print(\"Processing raw data for R-peak detection...\")\n",
    "        raw_data = data_process(file_data)\n",
    "        ecg_raw = raw_data[0, :15000, 0]\n",
    "        \n",
    "        # Process filtered data for P, Q, S, T wave detection\n",
    "        print(\"Processing filtered data for wave delineation...\")\n",
    "        filtered_data = data_process(low_pass_filter(notch_filter(baseline_wander(np.array(file_data[\"dataL2\"])))))\n",
    "        ecg_filtered = filtered_data[0, :15000, 0]\n",
    "        \n",
    "        sampling_rate = 500\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ULTRA-ACCURATE ECG ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"✓ Using raw data for R-peak detection\")\n",
    "        print(\"✓ Using filtered data for P, Q, S, T wave delineation\")\n",
    "        print(\"✓ Enhanced T-wave offset detection (first steep descent)\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        segment_results = process_ecg_segments(\n",
    "            ecg_raw=ecg_raw,\n",
    "            ecg_filtered=ecg_filtered,\n",
    "            sampling_rate=sampling_rate,\n",
    "            num_segments=4,\n",
    "            min_segment_length=5000\n",
    "        )\n",
    "\n",
    "        # Plot results\n",
    "        plot_ecg_segments(\n",
    "            ecg_raw,\n",
    "            ecg_filtered,\n",
    "            sampling_rate,\n",
    "            segment_results,\n",
    "            \"Ultra-Accurate Clinical Interval Analysis\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ULTRA-ACCURATE FULL ECG ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # --- 2. PEAK DETECTION ---\n",
    "        print(\"Detecting R-peaks on signal...\")\n",
    "        _, r_peaks, bpm, _ = qrs_detect(ecg_raw, sampling_rate)\n",
    "        print(f\"✓ Detected {len(r_peaks)} R-peaks. BPM: {bpm:.1f}\")\n",
    "\n",
    "        # --- 3. WAVE DELINEATION ---\n",
    "        print(\"Delineating waves (P, Q, S, T)...\")\n",
    "        waves = improved_delineate_ecg_waves(ecg_filtered, r_peaks, sampling_rate)\n",
    "        \n",
    "        # --- 4. CALCULATE STATISTICS ---\n",
    "        pr, qrs, qt = calculate_intervals(waves, sampling_rate)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"FULL SIGNAL STATISTICS\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"Avg PR Interval:  {np.nanmean(pr):.1f} ms\")\n",
    "        print(f\"Avg QRS Duration: {np.nanmean(qrs):.1f} ms\")\n",
    "        print(f\"Avg QT Interval:  {np.nanmean(qt):.1f} ms\")\n",
    "        print(\"=\"*40 + \"\\n\")\n",
    "        \n",
    "        # --- 5. PLOT ---\n",
    "        print(\"Generating full plot with statistics...\")\n",
    "        plot_full_ecg(\n",
    "            ecg_raw, \n",
    "            ecg_filtered, \n",
    "            sampling_rate, \n",
    "            waves, \n",
    "            r_peaks, \n",
    "            bpm\n",
    "        )\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found ({input_json}). Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d6acf",
   "metadata": {},
   "source": [
    "### qrs corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# ==========================================\n",
    "# 1. UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def baseline_wander(data, sampling_rate=500, knot_spacing=0.4):\n",
    "    \"\"\"\n",
    "    Cubic spline interpolation - fits smooth curve through evenly spaced points.\n",
    "    knot_spacing: distance between knots in seconds\n",
    "    \"\"\"\n",
    "    n_samples = len(data)\n",
    "    knot_interval = int(sampling_rate * knot_spacing)\n",
    "    \n",
    "    # Create knot points at regular intervals\n",
    "    knot_indices = np.arange(0, n_samples, knot_interval)\n",
    "    if knot_indices[-1] != n_samples - 1:\n",
    "        knot_indices = np.append(knot_indices, n_samples - 1)\n",
    "    \n",
    "    # Use percentile at each knot region to estimate baseline (robust to QRS)\n",
    "    knot_values = []\n",
    "    half_window = knot_interval // 2\n",
    "    for idx in knot_indices:\n",
    "        start = max(0, idx - half_window)\n",
    "        end = min(n_samples, idx + half_window)\n",
    "        knot_values.append(np.percentile(data[start:end], 50))\n",
    "    \n",
    "    # Fit cubic spline and subtract\n",
    "    spline = CubicSpline(knot_indices, knot_values)\n",
    "    baseline = spline(np.arange(n_samples))\n",
    "    \n",
    "    return data - baseline\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    return normalize(signal_data, min_val, max_val)\n",
    "\n",
    "def data_process(input_data):\n",
    "    \"\"\"\n",
    "    Robust data processing that handles both dictionary inputs and direct array inputs.\n",
    "    Fixed the FutureWarning issue.\n",
    "    \"\"\"\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "   \n",
    "    # Check if input is a dictionary and has the key\n",
    "    if isinstance(input_data, dict) and 'dataL2' in input_data:\n",
    "        raw_data = input_data['dataL2']\n",
    "    else:\n",
    "        # Assume it's already the data array\n",
    "        raw_data = input_data\n",
    "        \n",
    "    sig = np.array(raw_data)\n",
    "    datas.append(sig.astype('float32'))\n",
    "   \n",
    "    datas_array = np.array(datas)\n",
    "    min_val = np.min(datas_array)\n",
    "    max_val = np.max(datas_array)\n",
    "   \n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(process_signal(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated,\n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None):\n",
    "    \"\"\"Enhanced QRS detection with Amplitude Guardrails for AV Blocks\"\"\"\n",
    "    data_clean = data\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "        \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    if len(cleaned_r) >= 2:\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        if median_rr < 1.5:\n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                    \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    low_thresh = mean_val * 0.6\n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                        \n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "        \n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    min_expected_peaks = int(30/60 * segment_duration)\n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)]\n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "# ==========================================\n",
    "# 2. P-WAVE DETECTION\n",
    "# ==========================================\n",
    "\n",
    "def adaptive_noise_filter(segment, sampling_rate):\n",
    "    \"\"\"Apply stronger filtering in noisy regions\"\"\"\n",
    "    diff = np.diff(segment)\n",
    "    noise_std = np.std(diff)\n",
    "    \n",
    "    if noise_std > 0.05:\n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        low = 1 / nyquist\n",
    "        high = 25 / nyquist\n",
    "        b, a = sp_signal.butter(3, [low, high], btype='band')\n",
    "        return sp_signal.filtfilt(b, a, segment)\n",
    "    \n",
    "    return segment\n",
    "\n",
    "def calculate_signal_quality(segment):\n",
    "    \"\"\"Calculate signal quality index (0-1, higher is better)\"\"\"\n",
    "    diff = np.diff(segment)\n",
    "    noise_level = np.std(diff)\n",
    "    noise_score = np.exp(-noise_level / 0.1)\n",
    "    \n",
    "    baseline_drift = np.std(segment)\n",
    "    drift_score = np.exp(-baseline_drift / 0.3)\n",
    "    \n",
    "    signal_range = np.max(segment) - np.min(segment)\n",
    "    if 0.2 <= signal_range <= 1.5:\n",
    "        amplitude_score = 1.0\n",
    "    else:\n",
    "        amplitude_score = 0.5\n",
    "    \n",
    "    quality = (noise_score * 0.4 + drift_score * 0.4 + amplitude_score * 0.2)\n",
    "    \n",
    "    return quality\n",
    "\n",
    "def enhanced_p_wave_detection(signal, r_peaks, sampling_rate, segment_num):\n",
    "    \"\"\"Enhanced P-wave detection with Bradycardia fix\"\"\"\n",
    "    if len(r_peaks) < 3:\n",
    "        return np.full(len(r_peaks), np.nan)\n",
    "    \n",
    "    signal_quality = calculate_signal_quality(signal)\n",
    "    \n",
    "    p_peaks = np.full(len(r_peaks), np.nan)\n",
    "    p_qualities = np.zeros(len(r_peaks))\n",
    "    \n",
    "    if len(r_peaks) > 1:\n",
    "        rr_intervals = np.diff(r_peaks) / sampling_rate\n",
    "        avg_rr = np.mean(rr_intervals)\n",
    "        avg_hr = 60 / avg_rr if avg_rr > 0 else 0\n",
    "    else:\n",
    "        avg_hr = 0\n",
    "    \n",
    "    print(f\"Segment {segment_num}: Signal Quality = {signal_quality:.2f}, Avg HR = {avg_hr:.0f} BPM\")\n",
    "    \n",
    "    if avg_hr > 180:\n",
    "        # print(f\"  High heart rate detected - using adaptive short-cycle parameters\")\n",
    "        use_adaptive_short_cycle = True\n",
    "        min_quality_threshold = 20  \n",
    "    elif avg_hr > 120:\n",
    "        use_adaptive_short_cycle = True\n",
    "        min_quality_threshold = 30\n",
    "    else:\n",
    "        use_adaptive_short_cycle = False\n",
    "        if signal_quality > 0.7:\n",
    "            min_quality_threshold = 50\n",
    "        elif signal_quality > 0.5:\n",
    "            min_quality_threshold = 35\n",
    "        else:\n",
    "            min_quality_threshold = 25\n",
    "    \n",
    "    preliminary_pr_intervals = []\n",
    "    preliminary_p_amps = []\n",
    "    \n",
    "    t_wave_ends = []\n",
    "    for i in range(len(r_peaks) - 1):\n",
    "        r_curr = int(r_peaks[i])\n",
    "        r_next = int(r_peaks[i + 1])\n",
    "        rr_interval = r_next - r_curr\n",
    "        \n",
    "        if rr_interval < 0.4 * sampling_rate:  \n",
    "            estimated_t_end = r_curr + int(0.45 * rr_interval)\n",
    "        elif rr_interval < 0.6 * sampling_rate: \n",
    "            estimated_t_end = r_curr + int(0.55 * rr_interval)\n",
    "        else:  \n",
    "            estimated_t_end = r_curr + min(int(0.5 * sampling_rate), int(0.65 * rr_interval))\n",
    "        \n",
    "        t_wave_ends.append(estimated_t_end)\n",
    "    \n",
    "    if len(r_peaks) > 0:\n",
    "        last_r = int(r_peaks[-1])\n",
    "        t_wave_ends.append(last_r + int(0.5 * sampling_rate))\n",
    "    \n",
    "    for i, r in enumerate(r_peaks):\n",
    "        if i == 0: continue\n",
    "            \n",
    "        r = int(r)\n",
    "        rr_prev = r - int(r_peaks[i-1])\n",
    "        \n",
    "        if use_adaptive_short_cycle and rr_prev < 0.5 * sampling_rate:  \n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                t_end_prev = t_wave_ends[i - 1]\n",
    "                search_start = t_end_prev + int(0.02 * sampling_rate)\n",
    "            else:\n",
    "                search_start = int(r_peaks[i-1] + 0.25 * rr_prev)\n",
    "            \n",
    "            search_end = int(r - 0.02 * sampling_rate)\n",
    "            min_pr_ms = 80\n",
    "            max_pr_ms = 300\n",
    "            \n",
    "        else:\n",
    "            max_lookback_samples = int(0.40 * sampling_rate)\n",
    "            earliest_allowed_start = r - max_lookback_samples\n",
    "            \n",
    "            if i - 1 < len(t_wave_ends):\n",
    "                t_end_prev = t_wave_ends[i - 1]\n",
    "                search_start = max(t_end_prev + int(0.05 * sampling_rate), earliest_allowed_start)\n",
    "            else:\n",
    "                search_start = max(int(r_peaks[i-1] + 0.4 * sampling_rate), earliest_allowed_start)\n",
    "            \n",
    "            search_end = int(r - 0.03 * sampling_rate)\n",
    "            min_pr_ms = 80\n",
    "            max_pr_ms = 400\n",
    "        \n",
    "        search_start = max(0, search_start)\n",
    "        search_end = min(len(signal)-1, search_end)\n",
    "        \n",
    "        min_window_size = int(0.05 * sampling_rate)\n",
    "        if search_end - search_start < min_window_size:\n",
    "            continue\n",
    "        \n",
    "        segment = signal[search_start:search_end]\n",
    "        segment_filtered = adaptive_noise_filter(segment, sampling_rate)\n",
    "        \n",
    "        if use_adaptive_short_cycle:\n",
    "            min_prominence = 0.002\n",
    "            min_distance = int(0.05 * sampling_rate)\n",
    "            max_width = int(0.12 * sampling_rate)\n",
    "        else:\n",
    "            min_prominence = 0.003\n",
    "            min_distance = int(0.08 * sampling_rate)\n",
    "            max_width = int(0.15 * sampling_rate)\n",
    "        \n",
    "        try:\n",
    "            candidate_peaks, properties = sp_signal.find_peaks(\n",
    "                segment_filtered,\n",
    "                distance=min_distance,\n",
    "                prominence=min_prominence,\n",
    "                width=(int(0.02*sampling_rate), max_width)\n",
    "            )\n",
    "        except:\n",
    "            candidate_peaks = []\n",
    "        \n",
    "        if len(candidate_peaks) == 0:\n",
    "            continue\n",
    "        \n",
    "        candidate_peaks = search_start + candidate_peaks\n",
    "        \n",
    "        best_score = -np.inf\n",
    "        best_peak = None\n",
    "        \n",
    "        for cp in candidate_peaks:\n",
    "            cp = int(cp)\n",
    "            \n",
    "            pr_interval = (r - cp) / sampling_rate * 1000\n",
    "            if pr_interval < min_pr_ms or pr_interval > max_pr_ms:\n",
    "                continue\n",
    "            \n",
    "            score = 0\n",
    "            \n",
    "            if use_adaptive_short_cycle:\n",
    "                ideal_pr = 120\n",
    "                sigma = 40\n",
    "            else:\n",
    "                ideal_pr = 160\n",
    "                sigma = 60\n",
    "            \n",
    "            score += np.exp(-((pr_interval - ideal_pr) ** 2) / (2 * sigma ** 2)) * 150\n",
    "            \n",
    "            p_amp = abs(signal[cp])\n",
    "            if p_amp > 0.5:\n",
    "                score += 50 \n",
    "            elif 0.015 <= p_amp <= 0.5:\n",
    "                ideal_amp = 0.08\n",
    "                score += np.exp(-((p_amp - ideal_amp) ** 2) / (2 * 0.10 ** 2)) * 400\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                left_slope = signal[cp] - signal[cp - 5]\n",
    "                right_slope = signal[cp + 5] - signal[cp]\n",
    "                symmetry = 1 - abs(left_slope - right_slope) / (abs(left_slope) + abs(right_slope) + 1e-6)\n",
    "                score += symmetry * 60\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            score += 50 \n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_peak = cp\n",
    "        \n",
    "        if best_peak is not None and best_score > min_quality_threshold:\n",
    "             p_peaks[i] = best_peak\n",
    "             p_qualities[i] = best_score\n",
    "             preliminary_pr_intervals.append((r - best_peak) / sampling_rate * 1000)\n",
    "             preliminary_p_amps.append(abs(signal[best_peak]))\n",
    "\n",
    "    if len(preliminary_pr_intervals) >= 3:\n",
    "        median_pr = np.median(preliminary_pr_intervals)\n",
    "        \n",
    "        for i in range(1, len(r_peaks)):\n",
    "            if np.isnan(p_peaks[i]): continue\n",
    "            \n",
    "            pr = (r_peaks[i] - p_peaks[i]) / sampling_rate * 1000\n",
    "            \n",
    "            tolerance = 100 if use_adaptive_short_cycle else 120\n",
    "            if abs(pr - median_pr) > tolerance:\n",
    "                p_peaks[i] = np.nan\n",
    "                p_qualities[i] = 0\n",
    "\n",
    "    return p_peaks\n",
    "\n",
    "# ==========================================\n",
    "# 4. P-WAVE ONSET DETECTION\n",
    "# ==========================================\n",
    "\n",
    "def find_p_onset_constrained(signal, p_peak_idx, sampling_rate, prev_t_offset=None):\n",
    "    \"\"\"\n",
    "    Finds P-onset with strict constraints\n",
    "    \"\"\"\n",
    "    if np.isnan(p_peak_idx): return np.nan\n",
    "    p_idx = int(p_peak_idx)\n",
    "    \n",
    "    min_dist_samples = int(0.012 * sampling_rate)\n",
    "    start_search = p_idx - min_dist_samples\n",
    "    \n",
    "    max_lookback = int(0.12 * sampling_rate)\n",
    "    default_limit = p_idx - max_lookback\n",
    "    \n",
    "    if prev_t_offset is not None and not np.isnan(prev_t_offset):\n",
    "        t_end_buffer = int(prev_t_offset) + int(0.02 * sampling_rate)\n",
    "        limit_idx = max(default_limit, t_end_buffer)\n",
    "    else:\n",
    "        limit_idx = default_limit\n",
    "\n",
    "    if limit_idx >= start_search:\n",
    "        return start_search\n",
    "        \n",
    "    limit_idx = max(0, limit_idx)\n",
    "    \n",
    "    segment = signal[limit_idx:start_search + 1]\n",
    "    \n",
    "    if len(segment) < 3:\n",
    "        return limit_idx\n",
    "        \n",
    "    grads = np.gradient(segment)\n",
    "    \n",
    "    max_slope = np.max(np.abs(grads))\n",
    "    threshold = 0.10 * max_slope\n",
    "    \n",
    "    for i in range(len(grads) - 1, -1, -1):\n",
    "        if np.abs(grads[i]) < threshold:\n",
    "            found_idx = limit_idx + i\n",
    "            return found_idx\n",
    "            \n",
    "    return limit_idx\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. WAVE DELINEATION\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def find_t_wave_offset_with_stability(signal, t_peak_idx, sampling_rate, \n",
    "                                      next_r_peak=None, limit_idx=None):\n",
    "    if np.isnan(t_peak_idx):\n",
    "        return np.nan\n",
    "    \n",
    "    t_peak_idx = int(t_peak_idx)\n",
    "    t_peak_value = signal[t_peak_idx]\n",
    "    \n",
    "    # ===== STAGE 1: Determine Search Limits =====\n",
    "    if next_r_peak is not None:\n",
    "        max_search = int(0.70 * (next_r_peak - t_peak_idx))\n",
    "        max_search = max(int(0.100 * sampling_rate), max_search)\n",
    "    else:\n",
    "        max_search = int(0.200 * sampling_rate)\n",
    "    \n",
    "    if limit_idx is not None and not np.isnan(limit_idx):\n",
    "        limit_idx = int(limit_idx)\n",
    "        dist_to_limit = limit_idx - t_peak_idx\n",
    "        if dist_to_limit <= 5:\n",
    "            return t_peak_idx + 5\n",
    "        max_search = min(max_search, dist_to_limit - 3)\n",
    "    \n",
    "    max_idx = min(len(signal) - 1, t_peak_idx + max_search)\n",
    "    \n",
    "    if max_idx <= t_peak_idx + 8:\n",
    "        return t_peak_idx + 5\n",
    "    \n",
    "    # ===== STAGE 2: Setup Parameters =====\n",
    "    skip_samples = max(3, int(0.008 * sampling_rate))  # Skip 8ms from peak\n",
    "    \n",
    "    if max_idx <= t_peak_idx + skip_samples + 5:\n",
    "        return t_peak_idx + skip_samples\n",
    "    \n",
    "    segment = signal[t_peak_idx:max_idx]\n",
    "    \n",
    "    # Window size for variance calculation (30 samples ≈ 60ms at 500Hz)\n",
    "    window_size = min(15, int(0.060 * sampling_rate))                                                      \n",
    "    \n",
    "    # Detect T-wave polarity\n",
    "    if t_peak_value > 0:\n",
    "        target_descent_sign = -1\n",
    "    else:\n",
    "        target_descent_sign = 1\n",
    "    \n",
    "    # ===== STAGE 3: Calculate Derivatives (for inflection backup) =====\n",
    "    derivative_1st = np.gradient(segment)\n",
    "    \n",
    "    if len(derivative_1st) > 5:\n",
    "        kernel_size = 3\n",
    "        derivative_1st_smooth = np.convolve(derivative_1st, \n",
    "                                            np.ones(kernel_size)/kernel_size, \n",
    "                                            mode='same')\n",
    "    else:\n",
    "        derivative_1st_smooth = derivative_1st\n",
    "    \n",
    "    derivative_2nd = np.gradient(derivative_1st_smooth)\n",
    "    \n",
    "    # Adaptive thresholds for slope-based methods\n",
    "    slope_magnitudes = np.abs(derivative_1st_smooth[skip_samples:])\n",
    "    \n",
    "    if len(slope_magnitudes) > 0:\n",
    "        slope_75th = np.percentile(slope_magnitudes, 75)\n",
    "        slope_median = np.median(slope_magnitudes)\n",
    "        steep_threshold = max(slope_75th * 0.4, 0.002)\n",
    "        flat_threshold = max(slope_median * 0.15, 0.0008)\n",
    "    else:\n",
    "        steep_threshold = 0.003\n",
    "        flat_threshold = 0.001\n",
    "    \n",
    "    # ===== STAGE 4: NEW METHOD - Local Variance Stability Detection =====\n",
    "    stability_idx = None\n",
    "    \n",
    "    # Start search after skip_samples\n",
    "    search_start = skip_samples + int(0.010 * sampling_rate)  # At least 10ms from peak\n",
    "    search_end = len(segment) - window_size\n",
    "    \n",
    "    if search_start < search_end:\n",
    "        variance_ratios = []\n",
    "        candidate_points = []\n",
    "        \n",
    "        for i in range(search_start, search_end):\n",
    "            # Get windows before and after current point\n",
    "            before_window = segment[max(0, i-window_size):i]\n",
    "            after_window = segment[i:min(len(segment), i+window_size)]\n",
    "            \n",
    "            if len(before_window) < 10 or len(after_window) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Calculate variance (spread of values)\n",
    "            var_before = np.var(before_window)\n",
    "            var_after = np.var(after_window)\n",
    "            \n",
    "            # Also calculate standard deviation for robustness\n",
    "            std_before = np.std(before_window)\n",
    "            std_after = np.std(after_window)\n",
    "            \n",
    "            # Calculate ratio (how much does variance drop?)\n",
    "            if var_before > 0:\n",
    "                var_ratio = var_after / var_before\n",
    "                std_ratio = std_after / std_before\n",
    "                \n",
    "                # Store results\n",
    "                variance_ratios.append(var_ratio)\n",
    "                candidate_points.append(i)\n",
    "        \n",
    "        # Find where variance drops significantly (baseline is more stable)\n",
    "        if len(variance_ratios) > 0:\n",
    "            variance_ratios = np.array(variance_ratios)\n",
    "            candidate_points = np.array(candidate_points)\n",
    "            \n",
    "            # Threshold: variance after should be < 40% of variance before\n",
    "            # This means we've transitioned from T-wave to stable baseline\n",
    "            stability_mask = variance_ratios < 0.15                                                      \n",
    "            \n",
    "            if np.any(stability_mask):\n",
    "                # Take the FIRST point where stability is achieved\n",
    "                first_stable = candidate_points[stability_mask][0]\n",
    "                stability_idx = first_stable\n",
    "    \n",
    "    # ===== STAGE 5: METHOD 2 - Inflection Point Detection (Backup) =====\n",
    "    inflection_idx = None\n",
    "    in_steep_descent = False\n",
    "    \n",
    "    for i in range(skip_samples, len(derivative_1st_smooth) - 2):\n",
    "        current_slope = derivative_1st_smooth[i]\n",
    "        \n",
    "        if target_descent_sign * current_slope < -steep_threshold:\n",
    "            if not in_steep_descent:\n",
    "                in_steep_descent = True\n",
    "        \n",
    "        elif in_steep_descent:\n",
    "            next_slopes = derivative_1st_smooth[i:min(i+4, len(derivative_1st_smooth))]\n",
    "            \n",
    "            # Option A: Slope becomes flat\n",
    "            if np.all(np.abs(next_slopes) < flat_threshold * 1.5):\n",
    "                inflection_idx = i\n",
    "                break\n",
    "            \n",
    "            # Option B: Slope reverses direction\n",
    "            if target_descent_sign * current_slope > 0:\n",
    "                if i + 2 < len(derivative_1st_smooth):\n",
    "                    if target_descent_sign * derivative_1st_smooth[i+1] > 0:\n",
    "                        inflection_idx = i\n",
    "                        break\n",
    "            \n",
    "            # Option C: Slope magnitude drops below threshold\n",
    "            if np.abs(current_slope) < flat_threshold:\n",
    "                if i + 3 < len(derivative_1st_smooth):\n",
    "                    if np.mean(np.abs(derivative_1st_smooth[i:i+3])) < flat_threshold * 1.3:\n",
    "                        inflection_idx = i\n",
    "                        break\n",
    "                else:\n",
    "                    inflection_idx = i\n",
    "                    break\n",
    "    \n",
    "    # ===== STAGE 6: METHOD 3 - Second Derivative Zero-Crossing =====\n",
    "    curvature_inflection_idx = None\n",
    "    \n",
    "    if len(derivative_2nd) > skip_samples + 10:\n",
    "        for i in range(skip_samples + 5, len(derivative_2nd) - 1):\n",
    "            if derivative_2nd[i] * derivative_2nd[i+1] <= 0:\n",
    "                if i > skip_samples + int(0.015 * sampling_rate):\n",
    "                    if i + 3 < len(derivative_1st_smooth):\n",
    "                        avg_slope_after = np.mean(np.abs(derivative_1st_smooth[i:i+3]))\n",
    "                        if avg_slope_after < steep_threshold * 0.6:\n",
    "                            curvature_inflection_idx = i\n",
    "                            break\n",
    "    \n",
    "    # ===== STAGE 7: METHOD 4 - Minimum Slope Magnitude =====\n",
    "    min_slope_idx = None\n",
    "    \n",
    "    if stability_idx is None and inflection_idx is None and curvature_inflection_idx is None:\n",
    "        search_start_min = skip_samples + int(0.020 * sampling_rate)\n",
    "        search_end_min = min(len(derivative_1st_smooth), skip_samples + int(0.100 * sampling_rate))\n",
    "        \n",
    "        if search_start_min < search_end_min:\n",
    "            slope_window = np.abs(derivative_1st_smooth[search_start_min:search_end_min])\n",
    "            if len(slope_window) > 0:\n",
    "                local_min = np.argmin(slope_window)\n",
    "                min_slope_idx = search_start_min + local_min\n",
    "    \n",
    "    # ===== STAGE 8: Select Best Detection (Prioritized) =====\n",
    "    candidates = []\n",
    "    \n",
    "    # HIGHEST PRIORITY: Variance stability (your method!)\n",
    "    if stability_idx is not None:\n",
    "        candidates.append(('variance_stability', t_peak_idx + stability_idx, 120))\n",
    "    \n",
    "    # HIGH PRIORITY: Inflection point\n",
    "    if inflection_idx is not None:\n",
    "        candidates.append(('inflection', t_peak_idx + inflection_idx, 100))\n",
    "    \n",
    "    # MEDIUM PRIORITY: Curvature change\n",
    "    if curvature_inflection_idx is not None:\n",
    "        candidates.append(('curvature', t_peak_idx + curvature_inflection_idx, 80))\n",
    "    \n",
    "    # LOW PRIORITY: Minimum slope\n",
    "    if min_slope_idx is not None:\n",
    "        candidates.append(('min_slope', t_peak_idx + min_slope_idx, 60))\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        # Ultimate fallback\n",
    "        return min(t_peak_idx + int(0.040 * sampling_rate), max_idx)\n",
    "    \n",
    "    # Use the highest priority detection\n",
    "    best_method, best_offset, best_score = max(candidates, key=lambda x: x[2])\n",
    "    \n",
    "    # Debug info (optional - can be removed in production)\n",
    "    # print(f\"  T-offset method used: {best_method} (score: {best_score})\")\n",
    "    \n",
    "    # ===== STAGE 9: Validation =====\n",
    "    best_offset = max(best_offset, t_peak_idx + skip_samples)\n",
    "    best_offset = min(best_offset, max_idx)\n",
    "    \n",
    "    # Sanity check on duration\n",
    "    duration_ms = (best_offset - t_peak_idx) / sampling_rate * 1000\n",
    "    \n",
    "    if duration_ms < 15:\n",
    "        best_offset = t_peak_idx + int(0.030 * sampling_rate)\n",
    "    elif duration_ms > 300:\n",
    "        best_offset = t_peak_idx + int(0.100 * sampling_rate)\n",
    "    \n",
    "    return int(best_offset)\n",
    "\n",
    "def improved_delineate_ecg_waves(signal, r_peaks, sampling_rate):\n",
    "    \"\"\"\n",
    "    Complete ECG wave delineation with ultra-accurate T-offset detection\n",
    "    \"\"\"\n",
    "    waves = {\n",
    "        'p_peak': [], 'p_onset': [], 'p_offset': [],\n",
    "        'q_peak': [], 'q_onset': [],\n",
    "        's_peak': [], 's_offset': [],\n",
    "        't_peak': [], 't_onset': [], 't_offset': []\n",
    "    }\n",
    "    \n",
    "    signal_len = len(signal)\n",
    "    \n",
    "    # Detect P-peaks\n",
    "    p_peaks = enhanced_p_wave_detection(signal, r_peaks, sampling_rate, \"Segment\")\n",
    "    \n",
    "    # Helper for generic boundaries\n",
    "    def find_boundary_local(peak_idx, direction, max_search_samples, thresh_factor=0.05):\n",
    "        if np.isnan(peak_idx): return np.nan\n",
    "        peak_idx = int(peak_idx)\n",
    "        limit = peak_idx + (direction * max_search_samples)\n",
    "        limit = max(0, min(signal_len, limit))\n",
    "        if abs(limit - peak_idx) < 3: return peak_idx\n",
    "        start, end = sorted([peak_idx, limit])\n",
    "        segment = signal[start:end]\n",
    "        if direction == -1: segment = segment[::-1] \n",
    "        diff = np.diff(segment)\n",
    "        if len(diff) == 0: return peak_idx\n",
    "        max_slope = np.max(np.abs(diff))\n",
    "        thresh = max_slope * thresh_factor\n",
    "        for i in range(1, len(diff)):\n",
    "            if np.abs(diff[i]) < thresh:\n",
    "                return peak_idx + (direction * i)\n",
    "        return limit\n",
    "\n",
    "    last_t_offset = None\n",
    "\n",
    "    for i, r in enumerate(r_peaks):\n",
    "        r = int(r)\n",
    "        r_height = abs(signal[r]) if abs(signal[r]) > 0.05 else 1.0\n",
    "        \n",
    "        # --- P-WAVE ---\n",
    "        p_peak_val = p_peaks[i] if i < len(p_peaks) else np.nan\n",
    "        waves['p_peak'].append(p_peak_val)\n",
    "        \n",
    "        if not np.isnan(p_peak_val):\n",
    "            p_onset = find_p_onset_constrained(signal, p_peak_val, sampling_rate, last_t_offset)\n",
    "            waves['p_onset'].append(p_onset)\n",
    "            waves['p_offset'].append(find_boundary_local(int(p_peak_val), 1, int(0.08 * sampling_rate)))\n",
    "        else:\n",
    "            waves['p_onset'].append(np.nan)\n",
    "            waves['p_offset'].append(np.nan)\n",
    "\n",
    "        # --- Q-WAVE ---\n",
    "        win_q = int(0.05 * sampling_rate)\n",
    "        q_search_start = max(0, r - win_q)\n",
    "        q_window = signal[q_search_start:r]\n",
    "        q_idx = q_search_start + np.argmin(q_window) if len(q_window) > 0 else np.nan\n",
    "        waves['q_peak'].append(q_idx)\n",
    "        anchor_q = q_idx if not np.isnan(q_idx) else r\n",
    "        waves['q_onset'].append(find_boundary_local(anchor_q, -1, int(0.04 * sampling_rate)))\n",
    "        \n",
    "        # --- S-WAVE ---\n",
    "        win_s = int(0.06 * sampling_rate)\n",
    "        s_search_end = min(signal_len, r + win_s)\n",
    "        s_window = signal[r:s_search_end]\n",
    "        s_idx = r + np.argmin(s_window) if len(s_window) > 0 else np.nan\n",
    "        waves['s_peak'].append(s_idx)\n",
    "        anchor_s = s_idx if not np.isnan(s_idx) else r\n",
    "        waves['s_offset'].append(find_boundary_local(anchor_s, 1, int(0.04 * sampling_rate)))\n",
    "        \n",
    "        # --- T-WAVE ---\n",
    "        rr_next = (int(r_peaks[i+1]) - r) if i < len(r_peaks) - 1 else 1.0 * sampling_rate\n",
    "        dyn_t_start = int(0.10 * sampling_rate)\n",
    "        dyn_t_end = int(min(0.600 * sampling_rate, 0.65 * rr_next))\n",
    "        t_search_start = min(signal_len, r + dyn_t_start)\n",
    "        t_search_end = min(signal_len, r + dyn_t_end)\n",
    "        t_idx = np.nan\n",
    "        \n",
    "        if t_search_start < t_search_end:\n",
    "            t_window = signal[t_search_start:t_search_end]\n",
    "            if len(t_window) > 0:\n",
    "                local_peaks, _ = sp_signal.find_peaks(t_window, prominence=(0.05 * r_height))\n",
    "                if len(local_peaks) > 0:\n",
    "                    best_peak = local_peaks[np.argmax(t_window[local_peaks])]\n",
    "                    t_idx = t_search_start + best_peak\n",
    "                else:\n",
    "                    t_idx = t_search_start + np.argmax(t_window)\n",
    "        \n",
    "        waves['t_peak'].append(t_idx)\n",
    "        waves['t_onset'].append(find_boundary_local(t_idx, -1, int(0.08 * sampling_rate)))\n",
    "        \n",
    "        # --- T-OFFSET (ULTRA ACCURATE) ---\n",
    "        next_r = int(r_peaks[i+1]) if i < len(r_peaks) - 1 else None\n",
    "        next_p_onset_limit = None\n",
    "        if i + 1 < len(p_peaks):\n",
    "            next_p_peak = p_peaks[i+1]\n",
    "            if not np.isnan(next_p_peak):\n",
    "                next_p_onset_limit = find_boundary_local(int(next_p_peak), -1, int(0.08 * sampling_rate))\n",
    "        \n",
    "        t_offset = find_t_wave_offset_with_stability(\n",
    "            signal, \n",
    "            t_idx, \n",
    "            sampling_rate, \n",
    "            next_r_peak=next_r,\n",
    "            limit_idx=next_p_onset_limit\n",
    "        )\n",
    "\n",
    "        waves['t_offset'].append(t_offset)\n",
    "        \n",
    "        last_t_offset = t_offset\n",
    "\n",
    "    for k in waves:\n",
    "        waves[k] = np.array(waves[k])\n",
    "    \n",
    "    return waves\n",
    "\n",
    "\n",
    "def calculate_intervals(waves, sampling_rate):\n",
    "    \"\"\"Calculate PR, QRS, and QT intervals\"\"\"\n",
    "    pr_intervals = (waves['q_onset'] - waves['p_onset']) / sampling_rate * 1000\n",
    "    # qrs_durations = (waves['s_offset'] - waves['q_onset']) / sampling_rate * 1000\n",
    "    # qrs_durations = (waves['s_peak'] - waves['q_peak']) / sampling_rate * 1000\n",
    "    qrs_durations = (waves['s_peak'] - waves['q_onset']) / sampling_rate * 1000\n",
    "    qt_intervals = (waves['t_offset'] - waves['q_onset']) / sampling_rate * 1000\n",
    "   \n",
    "    pr_intervals = np.where((pr_intervals > 40) & (pr_intervals < 600), pr_intervals, np.nan)\n",
    "    qrs_durations = np.where((qrs_durations > 30) & (qrs_durations < 200), qrs_durations, np.nan)\n",
    "    qt_intervals = np.where((qt_intervals > 100) & (qt_intervals < 600), qt_intervals, np.nan)\n",
    "   \n",
    "    return pr_intervals, qrs_durations, qt_intervals\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_raw, ecg_filtered, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    \"\"\"Process ECG into segments for analysis\"\"\"\n",
    "    max_len = len(ecg_raw)\n",
    "    window_step = round((max_len - min_segment_length) / (num_segments - 1)) if num_segments > 1 else 0\n",
    "    results = []\n",
    "   \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = min(start_idx + min_segment_length, max_len)\n",
    "        if start_idx < 0: start_idx = 0\n",
    "        \n",
    "        segment_raw = ecg_raw[start_idx:end_idx]\n",
    "        segment_filtered = ecg_filtered[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment_raw) < 100: continue\n",
    "        \n",
    "        _, r_peaks, bpm, _ = qrs_detect(segment_raw, sampling_rate, len(segment_raw)/sampling_rate)\n",
    "        \n",
    "        waves = improved_delineate_ecg_waves(segment_filtered, r_peaks, sampling_rate)\n",
    "        pr, qrs, qt = calculate_intervals(waves, sampling_rate)\n",
    "        \n",
    "        def adj(arr):\n",
    "            if len(arr) == 0: return np.array([])\n",
    "            return arr + start_idx\n",
    "   \n",
    "        res = {\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_raw': segment_raw,\n",
    "            'ecg_filtered': segment_filtered,\n",
    "            'bpm': bpm,\n",
    "            'r_peaks': adj(r_peaks),\n",
    "            'avg_pr': np.nanmean(pr),\n",
    "            'avg_qrs': np.nanmean(qrs),\n",
    "            'avg_qt': np.nanmean(qt)\n",
    "        }\n",
    "        for k, v in waves.items():\n",
    "            res[k] = adj(v)\n",
    "            \n",
    "        results.append(res)\n",
    "   \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6. PLOTTING FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def plot_ecg_segments(ecg_raw, ecg_filtered, sampling_rate, results, title=\"ECG Analysis\"):\n",
    "    \"\"\"Plot ECG segments with detected waves and intervals\"\"\"\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(20, 4*num_segments))\n",
    "    if num_segments == 1: axes = [axes]\n",
    "   \n",
    "    time = np.arange(len(ecg_raw)) / sampling_rate\n",
    "   \n",
    "    def get_valid(indices):\n",
    "        if len(indices) == 0: return np.array([], dtype=int)\n",
    "        valid = indices[~np.isnan(indices)]\n",
    "        valid = valid[valid < len(ecg_raw)]\n",
    "        return valid.astype(int)\n",
    "\n",
    "    t_wave_issues = 0\n",
    "    \n",
    "    for i, (ax, res) in enumerate(zip(axes, results)):\n",
    "        seg_time = time[res['start_idx']:res['end_idx']]\n",
    "        \n",
    "        ax.plot(seg_time, res['ecg_filtered'], 'b-', alpha=0.7, linewidth=0.8, label='Filtered')\n",
    "        ax.plot(seg_time, res['ecg_raw'], 'k-', alpha=0.3, linewidth=0.5, label='Raw')\n",
    "        \n",
    "        peaks = [('r_peaks', 'ro', 'R'), ('p_peak', 'g^', 'P'), ('t_peak', 'bD', 'T')]\n",
    "        for key, style, lbl in peaks:\n",
    "            valid = get_valid(res[key])\n",
    "            if len(valid): \n",
    "                ax.plot(time[valid], ecg_filtered[valid], style, markersize=6, label=lbl)\n",
    "\n",
    "        p_onsets = res['p_onset']\n",
    "        q_onsets = res['q_onset']\n",
    "        t_offsets = res['t_offset']\n",
    "        t_peaks_arr = res['t_peak']\n",
    "        \n",
    "        for j in range(len(t_peaks_arr)):\n",
    "            if not np.isnan(t_peaks_arr[j]) and not np.isnan(t_offsets[j]):\n",
    "                if abs(t_peaks_arr[j] - t_offsets[j]) < 2:\n",
    "                    t_wave_issues += 1\n",
    "        \n",
    "        y_min = np.min(res['ecg_filtered'])\n",
    "        bar_y_pr = y_min - 0.05\n",
    "        bar_y_qt = y_min - 0.10\n",
    "        \n",
    "        count = 0\n",
    "        for j in range(len(p_onsets)):\n",
    "            if j < len(q_onsets) and not np.isnan(p_onsets[j]) and not np.isnan(q_onsets[j]):\n",
    "                pon = int(p_onsets[j])\n",
    "                qon = int(q_onsets[j])\n",
    "                if pon < qon:\n",
    "                    ax.hlines(y=bar_y_pr, xmin=time[pon], xmax=time[qon], colors='green', linewidth=4, alpha=0.7)\n",
    "                    if count == 0: ax.text(time[pon], bar_y_pr, 'PR', color='green', fontsize=8, ha='right', va='center')\n",
    "\n",
    "            if j < len(t_offsets) and not np.isnan(q_onsets[j]) and not np.isnan(t_offsets[j]):\n",
    "                qon = int(q_onsets[j])\n",
    "                toff = int(t_offsets[j])\n",
    "                if qon < toff:\n",
    "                    ax.hlines(y=bar_y_qt, xmin=time[qon], xmax=time[toff], colors='blue', linewidth=4, alpha=0.7)\n",
    "                    if count == 0: ax.text(time[toff], bar_y_qt, 'QT', color='blue', fontsize=8, ha='left', va='center')\n",
    "            count += 1\n",
    "\n",
    "        valid = get_valid(res['p_onset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'gx', markersize=8, label='P-start')\n",
    "        \n",
    "        valid = get_valid(res['q_onset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'm|', markersize=12, markeredgewidth=2, label='QRS-start')\n",
    "        \n",
    "        # ==========================================\n",
    "        # ADDED: QRS END PLOTTING (S-OFFSET)\n",
    "        # ==========================================\n",
    "        valid = get_valid(res['s_offset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'c|', markersize=12, markeredgewidth=2, label='QRS-end')\n",
    "        # ==========================================\n",
    "\n",
    "        valid = get_valid(res['t_offset'])\n",
    "        if len(valid): ax.plot(time[valid], ecg_filtered[valid], 'b|', markersize=12, markeredgewidth=2, label='T-end')\n",
    "\n",
    "        info = f\"Seg {res['segment_num']} | BPM: {res['bpm']:.0f} | \"\n",
    "        info += f\"PR: {res['avg_pr']:.0f}ms | QRS: {res['avg_qrs']:.0f}ms | QT: {res['avg_qt']:.0f}ms\"\n",
    "        \n",
    "        ax.set_title(info, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlim([seg_time[0], seg_time[-1]])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if i == 0: ax.legend(loc='upper right', ncol=7, fontsize='small') # Increased ncol for new item\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_p_wave_quality(signal, r_peaks, p_peaks, sampling_rate, segment_num=\"\"):\n",
    "    \"\"\"Visualize P-wave detection quality\"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    \n",
    "    time = np.arange(len(signal)) / sampling_rate\n",
    "    axes[0].plot(time, signal, 'k-', alpha=0.6, linewidth=0.8)\n",
    "    \n",
    "    valid_r = r_peaks[~np.isnan(r_peaks)].astype(int)\n",
    "    axes[0].plot(time[valid_r], signal[valid_r], 'ro', markersize=6, label='R-peaks')\n",
    "    \n",
    "    valid_p = p_peaks[~np.isnan(p_peaks)].astype(int)\n",
    "    axes[0].plot(time[valid_p], signal[valid_p], 'g^', markersize=6, label='P-waves')\n",
    "    \n",
    "    missing_p = np.where(np.isnan(p_peaks[1:]))[0] + 1\n",
    "    if len(missing_p) > 0:\n",
    "        missing_r = r_peaks[missing_p].astype(int)\n",
    "        axes[0].plot(time[missing_r], signal[missing_r], 'rx', markersize=10, \n",
    "                    markeredgewidth=2, label=f'Missing P ({len(missing_p)})')\n",
    "    \n",
    "    axes[0].set_title(f'P-wave Detection - Segment {segment_num}')\n",
    "    axes[0].set_ylabel('Amplitude (mV)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    pr_intervals = []\n",
    "    beat_numbers = []\n",
    "    beat_idx = 0\n",
    "    for i, (r, p) in enumerate(zip(r_peaks, p_peaks)):\n",
    "        if not np.isnan(p) and not np.isnan(r):\n",
    "            pr = (r - p) / sampling_rate * 1000\n",
    "            pr_intervals.append(pr)\n",
    "            beat_numbers.append(beat_idx)\n",
    "        beat_idx += 1\n",
    "    \n",
    "    if pr_intervals:\n",
    "        axes[1].plot(beat_numbers, pr_intervals, 'bo-', markersize=4)\n",
    "        axes[1].axhline(y=120, color='g', linestyle='--', alpha=0.5, label='Normal PR min (120ms)')\n",
    "        axes[1].axhline(y=200, color='orange', linestyle='--', alpha=0.5, label='1st° AVB threshold (200ms)')\n",
    "        axes[1].axhline(y=np.mean(pr_intervals), color='b', linestyle='-', alpha=0.7, \n",
    "                               label=f'Mean: {np.mean(pr_intervals):.1f}ms')\n",
    "        axes[1].set_ylabel('PR Interval (ms)')\n",
    "        axes[1].set_xlabel('Beat Number')\n",
    "        axes[1].set_title(f'PR Intervals (mean: {np.mean(pr_intervals):.1f}ms ± {np.std(pr_intervals):.1f}ms)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend()\n",
    "    \n",
    "    p_amplitudes = []\n",
    "    beat_numbers_amp = []\n",
    "    beat_idx = 0\n",
    "    for p in valid_p:\n",
    "        p_amplitudes.append(abs(signal[p]))\n",
    "        beat_numbers_amp.append(beat_idx)\n",
    "        beat_idx += 1\n",
    "    \n",
    "    if p_amplitudes:\n",
    "        axes[2].plot(beat_numbers_amp, p_amplitudes, 'go-', markersize=4)\n",
    "        axes[2].axhline(y=0.1, color='g', linestyle='--', alpha=0.5, label='Typical P (0.1mV)')\n",
    "        axes[2].axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='Low amplitude threshold')\n",
    "        axes[2].axhline(y=np.mean(p_amplitudes), color='g', linestyle='-', alpha=0.7,\n",
    "                               label=f'Mean: {np.mean(p_amplitudes):.3f}mV')\n",
    "        axes[2].set_ylabel('Amplitude (mV)')\n",
    "        axes[2].set_xlabel('Beat Number')\n",
    "        axes[2].set_title(f'P-wave Amplitudes (mean: {np.mean(p_amplitudes):.3f}mV ± {np.std(p_amplitudes):.3f}mV)')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend()\n",
    "    \n",
    "    window_size = int(2 * sampling_rate)\n",
    "    num_windows = len(signal) // window_size\n",
    "    quality_over_time = []\n",
    "    time_points = []\n",
    "    \n",
    "    for w in range(num_windows):\n",
    "        start = w * window_size\n",
    "        end = min((w + 1) * window_size, len(signal))\n",
    "        window_signal = signal[start:end]\n",
    "        quality = calculate_signal_quality(window_signal)\n",
    "        quality_over_time.append(quality)\n",
    "        time_points.append((start + end) / 2 / sampling_rate)\n",
    "    \n",
    "    if quality_over_time:\n",
    "        axes[3].plot(time_points, quality_over_time, 'r-', linewidth=2, label='Signal Quality')\n",
    "        axes[3].axhline(y=0.7, color='g', linestyle='--', alpha=0.5, label='Good (>0.7)')\n",
    "        axes[3].axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate (>0.5)')\n",
    "        axes[3].axhline(y=0.3, color='r', linestyle='--', alpha=0.5, label='Poor (<0.3)')\n",
    "        axes[3].fill_between(time_points, 0, quality_over_time, alpha=0.3, color='red')\n",
    "        axes[3].set_ylabel('Quality Index')\n",
    "        axes[3].set_xlabel('Time (s)')\n",
    "        axes[3].set_title('Signal Quality Over Time')\n",
    "        axes[3].set_ylim([0, 1])\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        axes[3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('/mnt/user-data/outputs/p_wave_quality.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    total_beats = len(r_peaks) - 1\n",
    "    detected_p = np.sum(~np.isnan(p_peaks[1:]))\n",
    "    detection_rate = detected_p / total_beats * 100 if total_beats > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- P-Wave Detection Summary (Segment {segment_num}) ---\")\n",
    "    print(f\"Total R-peaks: {len(r_peaks)}\")\n",
    "    print(f\"P-waves detected: {detected_p}/{total_beats} ({detection_rate:.1f}%)\")\n",
    "    print(f\"Mean signal quality: {np.mean(quality_over_time):.2f}\")\n",
    "    if pr_intervals:\n",
    "        print(f\"PR interval: {np.mean(pr_intervals):.1f} ± {np.std(pr_intervals):.1f} ms\")\n",
    "    if p_amplitudes:\n",
    "        print(f\"P amplitude: {np.mean(p_amplitudes):.3f} ± {np.std(p_amplitudes):.3f} mV\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def plot_full_ecg(ecg_raw, ecg_filtered, sampling_rate, waves, r_peaks, bpm, title=\"Full ECG Analysis\"):\n",
    "    # --- 1. Setup Figure (Dynamic Width) ---\n",
    "    duration_sec = len(ecg_raw) / sampling_rate\n",
    "    fig_width = max(15, min(100, int(duration_sec * 0.8))) \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(fig_width, 6))\n",
    "    time = np.arange(len(ecg_raw)) / sampling_rate\n",
    "\n",
    "    # --- 2. Plot Signals ---\n",
    "    ax.plot(time, ecg_filtered, 'b-', alpha=0.7, linewidth=0.8, label='Filtered')\n",
    "    ax.plot(time, ecg_raw, 'k-', alpha=0.3, linewidth=0.5, label='Raw')\n",
    "\n",
    "    # Helper to clean indices\n",
    "    def get_valid(indices):\n",
    "        if len(indices) == 0: return np.array([], dtype=int)\n",
    "        valid = indices[~np.isnan(indices)]\n",
    "        valid = valid[(valid >= 0) & (valid < len(ecg_raw))]\n",
    "        return valid.astype(int)\n",
    "\n",
    "    # --- 3. Plot Peaks (R, P, T) ---\n",
    "    valid_r = get_valid(r_peaks)\n",
    "    if len(valid_r):\n",
    "        ax.plot(time[valid_r], ecg_filtered[valid_r], 'ro', markersize=6, label='R_peak')\n",
    "\n",
    "    if 'p_peak' in waves:\n",
    "        valid_p = get_valid(waves['p_peak'])\n",
    "        if len(valid_p):\n",
    "            ax.plot(time[valid_p], ecg_filtered[valid_p], 'g^', markersize=6, label='P_peak')\n",
    "\n",
    "    if 't_peak' in waves:\n",
    "        valid_t = get_valid(waves['t_peak'])\n",
    "        if len(valid_t):\n",
    "            ax.plot(time[valid_t], ecg_filtered[valid_t], 'bD', markersize=6, label='T_peak')\n",
    "\n",
    "    # --- 4. Plot Interval Bars (PR & QT) ---\n",
    "    y_min = np.min(ecg_filtered)\n",
    "    bar_y_pr = y_min - (np.ptp(ecg_filtered) * 0.05)\n",
    "    bar_y_qt = y_min - (np.ptp(ecg_filtered) * 0.10)\n",
    "\n",
    "    p_onsets = waves.get('p_onset', [])\n",
    "    q_onsets = waves.get('q_onset', [])\n",
    "    t_offsets = waves.get('t_offset', [])\n",
    "\n",
    "    count_pr = 0\n",
    "    count_qt = 0\n",
    "    \n",
    "    num_beats = len(r_peaks)\n",
    "    \n",
    "    for j in range(num_beats):\n",
    "        if j < len(p_onsets) and j < len(q_onsets):\n",
    "            pon = p_onsets[j]\n",
    "            qon = q_onsets[j]\n",
    "            \n",
    "            if not np.isnan(pon) and not np.isnan(qon):\n",
    "                pon, qon = int(pon), int(qon)\n",
    "                if pon < qon and qon < len(time):\n",
    "                    ax.hlines(y=bar_y_pr, xmin=time[pon], xmax=time[qon], \n",
    "                             colors='green', linewidth=4, alpha=0.7)\n",
    "                    if count_pr == 0: \n",
    "                        ax.text(time[pon], bar_y_pr, 'PR', color='green', \n",
    "                               fontsize=8, ha='right', va='center', fontweight='bold')\n",
    "                    count_pr += 1\n",
    "\n",
    "        if j < len(q_onsets) and j < len(t_offsets):\n",
    "            qon = q_onsets[j]\n",
    "            toff = t_offsets[j]\n",
    "            \n",
    "            if not np.isnan(qon) and not np.isnan(toff):\n",
    "                qon, toff = int(qon), int(toff)\n",
    "                if qon < toff and toff < len(time):\n",
    "                    ax.hlines(y=bar_y_qt, xmin=time[qon], xmax=time[toff], \n",
    "                             colors='blue', linewidth=4, alpha=0.7)\n",
    "                    if count_qt == 0: \n",
    "                        ax.text(time[toff], bar_y_qt, 'QT', color='blue', \n",
    "                               fontsize=8, ha='left', va='center', fontweight='bold')\n",
    "                    count_qt += 1\n",
    "\n",
    "    # --- 5. Plot Specific Markers (Onsets/Offsets) ---\n",
    "    if 'p_onset' in waves:\n",
    "        valid_pon = get_valid(waves['p_onset'])\n",
    "        if len(valid_pon):\n",
    "            ax.plot(time[valid_pon], ecg_filtered[valid_pon], 'gx', markersize=8, label='P-start')\n",
    "\n",
    "    if 'q_onset' in waves:\n",
    "        valid_qon = get_valid(waves['q_onset'])\n",
    "        if len(valid_qon):\n",
    "            ax.plot(time[valid_qon], ecg_filtered[valid_qon], 'm|', markersize=12, markeredgewidth=2, label='QRS-start')\n",
    "\n",
    "    # ==========================================\n",
    "    # ADDED: QRS END PLOTTING (S-OFFSET)\n",
    "    # ==========================================\n",
    "    if 's_offset' in waves:\n",
    "        valid_soff = get_valid(waves['s_offset'])\n",
    "        if len(valid_soff):\n",
    "            ax.plot(time[valid_soff], ecg_filtered[valid_soff], 'c|', markersize=12, markeredgewidth=2, label='QRS-end')\n",
    "    # ==========================================\n",
    "\n",
    "    if 't_offset' in waves:\n",
    "        valid_toff = get_valid(waves['t_offset'])\n",
    "        if len(valid_toff):\n",
    "            ax.plot(time[valid_toff], ecg_filtered[valid_toff], 'b|', markersize=12, markeredgewidth=2, label='T-end')\n",
    "\n",
    "    # --- 6. Add Statistics Box ---\n",
    "    pr, qrs, qt = calculate_intervals(waves, sampling_rate)\n",
    "    stats_text = (\n",
    "        f\"HEART RATE: {bpm:.0f} BPM\\n\"\n",
    "        f\"Avg PR:  {np.nanmean(pr):.0f} ms\\n\"\n",
    "        f\"Avg QRS: {np.nanmean(qrs):.0f} ms\\n\"\n",
    "        f\"Avg QT:  {np.nanmean(qt):.0f} ms\"\n",
    "    )\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9)\n",
    "    ax.text(0.005, 0.95, stats_text, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', bbox=props, fontfamily='monospace')\n",
    "\n",
    "    # --- 7. Formatting ---\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.set_xlim([0, time[-1]])\n",
    "    ax.grid(True, which='both', alpha=0.3)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', ncol=5, fontsize='small')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ==========================================\n",
    "# 7. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Filter setup\n",
    "    freq = 500\n",
    "    low_pass_cutoff = 40\n",
    "    low_pass_order = 7\n",
    "    b_lp, a_lp = sp_signal.butter(low_pass_order, low_pass_cutoff / (freq / 2), btype=\"low\")\n",
    "    b_notch, a_notch = sp_signal.iirnotch(50, 50 / 20, freq)  \n",
    "\n",
    "    def low_pass_filter(data):\n",
    "        try:\n",
    "            return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "    def notch_filter(data):\n",
    "        try:\n",
    "            return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "    # input_json = r\"simulator\\contec\\30bpm_1756099931979.json\"   \n",
    "    # input_json = r\"simulator\\contec\\40bpm_1756099996286.json\"   \n",
    "    # input_json = r\"simulator\\contec\\60bpm_1756100055600.json\"   \n",
    "    # input_json = r\"simulator\\contec\\80bpm_1756100133843.json\"   \n",
    "    # input_json = r\"simulator\\contec\\100bpm_1756100188299.json\"   \n",
    "    # input_json = r\"simulator\\contec\\120bpm_1756100243822.json\"     \n",
    "    # input_json = r\"simulator\\contec\\140bpm_1756100303625.json\"   \n",
    "    # input_json = r\"simulator\\contec\\180bpm_1756100430492.json\"   \n",
    "    # input_json = r\"simulator\\contec\\200bpm_1756100489086.json\"   \n",
    "    # input_json = r\"simulator\\contec\\220bpm_1756100542987.json\"   \n",
    "\n",
    "    # input_json = r\"intervals\\Antor_1769669515402.json\"   \n",
    "    # input_json = r\"intervals\\Asif_1769673034525.json\"   \n",
    "    input_json = r\"intervals\\Atiur_1769684477237.json\"     #### 89\n",
    "    # input_json = r\"intervals\\Aupo_1769678570784.json\"     #### 98\n",
    "    # input_json = r\"intervals\\Faizur_1769672384762.json\"    \n",
    "    # input_json = r\"intervals\\Alauddin_1769763819330.json\"   \n",
    "\n",
    "    with open(input_json, 'r') as file:\n",
    "        file_data = json.load(file)\n",
    "\n",
    "\n",
    "    # # input_json = r\"exception\\L2_1759207950416.json\"   \n",
    "    # input_json = r\"issues\\L2_1757064122874.json\"  \n",
    "    # # input_json = r\"issues\\L2_1757579288752.json\"\n",
    "    # # input_json = r\"issues\\L2_1757737806463.json\"  \n",
    "    # # input_json = r\"v01_prob\\L2_1765984517025.json\"  \n",
    "    # # input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "    # # input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "    # # input_json = r\"issues2\\1757998097068\\L2_1757998097068.json\"\n",
    "    # # input_json = r\"issues2\\1758943573744\\L2_1758943573744.json\"\n",
    "    # # input_json = r\"issues2\\1759059066184\\L2_1759059066184.json\"     \n",
    "    # # input_json = r\"issues2\\1759117739887\\L2_1759117739887.json\"\n",
    "    # # input_json = r\"issues2\\1759118709079\\L2_1759118709079.json\"\n",
    "    # # input_json = r\"issues2\\1759202739736\\L2_1759202739736.json\"  \n",
    "    # # input_json = r\"issues2\\1759639059357\\L2_1759639059357.json\"  ####\n",
    "\n",
    "    # doubles = []\n",
    "    # with open(input_json, \"rb\") as f:\n",
    "    #     while chunk := f.read(8):\n",
    "    #         if len(chunk) < 8:\n",
    "    #             break\n",
    "    #         value = struct.unpack(\"<d\", chunk)[0]\n",
    "    #         doubles.append(value)\n",
    "\n",
    "    # file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "    # # def decrypt(input_file):\n",
    "    # #     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "    # #     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    # #     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    # #     with open(input_file, 'rb') as f:\n",
    "    # #         encrypted_data = f.read()\n",
    "    # #     enc = base64.b64decode(encrypted_data[24:])\n",
    "    # #     data = unpad(cipher.decrypt(enc), 16)\n",
    "    # #     decoded_string = data.decode('utf-8')\n",
    "    # #     return json.loads(decoded_string)\n",
    "\n",
    "    # input_json = r\"NHF2\\DATA_1750689015865.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750689460556.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750851207409.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750858856842.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750862721789.json\"  ####\n",
    "    # # input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "    # file_data = decrypt(input_json)\n",
    "\n",
    "    # # input_json = r\"NHF\\DATA_1752067426678.json\"  \n",
    "    # # input_json = r\"NHF\\DATA_1752121970835.json\"  \n",
    "    # # input_json = r\"NHF\\DATA_1754709586876.json\"  \n",
    "    # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "    # file_data = decrypt(input_json)  \n",
    "        \n",
    "    try:\n",
    "        # Process raw data for R-peak detection\n",
    "        print(\"Processing raw data for R-peak detection...\")\n",
    "        raw_data = data_process(file_data)\n",
    "        ecg_raw = raw_data[0, :15000, 0]\n",
    "        \n",
    "        # Process filtered data for P, Q, S, T wave detection\n",
    "        print(\"Processing filtered data for wave delineation...\")\n",
    "        filtered_data = data_process(low_pass_filter(notch_filter(baseline_wander(np.array(file_data[\"dataL2\"])))))\n",
    "        ecg_filtered = filtered_data[0, :15000, 0]\n",
    "        \n",
    "        sampling_rate = 500\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ULTRA-ACCURATE ECG ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"✓ Using raw data for R-peak detection\")\n",
    "        print(\"✓ Using filtered data for P, Q, S, T wave delineation\")\n",
    "        print(\"✓ Enhanced T-wave offset detection (first steep descent)\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        segment_results = process_ecg_segments(\n",
    "            ecg_raw=ecg_raw,\n",
    "            ecg_filtered=ecg_filtered,\n",
    "            sampling_rate=sampling_rate,\n",
    "            num_segments=4,\n",
    "            min_segment_length=5000\n",
    "        )\n",
    "\n",
    "        # Plot results\n",
    "        plot_ecg_segments(\n",
    "            ecg_raw,\n",
    "            ecg_filtered,\n",
    "            sampling_rate,\n",
    "            segment_results,\n",
    "            \"Ultra-Accurate Clinical Interval Analysis\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ULTRA-ACCURATE FULL ECG ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # --- 2. PEAK DETECTION ---\n",
    "        print(\"Detecting R-peaks on signal...\")\n",
    "        _, r_peaks, bpm, _ = qrs_detect(ecg_raw, sampling_rate)\n",
    "        print(f\"✓ Detected {len(r_peaks)} R-peaks. BPM: {bpm:.1f}\")\n",
    "\n",
    "        # --- 3. WAVE DELINEATION ---\n",
    "        print(\"Delineating waves (P, Q, S, T)...\")\n",
    "        waves = improved_delineate_ecg_waves(ecg_filtered, r_peaks, sampling_rate)\n",
    "        \n",
    "        # --- 4. CALCULATE STATISTICS ---\n",
    "        pr, qrs, qt = calculate_intervals(waves, sampling_rate)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"FULL SIGNAL STATISTICS\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"Avg PR Interval:  {np.nanmean(pr):.1f} ms\")\n",
    "        print(f\"Avg QRS Duration: {np.nanmean(qrs):.1f} ms\")\n",
    "        print(f\"Avg QT Interval:  {np.nanmean(qt):.1f} ms\")\n",
    "        print(\"=\"*40 + \"\\n\")\n",
    "        \n",
    "        # --- 5. PLOT ---\n",
    "        print(\"Generating full plot with statistics...\")\n",
    "        plot_full_ecg(\n",
    "            ecg_raw, \n",
    "            ecg_filtered, \n",
    "            sampling_rate, \n",
    "            waves, \n",
    "            r_peaks, \n",
    "            bpm\n",
    "        )\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found ({input_json}). Please check the path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32d2de",
   "metadata": {},
   "source": [
    "## flat line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import pywt\n",
    "import scipy.signal as sp_signal\n",
    "import matplotlib.pyplot as plt\n",
    "import base64   \n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "\n",
    "# Define filter coefficients if not defined\n",
    "fs = 500  # sampling rate\n",
    "nyq = 0.5 * fs\n",
    "\n",
    "# Example low pass filter (cutoff 40 Hz)\n",
    "low_cutoff = 40 / nyq\n",
    "b_lp, a_lp = sp_signal.butter(4, low_cutoff, btype='low')\n",
    "\n",
    "# Example notch filter (50 Hz)\n",
    "q = 30\n",
    "w0 = 50 / nyq\n",
    "b_notch, a_notch = sp_signal.iirnotch(w0, q)\n",
    "\n",
    "def decrypt(input_file):\n",
    "    \"\"\"Decrypt encrypted JSON file\"\"\"\n",
    "    private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "    with open(input_file, 'rb') as f:\n",
    "        encrypted_data = f.read()\n",
    "    enc = base64.b64decode(encrypted_data[24:])\n",
    "    data = unpad(cipher.decrypt(enc), 16)\n",
    "    decoded_string = data.decode('utf-8')\n",
    "    return json.loads(decoded_string)\n",
    "\n",
    "    \n",
    "def baseline_wander(X):\n",
    "    def get_median_filter_width(sampling_rate, duration):\n",
    "        res = int(sampling_rate * duration)\n",
    "        res += (res % 2) - 1\n",
    "        return res\n",
    "\n",
    "    ms_flt_array = [0.2, 0.6]\n",
    "    mfa = np.zeros(len(ms_flt_array), dtype=\"int\")\n",
    "    for i in range(0, len(ms_flt_array)):\n",
    "        mfa[i] = get_median_filter_width(500, ms_flt_array[i])\n",
    "    X0 = X\n",
    "    for mi in range(0, len(mfa)):\n",
    "        X0 = sp_signal.medfilt(X0, mfa[mi])\n",
    "    X0 = np.subtract(X, X0)\n",
    "    return X0\n",
    "\n",
    "\n",
    "def normalize(signal, min_val, max_val):\n",
    "    \"\"\"Normalize signal to range [0, 1].\"\"\"\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(signal)\n",
    "    return (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def process_signal(signal_data, min_val, max_val):\n",
    "    data = normalize(signal_data, min_val, max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_process(filename):\n",
    "    keys = ['dataL2']\n",
    "    datas = []\n",
    "    \n",
    "    for key in keys:\n",
    "        sig = np.array(filename[key])\n",
    "        datas.append(sig.astype('float32'))\n",
    "    \n",
    "    datas_array = np.array(datas)               # shape: (1, length) or (channels, length)\n",
    "    \n",
    "    # ── Compute real (raw) statistics here ───────────────────────────────\n",
    "    raw_min   = np.min(datas_array)\n",
    "    raw_max   = np.max(datas_array)\n",
    "    raw_mean  = np.mean(datas_array)\n",
    "    raw_std   = np.std(datas_array)\n",
    "    raw_var   = np.var(datas_array)\n",
    "    raw_median = np.median(datas_array)\n",
    "    \n",
    "    print(\"\\nRaw (pre-normalized) signal statistics:\")\n",
    "    print(f\"  Min    = {raw_min:12.4f}\")\n",
    "    print(f\"  Max    = {raw_max:12.4f}\")\n",
    "    print(f\"  Mean   = {raw_mean:12.4f}\")\n",
    "    print(f\"  Std    = {raw_std:12.4f}\")\n",
    "    print(f\"  Var    = {raw_var:14.6f}\")\n",
    "    print(f\"  Median = {raw_median:12.4f}\")\n",
    "    print(f\"  Range  = {raw_max - raw_min:.4f}\\n\")\n",
    "    \n",
    "    # Now do normalization (your existing code)\n",
    "    min_val = raw_min\n",
    "    max_val = raw_max\n",
    "    signal = []\n",
    "    for i in range(datas_array.shape[0]):\n",
    "        signal.append(normalize(datas_array[i, :], min_val, max_val))\n",
    "\n",
    "    final_data = np.stack(signal)\n",
    "    final_data = np.expand_dims(final_data, axis=0)\n",
    "    final_data = final_data.transpose(0, 2, 1)\n",
    "    \n",
    "    return final_data, {\n",
    "        'raw_min': raw_min, 'raw_max': raw_max, 'raw_mean': raw_mean,\n",
    "        'raw_std': raw_std, 'raw_var': raw_var, 'raw_median': raw_median,\n",
    "        'raw_range': raw_max - raw_min\n",
    "    }, datas_array[0]  # return flattened raw for simplicity\n",
    "\n",
    "\n",
    "def remove_close_peaks(r_peaks, validation_signal, min_dist_samples):\n",
    "    \"\"\"Remove peaks that are too close together, keeping the stronger one\"\"\"\n",
    "    if len(r_peaks) == 0: \n",
    "        return np.array([])\n",
    "    \n",
    "    sorted_idx = np.argsort(r_peaks)\n",
    "    r_peaks = r_peaks[sorted_idx]\n",
    "    validation_abs = np.abs(validation_signal[r_peaks.astype(int)])\n",
    "    \n",
    "    keep = []\n",
    "    last_kept = -min_dist_samples\n",
    "    \n",
    "    for i, current in enumerate(r_peaks):\n",
    "        if current - last_kept >= min_dist_samples:\n",
    "            keep.append(current)\n",
    "            last_kept = current\n",
    "        else:\n",
    "            if validation_abs[i] > validation_abs[len(keep)-1]:\n",
    "                keep[-1] = current\n",
    "                last_kept = current\n",
    "    \n",
    "    return np.array(keep)\n",
    "\n",
    "\n",
    "def amplitude_based_filtering(ecg_signal, peaks, segment_num=\"Unknown\"):\n",
    "    \"\"\"Filter out high amplitude outlier peaks using IQR method\"\"\"\n",
    "    if len(peaks) == 0:\n",
    "        return peaks, np.array([])\n",
    "    \n",
    "    peak_amplitudes = np.abs(ecg_signal[peaks.astype(int)])\n",
    "    \n",
    "    median_amp = np.median(peak_amplitudes)\n",
    "    q75, q25 = np.percentile(peak_amplitudes, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    if iqr > 0:\n",
    "        high_amp_threshold = q75 + 1.5 * iqr\n",
    "        \n",
    "        high_amp_indices = np.where(peak_amplitudes > high_amp_threshold)[0]\n",
    "        high_amp_count = len(high_amp_indices)\n",
    "        \n",
    "        # If we have 2-3 outlier peaks, remove them\n",
    "        # if 2 <= high_amp_count <= 3:\n",
    "        # if 3 <= high_amp_count <= 4 and len(peaks) - high_amp_count > 0:\n",
    "        # if 5 <= high_amp_count <= 6 and len(peaks) - high_amp_count > 0:\n",
    "        if len(peaks) - high_amp_count > 0:\n",
    "            mask = np.ones(len(peaks), dtype=bool)\n",
    "            # mask[high_amp_indices] = False\n",
    "            mask[high_amp_indices] = True\n",
    "            cleaned_peaks = peaks[mask]\n",
    "            cleaned_amplitudes = peak_amplitudes[mask]\n",
    "        else:\n",
    "            cleaned_peaks = peaks\n",
    "            cleaned_amplitudes = peak_amplitudes\n",
    "    else:\n",
    "        cleaned_peaks = peaks\n",
    "        cleaned_amplitudes = peak_amplitudes\n",
    "    \n",
    "    return cleaned_peaks, cleaned_amplitudes\n",
    "\n",
    "\n",
    "def remove_t_waves(ecg_signal, peaks, sampling_rate):\n",
    "    \"\"\"Remove T-wave false positives based on timing and morphology\"\"\"\n",
    "    if len(peaks) < 3:\n",
    "        return peaks\n",
    "    \n",
    "    sorted_peaks = np.sort(peaks)\n",
    "    cleaned_peaks = []\n",
    "    \n",
    "    for i, peak in enumerate(sorted_peaks):\n",
    "        is_r_peak = True\n",
    "        \n",
    "        if i > 0:\n",
    "            prev_peak = sorted_peaks[i-1]\n",
    "            interval_ms = (peak - prev_peak) / sampling_rate * 1000\n",
    "            \n",
    "            # Check if this could be a T-wave (160-450ms after R-peak)\n",
    "            if 160 < interval_ms < 450:\n",
    "                prev_amp = abs(ecg_signal[int(prev_peak)])\n",
    "                curr_amp = abs(ecg_signal[int(peak)])\n",
    "                \n",
    "                # T-waves are typically smaller and wider\n",
    "                if curr_amp < prev_amp * 0.5:\n",
    "                    half_max = curr_amp * 0.5\n",
    "                    \n",
    "                    # Measure width at half maximum\n",
    "                    left = peak\n",
    "                    while left > 0 and left > peak - 100:\n",
    "                        if abs(ecg_signal[int(left)]) < half_max:\n",
    "                            break\n",
    "                        left -= 1\n",
    "                    \n",
    "                    right = peak\n",
    "                    while right < len(ecg_signal) - 1 and right < peak + 100:\n",
    "                        if abs(ecg_signal[int(right)]) < half_max:\n",
    "                            break\n",
    "                        right += 1\n",
    "                    \n",
    "                    width_ms = (right - left) / sampling_rate * 1000\n",
    "                    \n",
    "                    # T-waves are wider than QRS complexes\n",
    "                    if width_ms > 40:\n",
    "                        is_r_peak = False\n",
    "        \n",
    "        if is_r_peak:\n",
    "            cleaned_peaks.append(peak)\n",
    "    \n",
    "    return np.array(cleaned_peaks)\n",
    "\n",
    "\n",
    "def robust_qrs_detect_internal(data_clean, sampling_rate):\n",
    "    \"\"\"Multi-strategy robust QRS detection for difficult cases\"\"\"\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # Calculate sharpness threshold\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    all_candidate_peaks = []\n",
    "    \n",
    "    # Strategy 1: Multi-band detection with multiple thresholds\n",
    "    freq_bands = [(5, 15), (8, 24), (10, 30), (12, 40)]\n",
    "    \n",
    "    for low_freq, high_freq in freq_bands:\n",
    "        low = low_freq / nyquist\n",
    "        high = high_freq / nyquist\n",
    "        b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "        filtered = sp_signal.filtfilt(b, a, data_clean)\n",
    "        \n",
    "        squared = filtered ** 2\n",
    "        window_size = int(0.15 * sampling_rate)\n",
    "        integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "        \n",
    "        mean_val = np.mean(integrated)\n",
    "        std_val = np.std(integrated)\n",
    "        \n",
    "        thresholds = [mean_val + 0.1 * std_val, mean_val + 0.2 * std_val, mean_val + 0.3 * std_val]\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            candidates, _ = sp_signal.find_peaks(\n",
    "                integrated, \n",
    "                height=threshold,\n",
    "                distance=int(0.2 * sampling_rate)\n",
    "            )\n",
    "            \n",
    "            search_window = int(0.1 * sampling_rate)\n",
    "            sharp_window = int(0.18 * sampling_rate)\n",
    "            \n",
    "            for peak in candidates:\n",
    "                start_sharp = max(0, peak - sharp_window)\n",
    "                end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "                if start_sharp < end_sharp:\n",
    "                    local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "                    \n",
    "                    if local_sharpness > sharpness_threshold:\n",
    "                        start = max(0, peak - search_window)\n",
    "                        end = min(len(original_data), peak + search_window)\n",
    "                        if start < end:\n",
    "                            local_segment = original_data[start:end]\n",
    "                            local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                            refined_peak = start + local_max_idx\n",
    "                            all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Strategy 2: Prominence-based detection\n",
    "    peaks_prom, properties = sp_signal.find_peaks(\n",
    "        original_data,\n",
    "        distance=int(0.2 * sampling_rate),\n",
    "        prominence=0.02\n",
    "    )\n",
    "    \n",
    "    sharp_window = int(0.18 * sampling_rate)\n",
    "    for peak in peaks_prom:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            if local_sharpness > sharpness_threshold * 0.8:\n",
    "                all_candidate_peaks.append(peak)\n",
    "    \n",
    "    # Strategy 3: Derivative-based detection\n",
    "    diff_signal = np.diff(original_data)\n",
    "    diff_squared = diff_signal ** 2\n",
    "    diff_squared = np.append(diff_squared, 0)\n",
    "    \n",
    "    mean_diff = np.mean(diff_squared)\n",
    "    std_diff = np.std(diff_squared)\n",
    "    \n",
    "    diff_peaks, _ = sp_signal.find_peaks(\n",
    "        diff_squared,\n",
    "        height=mean_diff + 0.5 * std_diff,\n",
    "        distance=int(0.15 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    search_window = int(0.08 * sampling_rate)\n",
    "    \n",
    "    for peak in diff_peaks:\n",
    "        start_sharp = max(0, peak - sharp_window)\n",
    "        end_sharp = min(len(strict_score), peak + sharp_window)\n",
    "        if start_sharp < end_sharp:\n",
    "            local_sharpness = np.max(strict_score[start_sharp:end_sharp])\n",
    "            \n",
    "            if local_sharpness > sharpness_threshold * 0.7:\n",
    "                start = max(0, peak - search_window)\n",
    "                end = min(len(original_data), peak + search_window)\n",
    "                if start < end:\n",
    "                    local_segment = original_data[start:end]\n",
    "                    local_max_idx = np.argmax(np.abs(local_segment))\n",
    "                    refined_peak = start + local_max_idx\n",
    "                    all_candidate_peaks.append(refined_peak)\n",
    "    \n",
    "    # Merge and deduplicate peaks\n",
    "    if len(all_candidate_peaks) > 0:\n",
    "        all_candidate_peaks = np.unique(all_candidate_peaks)\n",
    "        \n",
    "        min_distance = int(0.15 * sampling_rate)\n",
    "        sorted_peaks = np.sort(all_candidate_peaks)\n",
    "        \n",
    "        if len(sorted_peaks) > 0:\n",
    "            keep_mask = [True]\n",
    "            for i in range(1, len(sorted_peaks)):\n",
    "                if sorted_peaks[i] - sorted_peaks[i-1] >= min_distance:\n",
    "                    keep_mask.append(True)\n",
    "                else:\n",
    "                    start1 = max(0, sorted_peaks[i-1] - sharp_window)\n",
    "                    end1 = min(len(strict_score), sorted_peaks[i-1] + sharp_window)\n",
    "                    start2 = max(0, sorted_peaks[i] - sharp_window)\n",
    "                    end2 = min(len(strict_score), sorted_peaks[i] + sharp_window)\n",
    "                    \n",
    "                    sharp1 = np.max(strict_score[start1:end1]) if start1 < end1 else 0\n",
    "                    sharp2 = np.max(strict_score[start2:end2]) if start2 < end2 else 0\n",
    "                    \n",
    "                    if sharp2 > sharp1:\n",
    "                        keep_mask[-1] = False\n",
    "                        keep_mask.append(True)\n",
    "                    else:\n",
    "                        keep_mask.append(False)\n",
    "            \n",
    "            sorted_peaks = sorted_peaks[keep_mask]\n",
    "    \n",
    "    return sorted_peaks if len(all_candidate_peaks) > 0 else np.array([])\n",
    "\n",
    "\n",
    "def qrs_detect(data, sampling_rate, segment_duration=None, raw_segment=None):\n",
    "    if raw_segment is not None:\n",
    "        var_raw = np.var(raw_segment)\n",
    "        # if var_raw < 0.0095:                  \n",
    "        if var_raw < 0.005:                  \n",
    "            print(f\"Raw variance {var_raw:.6f} < 0.0095 → treating as asystole / flatline\")\n",
    "            return data, np.array([]), 0.0, np.array([])\n",
    "    # else:\n",
    "    #     var = np.var(data)\n",
    "    #     if var < 0.00015:                     \n",
    "    #         print(f\"Normalized variance {var:.6f} too low → possible asystole\")\n",
    "    #         return data, np.array([]), 0.0, np.array([])\n",
    "\n",
    "    # data_clean = baseline_wander(data) \n",
    "\n",
    "    data_clean = data \n",
    "\n",
    "    original_data = data_clean.copy()\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    \n",
    "    # --- STREAM 1: Standard Detection ---\n",
    "    low = 8 / nyquist\n",
    "    high = 24 / nyquist\n",
    "    b, a = sp_signal.butter(2, [low, high], btype='band')\n",
    "    filtered_standard = sp_signal.filtfilt(b, a, data_clean)\n",
    "    \n",
    "    filtered_abs = np.abs(filtered_standard)\n",
    "    diff = np.diff(filtered_abs)\n",
    "    diff = np.append(diff, 0)\n",
    "    squared = diff ** 2\n",
    "    \n",
    "    window_size = int(0.15 * sampling_rate)\n",
    "    integrated = np.convolve(squared, np.ones(window_size) / window_size, mode='same')\n",
    "    \n",
    "    mean_val = np.mean(integrated)\n",
    "    std_val = np.std(integrated)\n",
    "    threshold = mean_val + 0.20 * std_val\n",
    "    \n",
    "    candidates, _ = sp_signal.find_peaks(\n",
    "        integrated,\n",
    "        height=threshold,\n",
    "        distance=int(0.12 * sampling_rate)\n",
    "    )\n",
    "    \n",
    "    # --- STREAM 2: Sharpness Validator ---\n",
    "    low_strict = 10 / nyquist\n",
    "    high_strict = 40 / nyquist\n",
    "    b2, a2 = sp_signal.butter(2, [low_strict, high_strict], btype='band')\n",
    "    filtered_strict = sp_signal.filtfilt(b2, a2, data_clean)\n",
    "    diff_strict = np.diff(np.abs(filtered_strict))\n",
    "    diff_strict = np.append(diff_strict, 0)\n",
    "    strict_score = diff_strict ** 2\n",
    "    \n",
    "    if len(strict_score) > 0:\n",
    "        sharpness_threshold = np.percentile(strict_score, 94)\n",
    "    else:\n",
    "        sharpness_threshold = 0\n",
    "    \n",
    "    confirmed_peaks = []\n",
    "    search_window = int(0.18 * sampling_rate)\n",
    "    \n",
    "    for peak in candidates:\n",
    "        start_check = max(0, peak - search_window)\n",
    "        end_check = min(len(strict_score), peak + search_window)\n",
    "        if start_check >= end_check:\n",
    "            continue\n",
    "            \n",
    "        local_sharpness = np.max(strict_score[start_check:end_check])\n",
    "        \n",
    "        if local_sharpness > sharpness_threshold:\n",
    "            local_segment = original_data[start_check:end_check]\n",
    "            if len(local_segment) > 0:\n",
    "                abs_local_segment = np.abs(local_segment)\n",
    "                local_max_idx = np.argmax(abs_local_segment)\n",
    "                confirmed_peaks.append(start_check + local_max_idx)\n",
    "    \n",
    "    r_peaks = np.array(confirmed_peaks)\n",
    "    \n",
    "    # Remove close peaks\n",
    "    min_dist = int(0.15 * sampling_rate)\n",
    "    r_peaks = remove_close_peaks(r_peaks, original_data, min_dist)\n",
    "    \n",
    "    cleaned_r = np.sort(np.array([x for x in r_peaks if not (isinstance(x, float) and np.isnan(x))]))\n",
    "    \n",
    "    # =================================================================\n",
    "    # CRITICAL FIX: GAP FILLING WITH AMPLITUDE GUARDRAILS\n",
    "    # =================================================================\n",
    "    if len(cleaned_r) >= 2:\n",
    "        # Calculate reference height (Median of existing peaks)\n",
    "        existing_heights = np.abs(original_data[cleaned_r.astype(int)])\n",
    "        median_r_height = np.median(existing_heights) if len(existing_heights) > 0 else 0\n",
    "        \n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        median_rr = np.median(rr_intervals) if len(rr_intervals) > 0 else 1.0\n",
    "        new_peaks = list(cleaned_r)\n",
    "        \n",
    "        # Only fill gaps if median_rr suggests a normal rhythm (< 1.5s).\n",
    "        # If median_rr is already 2.0s (bradycardia), huge gaps are normal.\n",
    "        if median_rr < 1.5: \n",
    "            for i in range(len(rr_intervals)):\n",
    "                if rr_intervals[i] > 1.4 * median_rr:\n",
    "                    gap_start = cleaned_r[i]\n",
    "                    gap_end = cleaned_r[i+1]\n",
    "                    if gap_start >= gap_end:\n",
    "                        continue\n",
    "                        \n",
    "                    gap_integrated = integrated[gap_start:gap_end]\n",
    "                    # Lower threshold slightly for gap search\n",
    "                    low_thresh = mean_val * 0.6 \n",
    "                    \n",
    "                    gap_candidates, _ = sp_signal.find_peaks(\n",
    "                        gap_integrated,\n",
    "                        height=low_thresh,\n",
    "                        distance=int(0.10 * sampling_rate)\n",
    "                    )\n",
    "                    \n",
    "                    for gc in gap_candidates:\n",
    "                        abs_idx = gap_start + gc\n",
    "                        sw_start = max(0, abs_idx - search_window)\n",
    "                        sw_end = min(len(strict_score), abs_idx + search_window)\n",
    "                        if sw_start >= sw_end:\n",
    "                            continue\n",
    "                            \n",
    "                        # 1. Check Sharpness\n",
    "                        local_sharp_max = np.max(strict_score[sw_start:sw_end])\n",
    "                        if local_sharp_max > sharpness_threshold * 0.4:\n",
    "                            \n",
    "                            # 2. Refine Position\n",
    "                            local_segment = original_data[sw_start:sw_end]\n",
    "                            abs_local_segment = np.abs(local_segment)\n",
    "                            refine_idx = np.argmax(abs_local_segment)\n",
    "                            candidate_peak = sw_start + refine_idx\n",
    "                            \n",
    "                            # 3. AMPLITUDE CHECK (The Fix)\n",
    "                            # Even if it's sharp, is it tall enough?\n",
    "                            # AV Block P-waves are sharp but short.\n",
    "                            candidate_amp = np.abs(original_data[candidate_peak])\n",
    "                            \n",
    "                            # Must be at least 40-50% of the median R-peak height\n",
    "                            if candidate_amp > 0.45 * median_r_height:\n",
    "                                new_peaks.append(candidate_peak)\n",
    "\n",
    "        new_peaks = np.sort(np.unique(new_peaks))\n",
    "        cleaned_r = remove_close_peaks(new_peaks, original_data, min_dist)\n",
    "    \n",
    "    # =================================================================\n",
    "\n",
    "    # Determine expected peak count range\n",
    "    if segment_duration is None:\n",
    "        segment_duration = len(data_clean) / sampling_rate\n",
    "    \n",
    "    # Relaxed expectations for Bradycardia/AV Block\n",
    "    min_expected_peaks = int(30/60 * segment_duration) \n",
    "    max_expected_peaks = int(180/60 * segment_duration)\n",
    "    \n",
    "    # Fallback to robust only if counts are extremely off\n",
    "    if len(cleaned_r) < min_expected_peaks or len(cleaned_r) > max_expected_peaks:\n",
    "        initial_peaks = robust_qrs_detect_internal(data_clean, sampling_rate)\n",
    "        initial_peaks = remove_t_waves(data_clean, initial_peaks, sampling_rate)\n",
    "        cleaned_r, peak_amplitudes = amplitude_based_filtering(data_clean, initial_peaks, \"Segment\")\n",
    "    else:\n",
    "        cleaned_r = remove_t_waves(data_clean, cleaned_r, sampling_rate)\n",
    "    \n",
    "    # Calculate BPM\n",
    "    if len(cleaned_r) > 1:\n",
    "        rr_intervals = np.diff(cleaned_r) / sampling_rate\n",
    "        \n",
    "        # Valid intervals widened to account for Bradycardia/Pauses\n",
    "        # valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 3.5)] \n",
    "        valid_rr = rr_intervals[(rr_intervals > 0.2) & (rr_intervals < 4.0)] \n",
    "        \n",
    "        if len(valid_rr) > 0:\n",
    "            mean_rr = np.mean(valid_rr)\n",
    "            bpm = 60 / mean_rr if mean_rr > 0 else 0\n",
    "        else:\n",
    "            bpm = 0\n",
    "    else:\n",
    "        bpm = 0\n",
    "    \n",
    "    return data, cleaned_r, bpm, cleaned_r\n",
    "\n",
    "\n",
    "\n",
    "def process_ecg_segments(ecg_data, sampling_rate, num_segments=7, min_segment_length=3500):\n",
    "    max_len = len(ecg_data)\n",
    "    \n",
    "    if num_segments > 1:\n",
    "        window_step = (max_len - min_segment_length) / (num_segments - 1)\n",
    "        window_step = round(window_step)\n",
    "    else:\n",
    "        window_step = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * window_step\n",
    "        end_idx = start_idx + min_segment_length\n",
    "        \n",
    "        if end_idx > max_len:\n",
    "            start_idx = max_len - min_segment_length\n",
    "            end_idx = max_len\n",
    "            \n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            end_idx = min(min_segment_length, max_len)\n",
    "        \n",
    "        segment = ecg_data[start_idx:end_idx]\n",
    "        \n",
    "        if len(segment) < 100:\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        segment_duration = len(segment) / sampling_rate\n",
    "        # ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(segment, sampling_rate, segment_duration)\n",
    "\n",
    "        raw_segment = raw_ecg[start_idx:end_idx]   # ← the real raw amplitudes\n",
    "\n",
    "        quality, sqi, details = assess_ecg_quality(raw_segment, sampling_rate)\n",
    "\n",
    "        print(f\"Segment {i+1} SQI: {sqi:.2f} → {quality}\")\n",
    "\n",
    "        # if quality == \"BAD\" or quality == \"MARGINAL\":\n",
    "        if quality == \"BADDDDDDDDDDDDDDDDD\":\n",
    "            results.append({\n",
    "                'segment_num': i + 1,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'ecg_filtered': np.array([]),\n",
    "                'r_peaks': np.array([]),\n",
    "                'bpm': 0,\n",
    "                'cleaned_r': np.array([]),\n",
    "                'ecg_raw': segment\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        ecg_filtered, r_peaks, bpm, cleaned_r = qrs_detect(\n",
    "            segment,\n",
    "            sampling_rate,\n",
    "            segment_duration,\n",
    "            raw_segment=raw_segment                # ← pass raw here\n",
    "        )\n",
    "\n",
    "        print(f\"Segment {i+1}: Detected {len(r_peaks)} R-peaks, BPM: {bpm:.1f}\")\n",
    "        \n",
    "        adjusted_r_peaks = r_peaks + start_idx if len(r_peaks) > 0 else np.array([])\n",
    "        adjusted_cleaned_r = np.array(cleaned_r) + start_idx if len(cleaned_r) > 0 else np.array([])\n",
    "        \n",
    "        results.append({\n",
    "            'segment_num': i + 1,\n",
    "            'start_idx': start_idx,\n",
    "            'end_idx': end_idx,\n",
    "            'ecg_filtered': ecg_filtered,\n",
    "            'r_peaks': adjusted_r_peaks,\n",
    "            'bpm': bpm,\n",
    "            'cleaned_r': adjusted_cleaned_r,\n",
    "            'ecg_raw': segment\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_ecg_stats(signal, fs=500):\n",
    "    \"\"\"Compute common statistics for an ECG segment\"\"\"\n",
    "    if len(signal) == 0:\n",
    "        return {\n",
    "            'nsamples': 0,\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'var': np.nan,\n",
    "            'min': np.nan,\n",
    "            'max': np.nan,\n",
    "            'median': np.nan,\n",
    "            'rms': np.nan,\n",
    "            'duration_s': 0.0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'nsamples': len(signal),\n",
    "        'mean': float(np.mean(signal)),\n",
    "        'std': float(np.std(signal)),\n",
    "        'var': float(np.var(signal)),\n",
    "        'min': float(np.min(signal)),\n",
    "        'max': float(np.max(signal)),\n",
    "        'median': float(np.median(signal)),\n",
    "        'rms': float(np.sqrt(np.mean(signal**2))),\n",
    "        'duration_s': len(signal) / fs\n",
    "    }\n",
    "\n",
    "\n",
    "def format_stats_text(stats, prefix=\"\"):\n",
    "    \"\"\"Create a compact multi-line stats string for plotting\"\"\"\n",
    "    lines = [\n",
    "        f\"{prefix}Duration: {stats['duration_s']:.2f} s\",\n",
    "        f\"Samples:   {stats['nsamples']}\",\n",
    "        f\"Mean:      {stats['mean']:.4f}\",\n",
    "        f\"Std:       {stats['std']:.4f}\",\n",
    "        f\"Var:       {stats['var']:.6f}\",\n",
    "        f\"Min / Max: {stats['min']:.4f} / {stats['max']:.4f}\",\n",
    "        f\"Median:    {stats['median']:.4f}\",\n",
    "        f\"RMS:       {stats['rms']:.4f}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def plot_ecg_segments(ecg_data, sampling_rate, results, title=\"ECG Segments with R-peaks and BPM\", raw_ecg=None):\n",
    "    num_segments = len(results)\n",
    "    fig, axes = plt.subplots(num_segments, 1, figsize=(15, 3.5 * num_segments), sharex=False)\n",
    "    \n",
    "    if num_segments == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    time = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    global_stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "    fig.suptitle(f\"{title}\\nFull signal stats: {global_stats['duration_s']:.1f}s | \"\n",
    "                 f\"mean={global_stats['mean']:.4f}  std={global_stats['std']:.4f}\", \n",
    "                 fontsize=13, y=0.98)\n",
    "    \n",
    "    for i, (ax, result) in enumerate(zip(axes, results)):\n",
    "        segment_num = result['segment_num']\n",
    "        start_idx = result['start_idx']\n",
    "        end_idx = result['end_idx']\n",
    "        bpm = result['bpm']\n",
    "        r_peaks = result['r_peaks']\n",
    "        \n",
    "        segment_time = time[start_idx:end_idx]\n",
    "        segment_data = result['ecg_raw']\n",
    "        \n",
    "        ax.plot(segment_time, segment_data, 'b-', alpha=0.8, linewidth=1.1, label='ECG')\n",
    "        \n",
    "        if len(r_peaks) > 0:\n",
    "            r_times = r_peaks / sampling_rate\n",
    "            r_values = ecg_data[r_peaks.astype(int)]\n",
    "            ax.plot(r_times, r_values, 'ro', markersize=7, label='R-peaks', alpha=0.85)\n",
    "        \n",
    "        # ── Statistics box per segment (use raw if available) ───────────────────────────────\n",
    "        if raw_ecg is not None:\n",
    "            raw_segment = raw_ecg[start_idx:end_idx]\n",
    "            seg_stats = compute_ecg_stats(raw_segment, sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            seg_stats = compute_ecg_stats(segment_data, sampling_rate)\n",
    "            prefix = \"\"\n",
    "        stats_text = format_stats_text(seg_stats, prefix + f\"Seg {segment_num}  \")\n",
    "        stats_text += f\"\\nBPM:       {bpm:.1f}\"\n",
    "        \n",
    "        ax.text(0.02, 0.98, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=9.5,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.82, edgecolor='gray', boxstyle='round,pad=0.4'))\n",
    "        \n",
    "        segment_duration = (end_idx - start_idx) / sampling_rate\n",
    "        ax.set_title(f'Segment {segment_num}: {start_idx:,} – {end_idx:,}  |  BPM: {bpm:.1f}')\n",
    "        ax.set_ylabel('Amplitude (norm)')\n",
    "        ax.grid(True, alpha=0.35, linestyle='--')\n",
    "        ax.set_xlim([segment_time[0], segment_time[-1]])\n",
    "        ax.legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (seconds)')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])   # make room for suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # ── Console summary ───────────────────────────────────────────────\n",
    "    print(\"═\" * 70)\n",
    "    print(\"ECG SEGMENT STATISTICS SUMMARY\")\n",
    "    print(\"═\" * 70)\n",
    "    for res in results:\n",
    "        if raw_ecg is not None:\n",
    "            s = compute_ecg_stats(raw_ecg[res['start_idx']:res['end_idx']], sampling_rate)\n",
    "            prefix = \"Raw \"\n",
    "        else:\n",
    "            s = compute_ecg_stats(res['ecg_raw'], sampling_rate)\n",
    "            prefix = \"Norm \"\n",
    "        print(f\"Segment {res['segment_num']:2d} | {s['duration_s']:5.2f}s | \"\n",
    "              f\"mean={s['mean']:8.4f}  std={s['std']:7.4f}  BPM={res['bpm']:5.1f} ({prefix.strip()})\"\n",
    "            )\n",
    "    print(\"═\" * 70)\n",
    "    \n",
    "    \n",
    "def plot_full_ecg(ecg_data, sampling_rate, title=\"Full ECG Signal Analysis\", raw_ecg=None):\n",
    "    # _, r_peaks, global_bpm, _ = qrs_detect(ecg_data, sampling_rate)\n",
    "    _, r_peaks, global_bpm, _ = qrs_detect(\n",
    "        ecg_data,\n",
    "        sampling_rate,\n",
    "        raw_segment=raw_ecg[:len(ecg_data)]    # pass corresponding raw part\n",
    "    )\n",
    "        \n",
    "    if raw_ecg is not None:\n",
    "        stats = compute_ecg_stats(raw_ecg[:len(ecg_data)], sampling_rate)\n",
    "        prefix = \"Raw \"\n",
    "    else:\n",
    "        stats = compute_ecg_stats(ecg_data, sampling_rate)\n",
    "        prefix = \"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 6)) # Width of 20 makes the 15k samples readable\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = np.arange(len(ecg_data)) / sampling_rate\n",
    "    \n",
    "    # Plot the signal\n",
    "    plt.plot(time_axis, ecg_data, 'b-', linewidth=0.8, alpha=0.8, label='Filtered ECG')\n",
    "    \n",
    "    # Plot the peaks\n",
    "    if len(r_peaks) > 0:\n",
    "        # Filter out peaks that might be out of bounds (safety check)\n",
    "        valid_peaks = r_peaks[r_peaks < len(ecg_data)].astype(int)\n",
    "        \n",
    "        peak_times = valid_peaks / sampling_rate\n",
    "        peak_values = ecg_data[valid_peaks]\n",
    "        \n",
    "        plt.plot(peak_times, peak_values, 'ro', markersize=4, label='R-peaks')\n",
    "        \n",
    "        # Optional: Annotate every 5th peak to help navigation\n",
    "        for i, (t, v) in enumerate(zip(peak_times, peak_values)):\n",
    "            if i % 5 == 0:\n",
    "                plt.annotate(f'{t:.1f}s', (t, v), xytext=(0, 10), \n",
    "                             textcoords='offset points', ha='center', fontsize=8, color='red')\n",
    "\n",
    "    plt.title(f\"{title} | Global BPM: {global_bpm:.1f} | Total Peaks: {len(r_peaks)}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Normalized Amplitude\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, which='both', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Global Analysis: {len(r_peaks)} peaks detected over {len(ecg_data)/sampling_rate:.2f} seconds.\")\n",
    "    print(f\"{prefix}Full signal stats →  mean={stats['mean']:.4f}  std={stats['std']:.4f}  var={stats['var']:.6f}\")\n",
    "\n",
    "\n",
    "# def assess_ecg_quality(ecg_raw, fs=500):\n",
    "#     ecg = np.asarray(ecg_raw)\n",
    "\n",
    "#     if len(ecg) < fs:\n",
    "#         return \"BADDDDDDDDDDDDDDDDD\", 0.0, {\"reason\": \"Too short\"}\n",
    "\n",
    "#     # ── 1. Variance (flatline / saturation)\n",
    "#     var = np.var(ecg)\n",
    "\n",
    "#     # ── 2. Baseline wander (0–0.5 Hz)\n",
    "#     f, pxx = sp_signal.welch(ecg, fs=fs, nperseg=2048)\n",
    "#     baseline_power = np.sum(pxx[(f >= 0.05) & (f <= 0.5)])\n",
    "#     total_power = np.sum(pxx)\n",
    "#     baseline_ratio = baseline_power / total_power if total_power > 0 else 1.0\n",
    "\n",
    "#     # ── 3. Powerline noise (50 Hz)\n",
    "#     power_50hz = np.sum(pxx[(f >= 48) & (f <= 52)])\n",
    "#     power_5_40hz = np.sum(pxx[(f >= 5) & (f <= 40)])\n",
    "#     powerline_ratio = power_50hz / power_5_40hz if power_5_40hz > 0 else 1.0\n",
    "\n",
    "#     # ── 4. QRS energy dominance\n",
    "#     qrs_band_power = np.sum(pxx[(f >= 8) & (f <= 25)])\n",
    "#     qrs_ratio = qrs_band_power / total_power if total_power > 0 else 0\n",
    "\n",
    "#     # ── 5. Clipping detection\n",
    "#     clip_ratio = np.mean((ecg == np.max(ecg)) | (ecg == np.min(ecg)))\n",
    "\n",
    "#     # ── Scoring (0–1)\n",
    "#     score = 1.0\n",
    "#     score -= 0.4 if var < 0.005 else 0\n",
    "#     score -= min(baseline_ratio * 2, 0.3)\n",
    "#     score -= min(powerline_ratio * 2, 0.3)\n",
    "#     score -= 0.2 if clip_ratio > 0.02 else 0\n",
    "#     score += min(qrs_ratio * 1.5, 0.3)\n",
    "\n",
    "#     score = np.clip(score, 0, 1)\n",
    "\n",
    "#     if score >= 0.75:  # 75\n",
    "#         quality = \"GOOD\"\n",
    "#     # elif score >= 0.45:\n",
    "#     #     quality = \"MARGINAL\"\n",
    "#     else:\n",
    "#         quality = \"BADDDDDDDDDDDDDDDDD\"\n",
    "\n",
    "#     details = {\n",
    "#         \"variance\": float(var),\n",
    "#         \"baseline_ratio\": float(baseline_ratio),\n",
    "#         \"powerline_ratio\": float(powerline_ratio),\n",
    "#         \"qrs_ratio\": float(qrs_ratio),\n",
    "#         \"clip_ratio\": float(clip_ratio)\n",
    "#     }\n",
    "\n",
    "#     return quality, score, details\n",
    "\n",
    "# BADDDDDDDDDDDDDDDDD\n",
    "\n",
    "\n",
    "def assess_ecg_quality(ecg_raw, fs=500):\n",
    "    ecg = np.asarray(ecg_raw)\n",
    "\n",
    "    if len(ecg) < fs:\n",
    "        return \"BAD\", 0.0, {\"reason\": \"Too short\"}\n",
    "\n",
    "    # ── 1. Variance (flatline / saturation)\n",
    "    var = np.var(ecg)\n",
    "    \n",
    "    # ── 2. Check for monotonic rising/drowning patterns using peak differences ──\n",
    "    # Find all local maxima and minima (peaks and valleys)\n",
    "    maxima, _ = sp_signal.find_peaks(ecg, distance=50)  # local peaks\n",
    "    minima, _ = sp_signal.find_peaks(-ecg, distance=50)  # local valleys\n",
    "    \n",
    "    # Combine and sort all extrema\n",
    "    all_extrema = np.sort(np.concatenate([maxima, minima]))\n",
    "    \n",
    "    monotonic_score = 1.0\n",
    "    monotonic_details = {\"peak_difference_pattern\": \"variable\"}\n",
    "    \n",
    "    if len(all_extrema) >= 3:  # Need at least 3 extrema to analyze pattern\n",
    "        # Calculate amplitude differences between consecutive extrema\n",
    "        extrema_values = ecg[all_extrema]\n",
    "        amplitude_diffs = np.abs(np.diff(extrema_values))\n",
    "        \n",
    "        # Calculate time differences between consecutive extrema\n",
    "        time_diffs = np.diff(all_extrema) / fs  # in seconds\n",
    "        \n",
    "        # Analyze patterns:\n",
    "        # 1. Check if amplitude differences are consistently small (suggests drowning/smooth signal)\n",
    "        if len(amplitude_diffs) > 2:\n",
    "            amp_std = np.std(amplitude_diffs)\n",
    "            amp_mean = np.mean(amplitude_diffs)\n",
    "            \n",
    "            # Low standard deviation of amplitude differences suggests consistent pattern\n",
    "            if amp_std < 0.1 * amp_mean and amp_mean < 0.05 * (np.max(ecg) - np.min(ecg)):\n",
    "                monotonic_score -= 0.3\n",
    "                monotonic_details[\"peak_difference_pattern\"] = \"consistent_small\"\n",
    "        \n",
    "        # 2. Check for consistent rising/falling trend by analyzing peak-valley sequences\n",
    "        trend_strength = 0\n",
    "        trend_direction = 0\n",
    "        \n",
    "        # Analyze sequence of peaks and valleys\n",
    "        peak_valley_values = ecg[all_extrema]\n",
    "        peak_valley_signs = np.sign(np.diff(peak_valley_values))\n",
    "        \n",
    "        # Count consecutive same-sign differences\n",
    "        if len(peak_valley_signs) > 3:\n",
    "            same_sign_count = 0\n",
    "            max_same_sign = 0\n",
    "            \n",
    "            for i in range(1, len(peak_valley_signs)):\n",
    "                if peak_valley_signs[i] == peak_valley_signs[i-1]:\n",
    "                    same_sign_count += 1\n",
    "                    max_same_sign = max(max_same_sign, same_sign_count)\n",
    "                else:\n",
    "                    same_sign_count = 0\n",
    "            \n",
    "            # If many consecutive same-sign differences, strong trend exists\n",
    "            if max_same_sign >= 4:  # 4+ consecutive same-sign changes\n",
    "                monotonic_score -= 0.4\n",
    "                monotonic_details[\"peak_difference_pattern\"] = \"strong_trend\"\n",
    "        \n",
    "        # 3. Check if time intervals between extrema are too regular (suggests artificial pattern)\n",
    "        if len(time_diffs) > 3:\n",
    "            time_std = np.std(time_diffs)\n",
    "            time_mean = np.mean(time_diffs)\n",
    "            \n",
    "            if time_std < 0.1 * time_mean:  # Very regular timing\n",
    "                monotonic_score -= 0.2\n",
    "                monotonic_details[\"time_regularity\"] = \"high\"\n",
    "    \n",
    "    # ── 3. Baseline wander (0–0.5 Hz)\n",
    "    f, pxx = sp_signal.welch(ecg, fs=fs, nperseg=2048)\n",
    "    baseline_power = np.sum(pxx[(f >= 0.05) & (f <= 0.5)])\n",
    "    total_power = np.sum(pxx)\n",
    "    baseline_ratio = baseline_power / total_power if total_power > 0 else 1.0\n",
    "\n",
    "    # ── 4. Powerline noise (50 Hz)\n",
    "    power_50hz = np.sum(pxx[(f >= 48) & (f <= 52)])\n",
    "    power_5_40hz = np.sum(pxx[(f >= 5) & (f <= 40)])\n",
    "    powerline_ratio = power_50hz / power_5_40hz if power_5_40hz > 0 else 1.0\n",
    "\n",
    "    # ── 5. QRS energy dominance\n",
    "    qrs_band_power = np.sum(pxx[(f >= 8) & (f <= 25)])\n",
    "    qrs_ratio = qrs_band_power / total_power if total_power > 0 else 0\n",
    "\n",
    "    # ── 6. Clipping detection\n",
    "    clip_ratio = np.mean((ecg == np.max(ecg)) | (ecg == np.min(ecg)))\n",
    "    \n",
    "    # ── 7. Additional checks for extreme monotonic patterns ──\n",
    "    # Check overall trend using linear regression\n",
    "    x = np.arange(len(ecg))\n",
    "    slope, intercept = np.polyfit(x, ecg, 1)\n",
    "    trend_line = slope * x + intercept\n",
    "    trend_residuals = ecg - trend_line\n",
    "    trend_strength = np.abs(slope) * len(ecg) / (np.max(ecg) - np.min(ecg) + 1e-10)\n",
    "    \n",
    "    # If strong linear trend with low residuals\n",
    "    if trend_strength > 0.5 and np.std(trend_residuals) < 0.2 * np.std(ecg):\n",
    "        monotonic_score -= 0.5\n",
    "        monotonic_details[\"linear_trend\"] = \"strong\"\n",
    "\n",
    "    # ── Combined Scoring (0–1) ──\n",
    "    score = 1.0\n",
    "    \n",
    "    # Apply monotonic pattern penalty\n",
    "    score = max(0, score - (1 - monotonic_score))\n",
    "    \n",
    "    # Apply other penalties\n",
    "    score -= 0.4 if var < 0.005 else 0\n",
    "    score -= min(baseline_ratio * 2, 0.3)\n",
    "    score -= min(powerline_ratio * 2, 0.3)\n",
    "    score -= 0.2 if clip_ratio > 0.02 else 0\n",
    "    \n",
    "    score = np.clip(score, 0, 1)\n",
    "\n",
    "    # Quality classification\n",
    "    if score >= 0.7:\n",
    "        quality = \"GOOD\"\n",
    "    elif score >= 0.4:\n",
    "        quality = \"MARGINAL\"\n",
    "    else:\n",
    "        quality = \"BAD\"\n",
    "\n",
    "    details = {\n",
    "        \"variance\": float(var),\n",
    "        \"baseline_ratio\": float(baseline_ratio),\n",
    "        \"powerline_ratio\": float(powerline_ratio),\n",
    "        \"qrs_ratio\": float(qrs_ratio),\n",
    "        \"clip_ratio\": float(clip_ratio),\n",
    "        \"monotonic_pattern\": monotonic_details[\"peak_difference_pattern\"],\n",
    "        \"score\": float(score),\n",
    "        \"extrema_count\": len(all_extrema),\n",
    "        \"trend_strength\": float(trend_strength) if 'trend_strength' in locals() else 0.0\n",
    "    }\n",
    "\n",
    "    return quality, score, details\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    " \n",
    "# # input_json = r\"simulator\\contec\\trigeminy_1756103085272.json\" \n",
    "# # input_json = r\"simulator\\contec\\asystl_1756103447146.json\" \n",
    "# # input_json = r\"simulator\\contec\\1d av_1756104504294.json\"  \n",
    "# # input_json = r\"simulator\\contec\\3d av_1756104633918.json\"  \n",
    "# # input_json = r\"simulator\\contec\\280bpm_1756100716422.json\" \n",
    "# # input_json = r\"simulator\\contec\\av sequence_1756106676125.json\"  \n",
    "# # input_json = r\"simulator\\contec\\dmnd freq_1756106571373.json\"  \n",
    "# #    \n",
    "# # input_json = r\"simulator\\fluke\\trigeminy_1754543043205.json\"   \n",
    "# # input_json = r\"simulator\\fluke\\3d av_1754545068278.json\"   \n",
    "# # input_json = r\"simulator\\fluke\\asystole_1754544406847.json\"   \n",
    "\n",
    "# # input_json = r\"simulator\\fluke\\80bpm_1754288606201.json\"   \n",
    "# input_json = r\"0_bpm\\asystole_jan_2_1770026113214.json\"   \n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# input_json = r\"v01_prob\\teton_ecg.ecgdatas.json\"  \n",
    "# with open(input_json, 'r') as file:\n",
    "#     all_id_data = json.load(file)\n",
    "\n",
    "# file_data = all_id_data[3]['ecgValue']   \n",
    "\n",
    "\n",
    "\n",
    "# # input_json = r\"bpms\\afib_1766471694144.json\"\n",
    "# # input_json = r\"bpms\\bigeminy_1766467666407.json\"\n",
    "# # input_json = r\"bpms\\pvc 6_1766467718685.json\"    \n",
    "# # input_json = r\"bpms\\tri_1766467618314.json\"\n",
    "# # input_json = r\"v01_prob/220_1767858669130.json\"\n",
    "# # input_json = r\"v01_prob/240bpm_1767858615562.json\"\n",
    "# # input_json = r\"v01_prob\\25 contec_1768375918389.json\"\n",
    "# # input_json = r\"v01_prob\\30bpm contec_1768375716454.json\"\n",
    "# # input_json = r\"v01_prob\\2d av_1754545008828.json\"  \n",
    "# input_json = r\"v01_prob\\3rd_davb_1768554217066.json\"  \n",
    "\n",
    "# with open(input_json, 'r') as file:\n",
    "#     file_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_json = r\"exception\\L2_1759207950416.json\"   \n",
    "# input_json = r\"issues\\L2_1757064122874.json\"  \n",
    "# input_json = r\"v01_prob\\run 5 pvc.json\"  \n",
    "# input_json = r\"issues\\L2_1757579288752.json\"\n",
    "# input_json = r\"issues\\L2_1757737806463.json\"  \n",
    "# input_json = r\"v01_prob\\L2_1765984517025.json\"  \n",
    "# input_json = r\"1st-last-peaks\\L2_1759908627949.json\"\n",
    "# input_json = r\"1st-last-peaks\\L2_1759908888619.json\"\n",
    "\n",
    "# input_json = r\"issues2\\1757998097068\\L2_1757998097068.json\"\n",
    "# input_json = r\"issues2\\1758943573744\\L2_1758943573744.json\"\n",
    "# input_json = r\"issues2\\1759059066184\\L2_1759059066184.json\"     ###########\n",
    "# input_json = r\"issues2\\1759117739887\\L2_1759117739887.json\"\n",
    "# input_json = r\"issues2\\1759118709079\\L2_1759118709079.json\"\n",
    "# input_json = r\"issues2\\1759202739736\\L2_1759202739736.json\"\n",
    "# input_json = r\"issues2\\1759639059357\\L2_1759639059357.json\"\n",
    "# input_json = r\"issues2\\L2_1759208381248.json\"\n",
    "\n",
    "doubles = []\n",
    "with open(input_json, \"rb\") as f:\n",
    "    while chunk := f.read(8):\n",
    "        if len(chunk) < 8:\n",
    "            break\n",
    "        value = struct.unpack(\"<d\", chunk)[0]\n",
    "        doubles.append(value)\n",
    "\n",
    "file_data = {'dataL2': doubles}   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted JSON file (optional - commented out in your version)\"\"\"\n",
    "#     private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "#     cipher = AES.new(private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     return json.loads(decoded_string)\n",
    "\n",
    "# # # input_json = r\"NHF2\\DATA_1750689015865.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750689460556.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750851207409.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750858856842.json\"\n",
    "# # # input_json = r\"NHF2\\DATA_1750862721789.json\"\n",
    "# # input_json = r\"NHF2\\DATA_1750996455820.json\"\n",
    "# # file_data = decrypt(input_json)\n",
    "\n",
    "# # input_json = r\"NHF\\DATA_1752067426678.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1752121970835.json\"  ########\n",
    "# input_json = r\"NHF\\DATA_1754709586876.json\"  #####\n",
    "# # input_json = r\"NHF\\DATA_1754729551054.json\"\n",
    "# file_data = decrypt(input_json)\n",
    "\n",
    "\n",
    "\n",
    "# def decrypt(input_file):\n",
    "#     \"\"\"Decrypt encrypted CSV file using AES ECB mode\"\"\"\n",
    "#     Private_key = \"Msz377xMbcn++vrcDel9vxOuEss8fsWO\"\n",
    "    \n",
    "#     cipher = AES.new(Private_key.encode(), AES.MODE_ECB)\n",
    "#     with open(input_file, 'rb') as f:\n",
    "#         encrypted_data = f.read()\n",
    "\n",
    "#     enc = base64.b64decode(encrypted_data[24:])\n",
    "#     cipher = AES.new(Private_key.encode('utf-8'), AES.MODE_ECB)\n",
    "#     data = unpad(cipher.decrypt(enc), 16)\n",
    "\n",
    "#     decoded_string = data.decode('utf-8')\n",
    "#     data_list = decoded_string.split(\",\")\n",
    "#     float_list = [float(x) for x in data_list]\n",
    "\n",
    "#     return float_list\n",
    "\n",
    "# selected_path = \"v01_prob\\ECG_1735798172211.csv\"  ####\n",
    "# # selected_path = \"v01_prob\\ECG_L2_1738637533455.csv\"\n",
    "# file_data = decrypt(selected_path)\n",
    "# file_data = {'dataL2': file_data}\n",
    "\n",
    "\n",
    "\n",
    "def low_pass_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_lp, a_lp, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "\n",
    "def notch_filter(data):\n",
    "    try:\n",
    "        return sp_signal.filtfilt(b_notch, a_notch, data)\n",
    "    except:\n",
    "        return data\n",
    "\n",
    "     \n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "# processed_data, raw_global_stats, raw_ecg = data_process(\n",
    "#     low_pass_filter(notch_filter(file_data))\n",
    "# )\n",
    "\n",
    "# ecg_full = processed_data[0, :15000, 0]\n",
    "\n",
    "\n",
    "# processed_data, raw_global_stats, raw_ecg = data_process(\n",
    "#     low_pass_filter(notch_filter(file_data))\n",
    "# )\n",
    "# ecg_full = processed_data[0, :15000, 0]\n",
    "\n",
    "# data = data_process(file_data)\n",
    "# ecg_full = data[0, :15000, 0]\n",
    "\n",
    "processed, stats, raw_ecg = data_process(file_data)\n",
    "ecg_full = processed[0, :15000, 0]\n",
    "\n",
    "sampling_rate = 500\n",
    "\n",
    "results = process_ecg_segments(\n",
    "    ecg_data=ecg_full,\n",
    "    sampling_rate=sampling_rate,\n",
    "    num_segments=20,\n",
    "    min_segment_length=1500\n",
    ")\n",
    "\n",
    "plot_ecg_segments(ecg_full, sampling_rate, results, \"ECG Analysis: 4 Segments with R-peak Detection\", raw_ecg=raw_ecg)\n",
    "\n",
    "print(\"\\n--- Plotting Full Data ---\")\n",
    "plot_full_ecg(ecg_full, sampling_rate, \"Final Full Data View\", raw_ecg=raw_ecg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319eb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
